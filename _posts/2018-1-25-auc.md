---
layout: post
title: Probabilistic interpretation of AUC
tags:
- math
- r
---

Unfortunately this was not taught in any of my statistics or data analysis classes at university (wtf it so needs to be :scream_cat:).
So it took me some until I learned that the [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) has a nice probabilistic meaning.

## What's AUC anyway?

Consider:
1. A dataset $S$: $(\mathbf{x}\_1, y\_1), \ldots, (\mathbf{x}\_n, y\_n) \in \mathbb{R}^p \times \\{0, 1\\}$, where
  - $\mathbf{x}\_i$ is a vector of $p$ features collected for the $i$th subject,
  - $y\_i$ is the $i$th subject's label (binary outcome variable of interest, like a disease status, class membership, or whatever binary label).
2. A classification algorithm (like logistic regression, SVM, deep neural net, or whatever you like), trained on $S$, that assigns a score (or probability) $\hat{p}(\mathbf{x}\_{\ast})$ to any new observation $\mathbf{x}\_{\ast} \in \mathbb{R}^p$ signifying how likely its label is $y\_{\ast} = 1$.

Then:
3. A *decision threshold* (or *operating point*) can be chosen to assign a class label ($y\_{\ast} = 0$ or $1$) to $\mathbf{x}\_{\ast}$ based on the value of $\hat{p}(\mathbf{x}\_{\ast})$.
The chosen threshold determines the balance between how many *false positives* and *false negatives* will result from this classification.
4. Plotting the *true positive rate* (TPR) against the *false positive rate* (FPR) *as the operating point changes from its minimum to its maximum value* yields the [*receiver operating characteristic (ROC) curve*](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). Check the [confusion matrix](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix) if you are not sure what TPR and FPR refer to.
5. The area under the ROC curve, or AUC, is used as a measure of classifier performance.

Here is some R code for clarification (not even using `tidyverse` :stuck_out_tongue:):

```r
# load some data, fit a logistic regression classifier
data(iris)
versicolor_virginica <- iris[iris$Species != "setosa", ]
logistic_reg_fit <- glm(Species ~ Sepal.Width + Sepal.Length,
                        data = versicolor_virginica,
                        family = "binomial")
y <- ifelse(versicolor_virginica$Species == "versicolor", 0, 1)
y_pred <- logistic_reg_fit$fitted.values

# get TPR and FPR at different values of the decision threshold
threshold <- seq(0, 1, length = 100)
FPR <- sapply(threshold,
  function(thresh) {
    sum(y_pred >= thresh & y != 1) / sum(y != 1)
  })
TPR <- sapply(threshold,
  function(thresh) {
    sum(y_pred >= thresh & y == 1) / sum(y == 1)
  })

# plot an ROC curve
plot(FPR, TPR)
lines(FPR, TPR)
```

A rather ugly ROC curve emerges:

![ROC curve R example](/images/20180124-AUC/ROC.png?raw=true "An ugly ROC curve")

The area under the ROC curve, or AUC, seem like a nice heuristic to evaluate and compare the overall performance of classification models independent of the exact decision threshold chosen. But there's more to it.

## Probabilistic interpretation

As above, assume that we are looking at a dataset where we want to distinguish data points of *type 0* from those of *type 1*. Consider a classification algorithm that assigns to a random observation $\mathbf{x}\in\mathbb{R}^p$ a score (or probability) $\hat{p}(\mathbf{x}) \in [0,1]$ signifying membership in *class 1*. If the final classification between *class 1* and *class 0* is determined by a decision threshold $t\in[0, 1]$, then the *true positive rate* (a.k.a. *sensitivity* or *recall*) can be written as a conditional probability

$$T(t) := P[\hat{p}(\mathbf{x}) > t \,|\, \mathbf{x}\,\text{belongs to class 1}],$$

and the *false positive rate* (or *1 - specificity*) can be written as

$$F(t) := P[\hat{p}(\mathbf{x}) > t \,|\, \mathbf{x}\,\text{does not belong to class 1}].$$

For brevity of notation let's say $y(\mathbf{x}) = 1$ instead of "$\mathbf{x}$ belongs to class 1", and $y(\mathbf{x})=0$ instead of "$\mathbf{x}$ doesn't belong to class 1".

The ROC curve simply plots $T(t)$ against $F(t)$ while varying $t$ from 0 to 1.
Thus, if we view $T$ as a function of $F$, the AUC can be rewritten as follows.

$$
\begin{eqnarray}
  \mathrm{AUC} &=& \int_0^1 T(F_0) \,\mathrm{d}F_0 \nonumber \\
  &=& \int_0^1 P[\hat{p}(\mathbf{x}) > F^{-1}(F_0) \,|\, y(\mathbf{x}) = 1] \,\mathrm{d}F_0 \nonumber \\
  &=& \int_1^0 P[\hat{p}(\mathbf{x}) > F^{-1}(F(t)) \,|\, y(\mathbf{x}) = 1] \cdot F'(t) \,\mathrm{d}t \nonumber \\
  &=& \int_0^1 P[\hat{p}(\mathbf{x}) > t \,|\, y(\mathbf{x}) = 1] \cdot P[\hat{p}(\mathbf{x^{\prime}}) = t \,|\, y(\mathbf{x^{\prime}}) = 0] \,\mathrm{d}t \nonumber \\
  &=& \int_0^1 P[\hat{p}(\mathbf{x}) > \hat{p}(\mathbf{x^{\prime}}) \,\&\, \hat{p}(\mathbf{x^{\prime}}) = t \,|\, y(\mathbf{x}) = 1 \,\&\, y(\mathbf{x^{\prime}}) = 0] \,\mathrm{d}t \nonumber \\
  &=& P[\hat{p}(\mathbf{x}) > \hat{p}(\mathbf{x^{\prime}}) \,|\, y(\mathbf{x}) = 1 \,\&\, y(\mathbf{x^{\prime}}) = 0]. \nonumber
\end{eqnarray}
$$

So, given a randomly chosen observation $\mathbf{x}$ belonging to *class 1*, and a randomly chosen observation $\mathbf{x^{\prime}}$ belonging to *class 0*, the AUC is the probability that the evaluated classification algorithm will assign a higher score to $\mathbf{x}$ than to $\mathbf{x^{\prime}}$, i.e., the conditional probability of $\hat{p}(\mathbf{x}) > \hat{p}(\mathbf{x^{\prime}})$.

In other words, if the classification algorithm distinguishes "positive" and "negative" examples (e.g., disease status), then

> AUC is the probability of correct ranking of a random "positive"-"negative" pair.

## Computing AUC

The above probabilistic interpretation suggest a simple formula to compute AUC on a finite sample.

> Among all "positive"-"negative" pairs in the dataset compute the proportion of those which are ranked correctly by the evaluated classification algorithm.

Here is an inefficient implementation using results from the above logistic regression example:

```r
s <- 0
for (i in which(y == 1)) {
  for (j in which(y == 0)) {
    if (y_pred[i] > y_pred[j]) {
      s <- s + 1
    } else if (y_pred[i] == y_pred[j]) {
      s <- s + 0.5
    }
  }
}
s <- s / (sum(y == 1) * sum(y == 0))
s
# [1] 0.7918
```

The **proportion of correctly ranked "positive"-"negative" pairs** yields estimated $\mathrm{AUC} = 0.7918$.

We can compare this value to the area under the ROC curve computed with the [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule).

```r
s <- 0
for (i in 1:(length(FPR) - 1)) {
  dFPR <- abs(FPR[i+1] - FPR[i])
  s <- s + 0.5 * dFPR * (TPR[i+1] + TPR[i])
}
s
# [1] 0.7922
```

**Trapezoidal rule** yields estimated $\mathrm{AUC} = 0.7922$. The difference of $0.0004$ can be explained by the fact that we evaluated the ROC curve at only 100 points.

Since there is a minor disagreement, let's use some standard R package to compute AUC.

```r
library(ROCR)
pred <- prediction(y_pred, y)
auc <- as.numeric(performance(pred, measure = "auc")@y.values)
auc
# [1] 0.7918
```

Same as the proportion of correctly ranked pairs! :grin:

## So what? Why care about AUC anyway?

* It has a fu\*\*\* nice probabilistic meaning!

Besides, as a measure of classification performance AUC has many advantages compared to other "single number" performance measures:

* Independence of the decision threshold.
* Invariance to prior class probabilities or class prevalence in the data.
* Can choose/change a decision threshold based on cost-benefit analysis after model training.
* Extensively used in machine learning, and in medical research -- and that for good reasons, as for example explained in an [excellent blog post on deep learning research in medicine by Luke Oakden-Rayner](https://lukeoakdenrayner.wordpress.com/2017/12/06/do-machines-actually-beat-doctors-roc-curves-and-performance-metrics/).
