<!DOCTYPE html>
<html>
  <head>
    <title>Freedman's paradox ‚Äì Alexej Gossmann ‚Äì New Orleans üòé</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Recently I came across the classical 1983 paper A note on screening regression equations by David Freedman. Freedman shows in an impressive way the dangers of data reuse in statistical analyses. The potentially dangerous scenarios include those where the results of one statistical procedure performed on the data are fed into another procedure performed on the same data. As a concrete example Freedman considers the practice of performing variable selection first, and then fitting another model using only the identified variables on the same data that was used to identify them in the first place. Because of the unexpectedly high severity of the problem this phenomenon became known as ‚ÄúFreedman‚Äôs paradox‚Äù. Moreover, in his paper Freedman derives asymptotic estimates for the resulting errors.

" />
    <meta property="og:description" content="Recently I came across the classical 1983 paper A note on screening regression equations by David Freedman. Freedman shows in an impressive way the dangers of data reuse in statistical analyses. The potentially dangerous scenarios include those where the results of one statistical procedure performed on the data are fed into another procedure performed on the same data. As a concrete example Freedman considers the practice of performing variable selection first, and then fitting another model using only the identified variables on the same data that was used to identify them in the first place. Because of the unexpectedly high severity of the problem this phenomenon became known as ‚ÄúFreedman‚Äôs paradox‚Äù. Moreover, in his paper Freedman derives asymptotic estimates for the resulting errors.

" />
    
    <meta name="author" content="Alexej Gossmann" />

    
    <meta property="og:title" content="Freedman's paradox" />
    <meta property="twitter:title" content="Freedman's paradox" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Alexej Gossmann - New Orleans üòé" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->

    <!-- MathJax interation-->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"]]}});
      MathJax.Hub.Config({TeX: {Macros:{subscript:['_{#1}',1],superscript:['^{#1}',1]}}});
    </script>
    <!-- Turn on equation numbering -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
 </head>

  <body>
    <div class="wrapper-masthead">
      <header class="masthead clearfix">
        <a href="/" class="site-avatar"><img src="https://avatars0.githubusercontent.com/u/11449372?v=3&s=460" /></a>

        <div class="site-info">
          <h1 class="site-name"><a href="/">Alexej Gossmann</a></h1>
          <p class="site-description">New Orleans üòé</p>
        </div>

        <nav>
          <a href="/about" class="blue">About</a>
          <a href="/" class="yellow">Blog</a>
          <a href="/talks" class="green">Presentations</a>
          <a href="/publications" class="magenta">Publications</a>
          <a href="/software" class="cyan">Software</a>
        </nav>
      </header>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Freedman's paradox</h1>

  <div class="entry">
    <p>Recently I came across the classical 1983 paper <a href="http://www.public.asu.edu/~gasweete/crj604/readings/1983-Freedman%20(Screening%20Regression%20Equations).pdf"><em>A note on screening regression equations</em></a> by <a href="https://en.wikipedia.org/wiki/David_A._Freedman">David Freedman</a>. Freedman shows in an impressive way the dangers of data reuse in statistical analyses. The potentially dangerous scenarios include those where the results of one statistical procedure performed on the data are fed into another procedure performed on the same data. As a concrete example Freedman considers the practice of performing variable selection first, and then fitting another model using only the identified variables on the same data that was used to identify them in the first place. Because of the unexpectedly high severity of the problem this phenomenon became known as <a href="https://en.wikipedia.org/wiki/Freedman%27s_paradox">‚ÄúFreedman‚Äôs paradox‚Äù</a>. Moreover, in his paper Freedman derives asymptotic estimates for the resulting errors.</p>

<p>The 1983 paper presents a simulation with only 10 repetitions. But in the present day it is very easy (both in terms of computational time and implementation difficulty) to reproduce the simulation with many more repetitions (even my phone‚Äôs computational power is probably higher than that of the high performance computer that Freedman used in the 80‚Äôs). We also have more convenient ways to visualize the results than in the 80‚Äôs. So let‚Äôs do it.</p>

<p>I am going to use a few R packages (most notably the package <code class="highlighter-rouge">broom</code> to fit and analyze many many linear models in a single step).</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">broom</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">20170605</span><span class="p">)</span></code></pre></figure>

<p>The considered data structure is the following:</p>

<ul>
  <li>A matrix of predictors with 100 rows and 50 columns is generated with independent standard normal entries.</li>
  <li>The response variable is generated independently of the model matrix (also from the standard normal distribution), i.e., the true answer is that there is no relationship between predictors and response.</li>
</ul>

<p>Instead of Freedman‚Äôs 10 repetitions we perform 1000. So let‚Äôs generate all 1000 datasets at once as stacked in a large data frame:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">n_row</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="c1"># n_col is set to 51 because the 51st column will serve as y
</span><span class="n">n_col</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">51</span><span class="w">
</span><span class="n">n_rep</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1000</span><span class="w">

</span><span class="c1"># a stack of matrices for all n_rep repetitions is generated...
</span><span class="n">X</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">rnorm</span><span class="p">(</span><span class="n">n_rep</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">n_row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">n_col</span><span class="p">),</span><span class="w"> </span><span class="n">n_rep</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">n_row</span><span class="p">,</span><span class="w"> </span><span class="n">n_col</span><span class="p">)</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">n_col</span><span class="p">)</span><span class="w">
</span><span class="c1"># ...and then transformed to a data frame with a repetition number column
</span><span class="n">X_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as_data_frame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">mutate</span><span class="p">(</span><span class="n">repetition</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">n_rep</span><span class="p">,</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n_row</span><span class="p">))</span></code></pre></figure>

<p><strong>The data are analyzed in two successive linear models. The second (illegally) reusing the results of the first.</strong></p>

<p><em>The first model fit.</em>
After the 1000 ordinary linear models are fit to the data, we record for each of them the R squared, the F test statistic with corresponding p-value, and the t test statistics with p-values for the individual regression coefficients.</p>

<p>Using functions from the <code class="highlighter-rouge">broom</code> package we can fit and extract information from all 1000 models at once.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># all models can be fit at once...
</span><span class="n">models_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">X_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">group_by</span><span class="p">(</span><span class="n">repetition</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">do</span><span class="p">(</span><span class="n">full_model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">X</span><span class="m">51</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">select</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">repetition</span><span class="p">)))</span><span class="w">
</span><span class="c1"># ...then the results are extracted
</span><span class="n">model_coefs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tidy</span><span class="p">(</span><span class="n">models_df</span><span class="p">,</span><span class="w"> </span><span class="n">full_model</span><span class="p">)</span><span class="w">
</span><span class="n">model_statistics</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glance</span><span class="p">(</span><span class="n">models_df</span><span class="p">,</span><span class="w"> </span><span class="n">full_model</span><span class="p">)</span><span class="w">
</span><span class="n">model_statistics</span><span class="o">$</span><span class="n">data_reuse</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">model_statistics</span><span class="p">))</span></code></pre></figure>

<p><em>The second model fit.</em>
For each one of the first 1000 models, the corresponding second linear model is fit using only those variables which have p-values significant at the 25% level in the first model.
That is, the second model uses the first model for variable selection.</p>

<p>This gives us 1000 reduced re-fitted linear models. We record the same model statistics (R squared, F, and t tests) as for the first group of models.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">reduced_models</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">()</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">n_rep</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">full_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">X_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">filter</span><span class="p">(</span><span class="n">repetition</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="w">
  </span><span class="n">significant_coefs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model_coefs</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">filter</span><span class="p">(</span><span class="n">repetition</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p.value</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">0.25</span><span class="p">)</span><span class="w">
  </span><span class="n">reduced_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">select</span><span class="p">(</span><span class="n">full_data</span><span class="p">,</span><span class="w">
                         </span><span class="n">one_of</span><span class="p">(</span><span class="n">unlist</span><span class="p">(</span><span class="n">significant_coefs</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="s2">"term"</span><span class="p">])),</span><span class="w"> </span><span class="n">X</span><span class="m">51</span><span class="p">)</span><span class="w">
  </span><span class="n">reduced_models</span><span class="p">[[</span><span class="n">i</span><span class="p">]]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">X</span><span class="m">51</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reduced_data</span><span class="p">)</span><span class="w">
  </span><span class="n">tmp_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glance</span><span class="p">(</span><span class="n">reduced_models</span><span class="p">[[</span><span class="n">i</span><span class="p">]])</span><span class="w">
  </span><span class="n">tmp_df</span><span class="o">$</span><span class="n">repetition</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">i</span><span class="w">
  </span><span class="n">tmp_df</span><span class="o">$</span><span class="n">data_reuse</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kc">TRUE</span><span class="w">
  </span><span class="n">model_statistics</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bind_rows</span><span class="p">(</span><span class="n">model_statistics</span><span class="p">,</span><span class="w"> </span><span class="n">tmp_df</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<p>Finally let‚Äôs look at the results. The figure shows the distributions of the considered model statistics across the 1000 repetitions for model fits with and without data reuse (the code producing this figure is given at the bottom of this post):</p>

<p><img src="../images/2017-6-5-Freedmans_paradox/unnamed-chunk-5-1.png" alt="plot of chunk unnamed-chunk-5" /></p>

<p>Well, the R squared statistic shows a moderate change between models with or without data reuse (average of 0.3093018 vs. 0.5001641). The F test statistic however grows immensely to an average of 3.2806118 (from 1.0480097), and the p-values fall after data reuse to an average of 0.0112216 (from 0.5017696), below the widely used (but arbitrary) 5% significance level.</p>

<p>Obviously the model with data reuse is highly misleading here, because in fact there are absolutely no relationships between the predictor variables and the response (as per the data generation procedure).</p>

<p>In fact, Freedman derived asymptotic estimates for the magnitudes of change in the considered model statistics, and they indeed match the above observations. However I‚Äôm too lazy to summarize them here. So I refer to <a href="http://www.public.asu.edu/~gasweete/crj604/readings/1983-Freedman%20(Screening%20Regression%20Equations).pdf">the primary source</a>.</p>

<hr />
<p>This code generates the above figure:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">model_statistics</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">select</span><span class="p">(</span><span class="n">r.squared</span><span class="p">,</span><span class="w"> </span><span class="n">p.value</span><span class="p">,</span><span class="w"> </span><span class="n">statistic</span><span class="p">,</span><span class="w"> </span><span class="n">repetition</span><span class="p">,</span><span class="w"> </span><span class="n">data_reuse</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">mutate</span><span class="p">(</span><span class="n">data_reuse</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ifelse</span><span class="p">(</span><span class="n">data_reuse</span><span class="p">,</span><span class="w"> </span><span class="s2">"With Data Reuse"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Without Data Reuse"</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">mutate</span><span class="p">(</span><span class="n">data_reuse</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">data_reuse</span><span class="p">,</span><span class="w"> </span><span class="n">levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Without Data Reuse"</span><span class="p">,</span><span class="w"> </span><span class="s2">"With Data Reuse"</span><span class="p">),</span><span class="w">
                             </span><span class="n">ordered</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">rename</span><span class="p">(</span><span class="s2">"F-statistic"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">statistic</span><span class="p">,</span><span class="w"> </span><span class="s2">"p-value"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">p.value</span><span class="p">,</span><span class="w"> </span><span class="s2">"R squared"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">r.squared</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">gather</span><span class="p">(</span><span class="n">stat</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">repetition</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">data_reuse</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">ggplot</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">stat</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">value</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_violin</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">stat</span><span class="p">),</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"width"</span><span class="p">,</span><span class="w"> </span><span class="n">draw_quantiles</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.25</span><span class="p">,</span><span class="w"> </span><span class="m">0.5</span><span class="p">,</span><span class="w"> </span><span class="m">0.75</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_hline</span><span class="p">(</span><span class="n">yintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.3</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">data_reuse</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">theme_linedraw</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">ggtitle</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="n">n_rep</span><span class="p">,</span><span class="w"> </span><span class="s2">"repetitions of an LM fit with"</span><span class="p">,</span><span class="w"> </span><span class="n">n_row</span><span class="p">,</span><span class="w"> </span><span class="s2">"rows,"</span><span class="p">,</span><span class="w"> </span><span class="n">n_col</span><span class="p">,</span><span class="w"> </span><span class="s2">"columns"</span><span class="p">))</span></code></pre></figure>


  </div>

  <div class="date">
    Written on June  5, 2017
  </div>

  <div class="date">
    Tags:
		
		<a href="/tag/r">#r</a>
		
		<a href="/tag/math">#math</a>
		
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'agisga';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:alexej.go@gmail.com"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/alexej.yexela"><i class="svg-icon facebook"></i></a>

<a href="https://github.com/agisga"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/alexejgossmann"><i class="svg-icon linkedin"></i></a>

<a href="/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/agisga"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-94080131-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/Freedmans_paradox/',
		  'title': 'Freedman\'s paradox'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
