<!DOCTYPE html>
<html>
  <head>
    <title>"Testing Statistical Hypotheses" and "Theory of Point Estimation" impressions – Alexej Gossmann – I am interested in math, stats, coding, genetics, ...</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="
" />
    <meta property="og:description" content="
" />
    
    <meta name="author" content="Alexej Gossmann" />

    
    <meta property="og:title" content=""Testing Statistical Hypotheses" and "Theory of Point Estimation" impressions" />
    <meta property="twitter:title" content=""Testing Statistical Hypotheses" and "Theory of Point Estimation" impressions" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Alexej Gossmann - I am interested in math, stats, coding, genetics, ..." href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
 
    <!-- MathJax interation-->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"]]}});
      MathJax.Hub.Config({TeX: {Macros:{subscript:['_{#1}',1],superscript:['^{#1}',1]}}});
    </script> 
    <!-- Turn on equation numbering -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
    </script>
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
 </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://avatars0.githubusercontent.com/u/11449372?v=3&s=460" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Alexej Gossmann</a></h1>
            <p class="site-description">I am interested in math, stats, coding, genetics, ...</p>
          </div>

          <nav>
            <a href="/about">About</a>
            <a href="/">Blog</a>
            <a href="/talks">Presentations</a>
            <a href="/publications">Publications</a>
            <a href="/software">Software</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>"Testing Statistical Hypotheses" and "Theory of Point Estimation" impressions</h1>

  <div class="entry">
    <script type="text/javascript">
function toggleMe(a){
var e=document.getElementById(a);
if(!e)return true;
if(e.style.display=="none"){
e.style.display="block"
}
else{
e.style.display="none"
}
return true;
}
</script>

<p>I am currently reading Lehmann &amp; Romano &quot;Testing Statistical Hypotheses&quot; (3rd ed.) and Lehmann &amp; Casella &quot;Theory of Point Estimation&quot; (2nd ed.), abbr. TSH and TPE. The following is a collection of <del>random facts</del> observations I made while reading TSH and TPE.</p>

<p><img class='emoji' title=':construction_worker:' alt=':construction_worker:' src='https://assets.github.com/images/icons/emoji/unicode/1f477.png' height='20' width='20' align='absmiddle' /> <em>This post will be regularly updated.</em> <img class='emoji' title=':construction_worker:' alt=':construction_worker:' src='https://assets.github.com/images/icons/emoji/unicode/1f477.png' height='20' width='20' align='absmiddle' /></p>

<p><strong>Clickable items</strong> (each could as well be a separate post)<strong>:</strong></p>

<!-- 
* some things that were not clear to me before, 
* some results I found interesting, 
* definitions that are better than their equivalents that I have seen before,
* and other observations I made while reading TSH and TPE.

Brainstorming:

* MLE = posterior mode with a uniform prior; LASSO estimate = posterior mode with a Laplacian prior
* (informal?) fundamental Neyman-Pearson lemma and generalizations?
* something on linear hypotheses as chapter 7, combined with treatment in McCulloch, Searle, Neuhaus

-->

<p><br>
<input type="button" onclick="return toggleMe('unbiased')" value="+ Topic:"> <b>On the notion of unbiasedness of estimators, hypotheses tests, and confidence intervals</b><br>
<div id="unbiased" style="display:none"></p>

<h2>On the notion of unbiasedness of estimators, hypotheses tests, and confidence intervals</h2>

<p>The following discusses various well-known definitions of unbiasedness, their generalizations and relationships with each other, as well as some of the underlying intuition (such as the relationship between hypotheses tests and confidence intervals).</p>

<h3>Unbiased estimators</h3>

<p>The well-known and widely used definition of an unbiased estimator $\hat{\theta}$ of a parameter $\theta$ is</p>

<p>$$\mathrm{E}\subscript{\theta}(\hat{\theta}) = \theta.$$</p>

<p>However it can be generalized as follows. Assume that there is a loss function $L(\theta, \hat{\theta})$, which only depends on the correct parameter $\theta$ and the estimate $\hat{\theta}$ (i.e. it measures how far off the estimator is from the parameter that it aims to estimate).
Then $\hat{\theta}$ is said to be unbiased for $\theta$ with respect to $L$, if for all $\theta^\prime$ it holds that</p>

<p>$$\mathrm{E}\subscript{\theta}(L(\theta^\prime, \hat{\theta})) \geq \mathrm{E}\subscript{\theta}(L(\theta, \hat{\theta})).$$</p>

<p>That is, if $\hat{\theta}$ is on average closer to the correct parameter $\theta$ than to any wrong parameter $\theta^\prime$ in the parameter space.</p>

<p>When estimating a real valued $\theta$ with the square of the error as loss, the above condition becomes</p>

<p>$$\mathrm{E}\subscript{\theta}\left(\left| \theta^\prime - \hat{\theta} \right|^2\right) \geq \mathrm{E}\subscript{\theta}\left(\left| \theta - \hat{\theta}\right|^2\right).$$</p>

<p>If $\mathrm{E}\subscript{\theta}\hat{\theta}$ is one of the possible values of $\theta$, then by adding and subtracting $\mathrm{E}\subscript{\theta}\hat{\theta}$ inside the parentheses on both sides of the equation it follows that the above unbiasedness condition is satisfied if and only if</p>

<p>$$\mathrm{E}\subscript{\theta}(\hat{\theta}) = \theta.$$</p>

<p>This equivalence also holds under somewhat more general assumptions, see exercise 1.2 in TSH.</p>

<h3>Unbiased tests</h3>

<p>Consider a level $\alpha$ test $\phi$ of the hypothesis $H : \theta \in \Omega\subscript{H}$ against an alternative $K : \theta \in \Omega\subscript{K}$.
Denote the power function of $\phi$ by $\beta\subscript{\phi}(\theta) = \mathrm{E}\subscript{\theta} \phi(X)$.
Then it is natural to define unbiasedness of $\phi$ by the criterion</p>

<p>$$
\begin{eqnarray}
\nonumber
\beta\subscript{\phi}(\theta) &amp;\leq&amp; \alpha \quad \mathrm{if}\, H : \theta \in \Omega\subscript{H}, \\\
\beta\subscript{\phi}(\theta) &amp;\geq&amp; \alpha \quad \mathrm{if}\,  K : \theta \in \Omega\subscript{K}. 
\nonumber
\end{eqnarray}
$$</p>

<p>In particular, it follows that $\beta\subscript{\phi}(\theta) = \alpha$ on the common boundary of $\Omega\subscript{H}$ and $\Omega\subscript{K}$. In fact, a test that is the most powerful among all such tests, is UMP unbiased (Lemma 4.1.1 in TSH). </p>

<p>However, the definition of an unbiased test can be generalized in the same way as that of an unbiased estimator shown above.
Assume that there is a loss function $L(\theta, \phi(x))$, which only depends on the true value of $\theta$ and the decision $\phi(x)$ takes by the test $\phi$. Then the hypothesis test is unbiased with respect to $L$, if for all $\theta^\prime$ it holds that</p>

<p>$$\mathrm{E}\subscript{\theta}(L(\theta^\prime, \phi(X))) \geq \mathrm{E}\subscript{\theta}(L(\theta, \phi(X))).$$</p>

<p>For the test $\phi$ of $H$ vs. $K$ let the loss function be equal to $\alpha$ if a Type II error is committed and equal $(1-\alpha)$ if a Type I error is committed. Then </p>

<p>$$
\mathrm{E}\subscript{\theta}(L(\theta^\prime, \phi(X))) = 
\begin{cases}
\alpha (1 - \beta\subscript{\phi}(\theta)) \quad &amp;\mathrm{if}&amp;\, \theta^\prime \in \Omega\subscript{K}\\\ 
(1-\alpha) \beta\subscript{\phi}(\theta) \quad &amp;\mathrm{if}&amp;\, \theta^\prime \in \Omega\subscript{H},
\end{cases}
$$</p>

<p>It follows that if $\theta \in \Omega\subscript{H}$ then $\alpha (1 - \beta\subscript{\phi}(\theta)) \geq (1-\alpha) \beta\subscript{\phi}(\theta)$, and consequently</p>

<p>$$\beta\subscript{\phi}(\theta) \leq \alpha.$$</p>

<p>Similarly, by considering $\theta\in\Omega\subscript{K}$, we get $\beta\subscript{\phi}(\theta) \geq \alpha$. Thus the usual definition is a special case of the more general loss-function-based definition.</p>

<h3>Unbiased confidence sets</h3>

<p>As is well-known, the defining condition for a confidence interval $\left(\underline{\theta}, \overline{\theta}\right)$ is</p>

<p>$$P\subscript{\theta}\left(\underline{\theta}(X) \leq \theta \leq \overline{\theta}(X)\right) \geq 1-\alpha,$$</p>

<p>for all $\theta$.</p>

<h4>Hypotheses tests vs. confidence intervals</h4>

<p>It is well-known that hypotheses tests and confidence intervals generally do exactly the same thing.
However, to describe with mathematical rigour in what sense it is true requires a little thinking.</p>

<p>Consider a level $\alpha$ test of a two-sided hypothesis test $H : \theta = \theta\subscript{0}$ vs. $K : \theta \neq \theta\subscript{0}$, and denote its acceptance region by $A(\theta\subscript{0})$.
Define the inclusion region of the confidence set to be</p>

<p>$$S(x) := \{ \theta : x\in A(\theta) \},$$</p>

<p>that is, $\theta \in S(x)$ if and only if $x\in A(\theta)$. Then $S(x)$ defines a $(1-\alpha) \cdot 100\%$ confidence set, because for all $\theta$ we have</p>

<p>$$P\subscript{\theta}(\theta \in S(x)) = P\subscript{\theta}(x\in A(\theta)) \geq 1 - \alpha.$$</p>

<p>Conversely, if we start out with a family of confidence sets $\{S(x) : x\in\mathcal{X}\}$, and define $A(\theta) := \{x : \theta\in S(x)\}$, then for any $\theta$ it holds that</p>

<p>$$P\subscript{\theta}(x\in A(\theta)) = P\subscript{\theta}(\theta \in S(x)) \geq 1 - \alpha.$$</p>

<p>It follows that $P\subscript{\theta}(\mathrm{Type\,I\,error}) \leq \alpha$, that is, $A(\theta)$ is the acceptance region of a level $\alpha$ test.</p>

<h4>Unbiased and uniformly most accurate unbiased confidence sets</h4>

<p>Now it suggests itself to define an unbiased confidence set as one that stems from an unbiased hypothesis test by the above procedure. 
In the two-sided case discussed above this condition reduces to</p>

<p>$$P\subscript{\theta}\left(\underline{\theta}(X) \leq \theta^\prime \leq \overline{\theta}(X)\right) \leq 1 - \alpha$$</p>

<p>for all $\theta^\prime$ and $\theta$ such that $\theta \neq \theta^\prime$. That is, the inclusion probability of the null hypothesis parameter $\theta^\prime$ in the confidence interval, when the alternative $\theta$ is true, is less than the confidence level. Lemma 5.5.1 in TSH shows that the confidence set derived from an unbiased level $\alpha$ hypothesis test has indeed the form of an interval.</p>

<p>Similarly, uniformly most accurate confidence intervals correspond to uniformly most powerful tests (see section 3.5 in TSH for more detail).
However, UMP tests usually do not exist, which is a reason to concentrate on unbiasedness instead. In particular, UMP unbiased tests correspond to uniformly most accurate unbiased confidence sets, i.e.  $S(x)$ such that for all $\theta^\prime$ and $\theta$ with $\theta\in K(\theta^\prime)$ the probability $P\subscript{\theta}(\theta^\prime\in S(x))$ is minimized.
</div></p>

<p><br>
<input type="button" onclick="return toggleMe('para2')" value="+ Topic:"> <b>Conditional expectation, conditional distribution, sufficiency, decision procedures</b><br>
<div id="para2" style="display:none"></p>

<h2>Conditional expectation, conditional distribution, sufficiency, decision procedures</h2>

<p>Consider a random variable $X$ with sample space $(\mathcal{X}, \mathcal{A})$ and probability distribution $P^X$, and a statistic $T(X)$ with range space $(\mathcal{T}, \mathcal{B})$.</p>

<h4>Definition [$\mathrm{E}(f(X)|t)$]</h4>

<p><em>Let $f(x)$ be a non-negative, $\mathcal{A}$-measurable and $P^X$-integrable function.
A $\mathcal{B}$-measurable function $g(t)$ is the conditional expectation of $X$ for given $t$, i.e. $\mathrm{E}(f(X)|t) = \mathrm{E}(f(X)|T=t) = g(t)$, if for all sets $B\in\mathcal{B}$ it holds that</em></p>

<p>$$\int\subscript{T^{-1}(B)} f(x) dP^X(x) = \int\subscript{B} g(t) dP^T(t).$$</p>

<p>Some observations regarding this definition:</p>

<ul>
<li><p>In fact, if we define $f\subscript{0}(x) = g(T(x))$, then by Lemma 2.3.2 in TSH the above formula becomes</p>

<p>$$\int\subscript{A} f(x) dP^X(x) = \int\subscript{A} f\subscript{0}(x) dP^X(x), \forall A \in \mathcal{A}\subscript{0},$$</p>

<p>where $\mathcal{A}\subscript{0}$ is the $\sigma$-algebra induced by $T$.</p></li>
<li><p>The existence and uniqueness $(\mathcal{A}\subscript{0}, P^X)$ of such a function $f\subscript{0}$ follows from Radon-Nikodym Theorem.</p></li>
<li><p>If $f$ is not non-negative, then we can use the usual decomposition $f = f^+ - f^-$ and define</p>

<p>$$\mathrm{E}(f(X)|t) = \mathrm{E}(f^+(X)|t) - \mathrm{E}(f^-(X)|t).$$</p></li>
</ul>

<h4>Definition [$P(A|t)$]</h4>

<p><em>Let $I\subscript{A}(X)$ be a random variable that is equal to one if and only if $X\in A$. The conditional probability of $A$ given $T=t$ can be defined as</em></p>

<p>$$P(A|t) = E(I\subscript{A}(X) | t).$$</p>

<p>This definition seems natural, and in fact, if $T$ has Euclidean domain and range spaces or if $\mathrm{E}|f(X)| &lt; \infty$, then the above defines the <em>conditional probability distribution</em> $P^{X|t}$ (see Theorems 2.5.2 and 2.5.3 in TSH).</p>

<h4>Definition [Sufficiency]</h4>

<p><em>Let $\mathcal{P} = \{P\subscript{\theta} : \theta\in\Omega\}$ be a family of distributions over a sample space $(\mathcal{X}, \mathcal{A})$.</em>
<em>$T$ is sufficient for $\mathcal{P}$ (or $\theta$) if $P\subscript{\theta}(A|t)$ is independent of $\theta$ for every $A\in\mathcal{A}$.</em></p>

<p>In particular, the class of decision procedures depending on a sufficient statistic $T$ is <em>essentially complete</em>. To see this, assume that the sample space is Euclidean, then by Theorem 2.5.1 in TSH there exists the conditional probability distribution $P^{X|t}$. Let $\phi(x)$ be a decision procedure. Given only the value of the sufficient statistic $T(X)$ (but not $X$), define another decision procedure $\psi(t)$ as a random sample from the distribution $P^{X|t}$. Then $\phi(X)$ and $\psi(T)$ have identical distributions. Consequently, both decision procedures have the same risk,</p>

<p>$$R(\theta, \psi) = \mathrm{E}(L(\theta, \psi(T))) = \mathrm{E}(L(\theta, \phi(X))) = R(\theta, \phi).$$</p>

<p>Thus, for any decision procedure that is based on $X$, there is a decision procedure based on $T$ that is equally good or better.</p>

<p>For a proof in the general (non-Euclidean) case see exercise 2.13 in TSH.</p>

<h3>General conditional expectation</h3>

<p>Let $X$ and $Y$ be two real-valued random variables, which can be written as mappings $X: \Omega \to \mathbb{R}$ and $Y: \Omega \to \mathbb{R}$ over a measurable space $(\Omega, \mathcal{A}, P)$. The above definition of $\mathrm{E}(X|T(X)=t)$ suggests a similar definition of $\mathrm{E}(X|Y=y)$. Namely, $\mathrm{E}(X|Y=y) = g(y)$ if for all Borel sets $A$ it holds that</p>

<p>$$\int\subscript{Y^{-1}(A)} X(\omega) dP(\omega) = \int\subscript{A} g(y) dP^Y(y).$$</p>

<p>In fact, a more general version of this definition is given in Feller&#39;s &quot;An Introduction to Probability Theory and its Applications. Volume II&quot; (10.6) as,</p>

<p>$$\mathrm{E}(X\cdot I\subscript{A}(Y)) = \int\subscript{A} \mathrm{E}(X | y) \mu\{dy\},$$</p>

<p>for any pair of random variables $X$ and $Y$.</p>

<p>If $X$ and $Y$ are real-valued one-dimensional, then the pair $(X,Y)$ can be viewed as a random vector in the plane. Each set $\{Y \in A\}$ consists of parallels to the $x$-axis, and we can define a $\sigma$-algebra induced by $Y$ as the collection of all sets $\{Y \in A\}$, where $A$ are Borel sets. Then $\mathrm{E}(X|Y)$ is a random variable, such that $\mathrm{E}(X\cdot I\subscript{B}) = \mathrm{E}(\mathrm{E}(X|Y) \cdot I\subscript{B})$ for all $B=\{Y\in A\}$ with $A$ being a Borel set. This leads to the following general definition.</p>

<h4>Definition [Conditional expectation]</h4>

<p><em>Let $\mathcal{A}$ be the underlying $\sigma$-algebra of sets, and let $\mathcal{B}$ be a $\sigma$-algebra contained in $\mathcal{A}$. Let $X$ be a random variable.</em></p>

<ol>
<li><p><em>A random variable $U$ is called a conditional expectation of $X$ relative to $\mathcal{B}$ (or $U=\mathrm{E}(X|\mathcal{B})$), if it is $\mathcal{B}$-measurable and for all $B\in\mathcal{B}$ it holds that</em></p>

<p>$$\mathrm{E}(X\cdot I\subscript{B}) = \mathrm{E}(U \cdot I\subscript{B}).$$</p></li>
<li><p><em>If $\mathcal{B}$ is the $\sigma$-algebra generated by a random variable $Y$, then $\mathrm{E}(X|Y) = \mathrm{E}(X|\mathcal{B})$.</em></p></li>
</ol>

<p></div></p>

<p><br>
<input type="button" onclick="return toggleMe('neyman-pearson')" value="+ Topic:"> <b>An informal summary of Neyman-Pearson and generalizations</b><br>
<div id="neyman-pearson" style="display:none"></p>

<h2>An informal summary of Neyman-Pearson and generalizations</h2>

<p>The following offers an informal view on the fundamental lemma of Neyman and Pearson and generalizations thereof.
For a mathematically rigorous presentation see the corresponding results in TSH, which are cited in this article.</p>

<p><em>Notation</em>: MP = &quot;most powerful&quot;, UMP = &quot;uniformly most powerful&quot;, $H$ denotes the null hypothesis, $K$ denotes the alternative hypothesis, $\alpha$ denotes the level of the hypothesis test, lower case Roman letters denote realizations of random variables (upper case).</p>

<ol>
<li><p>What is actually called fundamental lemma of Neyman and Pearson in TSH (Theorem 3.2.1) is concerned with a test of two simple hypotheses. Under the null hypothesis the random variable $X$ is assumed to follow a probability distribution with density $p\subscript{0}$, while under the alternative hypothesis the density is $p\subscript{1}$. </p>

<p>Consider $H : p\subscript{0}$ vs. $K : p\subscript{1}$. MP test $\phi$ exists. It rejects the null if $\frac{p\subscript{1}}{p\subscript{0}} &gt; k$, accepts the null if $\frac{p\subscript{1}}{p\subscript{0}} &lt; k$, and rejects with probability $\gamma$ if $\frac{p\subscript{1}}{p\subscript{0}} = k$, where $\gamma$ and $k$ are chosen to satisfy $\mathrm{E}\subscript{p\subscript{0}} \phi(X) = \alpha$.</p></li>
<li><p>For one-parameter families of distributions and one-sided hypotheses, the Neyman-Pearson lemma can be generalized to construct a UMP test if the distributions in question have monotone likelihood ratios. This is Theorem 3.4.1 in TSH.</p>

<p>Consider $H : \theta \leq \theta\subscript{0}$ vs. $K : \theta &gt; \theta\subscript{0}$ ($\theta \in \mathbb{R}$). If $\frac{p\subscript{\theta^\prime}(x)}{p\subscript{\theta}(x)}$ is nondecreasing in $T(x)$ for any $\theta &lt; \theta^\prime$, then a UMP test $\phi$ exists. It rejects if $T(x) &gt; C$, accepts if $T(x) &lt; C$, and rejects with probability $\gamma$ if $T(x) = C$, where $C$ and $\gamma$ are determined by $\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha$.</p>

<p>By interchanging the inequalities one obtains a UMP test for the dual problem $H : \theta \geq \theta\subscript{0}$ vs. $K : \theta &lt; \theta\subscript{0}$.</p>

<p>Additionally, this test minimizes the Type I error subject to $\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha$.</p></li>
<li><p>An analogous UMP test exists for a two-sided null hypothesis $H : \theta \leq \theta\subscript{1} \,\mathrm{or}\, \theta \geq \theta\subscript{2}$ in one-parameter exponential families. It rejects if $C\subscript{1} &lt; T(x) &lt; C\subscript{2}$, accepts if $T(x) &lt; C\subscript{1}$ or $T(x) &gt; C\subscript{2}$, rejects with probability $\gamma$ if $T(x) = C\subscript{i}$ (for $i=1$ or $i=2$), and satisfies $\mathrm{E}\subscript{\theta\subscript{1}} \phi(X) = \alpha = \mathrm{E}\subscript{\theta\subscript{2}} \phi(X)$. Subject to the last condition, this test minimizes the Type I error. See Theorem 3.7.1 in TSH.</p>

<p>A UMP test for a two-sided alternative hypothesis $K : \theta \leq \theta\subscript{1} \,\mathrm{or}\, \theta \geq \theta\subscript{2}$ does not exist (e.g. see the corresponding section in this blog post). However, a UMP <em>unbiased</em> test analogous to the above exists (see Section 4.2 in TSH).</p></li>
<li><p>When the null hypothesis specifies that $X$ is distributed according to one of finitely many densities $p\subscript{1}, p\subscript{2}, \dots, p\subscript{m}$, and the alternative hypothesis is $p\subscript{m+1}$, then there exists a test $\phi$ that maximizes $\int \phi p\subscript{m+1} d\mu$. For suitable constants $k\subscript{1}, k\subscript{2}, \dots, k\subscript{m}$, this test rejects the null if $p\subscript{m+1}(x) &gt; \sum\subscript{i=1}^m k\subscript{i} p\subscript{i}(x)$, it accepts the null if $p\subscript{m+1}(x) &lt; \sum\subscript{i=1}^m k\subscript{i} p\subscript{i}(x)$, and it satisfies $\int \phi p\subscript{i} d\mu \leq \alpha$ for $i = 1,2,\dots,m$.</p>

<p>See Theorem 3.6.1 and Corollary 3.6.1 in TSH for more detail.</p></li>
<li><p>Assume a setting similar to the one in the last point, except that the number of distributions under the null hypothesis does not need to be finite. That is, $H : f\subscript{\theta}, \theta \in \omega$ vs. $K : g$.
One can define a <em>least favorable</em> distribution $\Lambda$ over $\omega$ and assume that $\theta \sim \Lambda$. As $\Lambda$ is least favorable, one can expect that it leads to a hypothesis test that works best in the works case (i.e. at values $\theta$ closest to $K$). Thus, $\Lambda$ will typically be a distribution of $H$ that is closest to $K$. In particular, one would have $\Lambda(\omega^\prime) = 1$ for some &quot;boundary region&quot; of $\omega$. Then a MP test $\phi$ exists. It rejects if $g(x) &gt; k \int f\subscript{\theta}(x) d\Lambda(\theta)$, accepts if $g(x) &lt; k \int f\subscript{\theta}(x) d\Lambda(\theta)$, and satisfies $\sup\subscript{\theta\in\omega} \mathrm{E}\subscript{\theta} \phi(X) = \alpha$.</p>

<p>See Theorem 3.8.1 and Corollary 3.8.1 in TSH for rigour and detail.</p></li>
<li><p>For multi-parameter exponential families the existence of a UMP test typically cannot be established. However, UMP <em>unbiased</em> tests can be constructed without great difficulties. Assume that $\theta\in\mathbb{R}$ is the parameter to be tested, and that $(U, T)$ is a sufficient statistic, where $U$ corresponds to $\theta$ and $T$ corresponds to all other parameters. Then UMP unbiased tests exist for most of the usual hypotheses, and can be written in the same way as in the one-parameter case, except that now all constants specifying the rejection region depend on $T$ (e.g. the rejection rule has the form $u &gt; C(t)$, etc.). Also, the size of the test is measured conditional on $T$.</p>

<p>See Theorem 4.4.1 in TSH.</p></li>
<li><p>UMP unbiased tests for multi-parameter exponential families, as discussed in the last point, are independent of $T$ if a number of additional conditions are satisfied. For example, assume that $V = h(U, T)$ is independent of $T$ (with $\theta = \theta\subscript{1}$ and $\theta = \theta\subscript{2}$) and that $h$ is increasing in $u$. Then a UMP unbiased test for a two-sided null hypothesis rejects if $C\subscript{1} &lt; v &lt; C\subscript{2}$, accepts if $v &lt; C\subscript{1}$ or $v &gt; C\subscript{2}$, etc.</p>

<p>See Theorem 5.1.1 in TSH for more.</p></li>
<li><p>If the problem of testing $H : \Omega\subscript{0}$ vs. $K : \Omega\subscript{1}$ remains invariant under a finite group $G = \{g\subscript{1}, g\subscript{2}, \dots, g\subscript{N} \}$, then there exists a UMP invariant test that rejects when $\frac{\sum p\subscript{\overline{g}\subscript{i} \theta\subscript{1}} (x)}{\sum p\subscript{\overline{g}\subscript{i} \theta\subscript{0}} (x)} &gt; C$ (for any $\theta\subscript{0} \in \Omega\subscript{0}$ and any $\theta\subscript{1}$ in $\Omega\subscript{1}$). See Theorem 6.3.1 in TSH.</p></li>
</ol>

<p></div></p>

<p><br>
<input type="button" onclick="return toggleMe('permutation')" value="+ Topic:"> <b>Permutation tests</b><br>
<div id="permutation" style="display:none"></p>

<h2>Permutation tests</h2>

<p>When a parametric probabilistic model cannot be assumed, one can still construct exact level-$\alpha$ hypotheses tests as permutation tests. Here, based on sections 5.8 and 5.9 of TSH, I discuss the concept by considering as an example a permutation test for the difference of two means. </p>

<p>Assume that each of the random variables $X\subscript{1}, \dots, X\subscript{m}$ has mean $\eta$ and that each of $Y\subscript{1}, ..., Y\subscript{n}$ has mean $\xi$. Additionally assume that the distributions of all those variables differ only with respect to the mean, for example, $X\subscript{i} \sim \mathrm{i.i.d.}\, f(x\subscript{i})$ and $Y\subscript{i} \sim \mathrm{i.i.d.}\, f(y\subscript{i} - \Delta)$ with $\Delta = \eta - \xi$. The density function $f$ is not known apart from the fact that it is continuous a.e. We want to test the hypothesis $H : \Delta = 0$.</p>

<p>Let $N:=n+m$, denote the random vector containing all $X$s and $Y$s as $Z := (X^T, Y^T)^T$, and let $S(z)$ be the set of all permutations of the entries of a realization $z$ of the random vector $Z$.
Then a level-$\alpha$ test $\phi$ has to satisfy</p>

<p>$$\int \phi(z) \prod\subscript{i=1}^N f(z\subscript{i}) dz = \alpha.$$</p>

<p>Interestingly, it turns out that this equality holds if and only if</p>

<p>$$\frac{1}{N!} \sum\subscript{w\in S(z)} \phi(w) = \alpha.$$</p>

<p>A more general result that accounts for population stratification is given by theorem 5.8.1 in TSH.</p>

<p>The power of $\phi$  against an alternative $h(z)$ is given by</p>

<p>$$\int \phi(z) h(z) dz = \int \mathrm{E}\left(\phi(Z) \middle| T=t\right) dP^T(t).$$</p>

<p>Let $T(Z)$ be the order statistic. It holds that $S(z) = S(T(z)) = S(t)$, and from the expression of the conditional expectation $\mathrm{E}\left(\phi(Z) \middle| T=t\right)$ (see Example 2.4.1 and Problem 2.6), it can be further derived that the most powerful test $\phi$ maximizes</p>

<p>$$\sum\subscript{z\in S(t)} \phi(z) \frac{h(z)}{\sum\subscript{w\in S(z)} h(w)}$$</p>

<p>subject to</p>

<p>$$\frac{1}{N!} \sum\subscript{z\in S(t)} \phi(z) = \alpha.$$</p>

<p>Now, the Neyman-Pearson fundamental lemma implies that the hypothesis should be rejected whenever $\frac{h(z)N!}{\sum\subscript{w\in S(z)} h(w)}$ is too large.
This leads to a most powerful test $\phi$ given by</p>

<p>$$\phi(z) = \begin{cases}
1, \quad\mathrm{if}\, h(z) &gt; C(T(z)), \\\
\gamma, \quad\mathrm{if}\, h(z) = C(T(z)), \\\
0, \quad\mathrm{if}\, h(z) &lt; C(T(z)).
\end{cases}$$</p>

<p>Thus the test is carried out by... </p>

<ol>
<li>ordering the points in $S(z)$ in a decreasing order according to $h$,</li>
<li>rejecting if $h(z)$ is one of the $k$ largest values and rejecting with probability $\gamma$ if $h(z)$ is $(k+1)$st largest, where $k$ and $\gamma$ are determined by</li>
</ol>

<p>$$k+\gamma = \alpha \cdot N!$$</p>

<p>More general versions of this approach, which incorporate population stratification and randomization, are given in section 5.8-5.13 in TSH.</p>

<p>The above test is not UMP because it depends on $h$. However, it can be shown that if under the null hypothesis each $Z\subscript{i}$ follows the same normal distribution $\mathcal{N}(\xi, \sigma^2)$, then the derived test is most powerful among all unbiased tests of level $\alpha$ against all normal alternatives under consideration (see Lemma 5.9.1 in TSH for an even more general result).
Such an approach is appropriate when the data is assumed to be approximately normal but the assumption is not considered reliable. The permutation test is maximizing the power against all normal alternatives, while still being unbiased against all other alternatives.</p>

<p></div></p>

<p><br>
<input type="button" onclick="return toggleMe('two-sided')" value="+ Topic:"> <b>UMP tests for two-sided hypotheses</b><br>
<div id="two-sided" style="display:none"></p>

<h2>UMP tests for two-sided hypotheses</h2>

<p>The (non-) existence of uniformly most powerful (or UMP) tests for two-sided hypotheses is an interesting phenomenon. </p>

<h4>Example of existence</h4>

<p>First let&#39;s look at an example when such a test does exist. This is Problem 3.2 in TSH.</p>

<p>For $i = 1,\dots, n$ let $X\subscript{i}$ be i.i.d. $\mathrm{Uniform}(0,\theta)$ random variables, denote their realizations by lower case $x\subscript{i}$s, and let $X$ denote the vector of the $X\subscript{i}$s. Consider the hypothesis $H : \theta = \theta\subscript{0}$ against the alternative $K : \theta \neq \theta\subscript{0}$.</p>

<p>Denote $x\subscript{(n)} := \max\{x\subscript{1}, \dots, x\subscript{n}\}$. Let $\phi$ be a hypothesis test which rejects $H : \theta = \theta\subscript{0}$ in favor of a two-sided alternative, if either $x\subscript{(n)} \geq \theta\subscript{0}$ or $x\subscript{(n)} &lt; \theta\subscript{0} \sqrt[n]{\alpha}$.</p>

<h5>Proof</h5>

<p>Using the fundamental lemma of Neyman and Pearson, it is straightforward to prove that $\phi$ is UMP. Namely, $\phi$ is a UMP test at level $\alpha$ by Neyman-Pearson, if for any fixed $\theta\subscript{1} \neq \theta\subscript{0}$, the test $\phi$ can be written as </p>

<p>$$\phi(x) = \begin{cases} 
1, \quad &amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &gt; k p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &lt; k p\subscript{\theta\subscript{0}}(x), 
\end{cases}$$</p>

<p>with a suitable $k$, and if it satisfies</p>

<p>$$\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha.$$</p>

<p>We have that</p>

<p>$$\begin{eqnarray}
\nonumber
\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) &amp;=&amp; P\subscript{\theta\subscript{0}}\left(X\subscript{(n)} &gt; \theta\subscript{0}\right) + P\subscript{\theta\subscript{0}}\left(X\subscript{(n)} &lt; \theta\subscript{0}\sqrt[n]{\alpha}\right)\\\
&amp;=&amp; 0 + \left(\frac{\theta\subscript{0} \sqrt[n]{\alpha}}{\theta\subscript{0}}\right)^n = \alpha.
\nonumber
\end{eqnarray}$$</p>

<p>As for the other Neyman-Pearson condition, we have to consider multiple cases:</p>

<ul>
<li>If $\theta\subscript{1} &gt; \theta\subscript{0}$, then $k = \left(\frac{\theta\subscript{0}}{\theta\subscript{1}}\right)^n$ yields the desired result.</li>
<li>If $\theta\subscript{0}\sqrt[n]{\alpha} &lt; \theta\subscript{1} &lt; \theta\subscript{0}$, then $k = \left(\frac{\theta\subscript{0}}{\theta\subscript{1}}\right)^n$ can be used as well.</li>
<li>If $\theta\subscript{1} &lt; \theta\subscript{0}\sqrt[n]{\alpha} &lt; \theta\subscript{0}$, then $k = 0$.</li>
</ul>

<div align="right">
$\blacksquare$
</div>

<h4>Example of non-existence</h4>

<p>Thus, we saw an example of a UMP test for a two-sided hypothesis. </p>

<p>However, when the underlying distribution comes from an exponential family, then a UMP test does not exist for $H : \theta = \theta\subscript{0}$ vs. $K : \theta \neq \theta\subscript{0}$ (Problem 3.54 in TSH). This follows quite easily from the consideration of UMP tests for the one-sided hypotheses $H\subscript{1} : \theta \leq \theta\subscript{0}$ vs. $K\subscript{1} : \theta &gt; \theta\subscript{0}$, and $H\subscript{2} : \theta \geq \theta\subscript{0}$ vs. $K\subscript{2} : \theta &lt; \theta\subscript{0}$.
A detailed proof follows.</p>

<h5>Proof</h5>

<p>According to Theorem 3.4.1 in TSH, a UMP test of $H\subscript{1}$ exists and can be written as</p>

<p>$$\phi\subscript{1}(x) = \begin{cases} 
1, \quad &amp;\mathrm{if}\, T(x) &gt; C\subscript{1},\\\
0, \quad &amp;\mathrm{if}\, T(x) &lt; C\subscript{1}.
\end{cases}$$</p>

<p>Similarly, a UMP test of $H\subscript{2}$ exists and can be written as</p>

<p>$$\phi\subscript{2}(x) = \begin{cases} 
1, \quad &amp;\mathrm{if}\, T(x) &lt; C\subscript{2},\\\
0, \quad &amp;\mathrm{if}\, T(x) &gt; C\subscript{2}.
\end{cases}$$</p>

<p>Clearly, $\phi\subscript{1}$ and $\phi\subscript{2}$ are level-$\alpha$ tests for $H$ vs. $K$ as well.</p>

<p>Let $\phi\subscript{0}$ be a level-$\alpha$ test of $H$ vs. $K$. Fix a $\theta\subscript{1} &gt; \theta\subscript{0}$ and a $\theta\subscript{2} &lt; \theta\subscript{0}$. Assume that </p>

<p>$$\mathrm{E}\subscript{\theta\subscript{i}} \phi\subscript{0}(X) \geq \mathrm{E}\subscript{\theta\subscript{i}} \phi\subscript{i}(X)$$</p>

<p>for $i = 1,2$. Then $\phi\subscript{0}$ is most powerful for testing $\theta\subscript{0}$ vs. $\theta\subscript{1}$ and for testing $\theta\subscript{0}$ vs. $\theta\subscript{2}$. Thus, by the fundamental lemma of Neyman and Pearson the UMP test can be rewritten as</p>

<p>$$
\begin{equation}
\phi\subscript{0}(x) = \begin{cases} 
1, \quad &amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &gt; k\subscript{1} p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &lt; k\subscript{1} p\subscript{\theta\subscript{0}}(x), 
\end{cases}
\label{eq1}
\end{equation}
$$</p>

<p>$$
\begin{equation}
\phi\subscript{0}(x) = \begin{cases} 
1, \quad &amp;\mathrm{if}\, p\subscript{\theta\subscript{2}}(x) &gt; k\subscript{2} p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;\mathrm{if}\, p\subscript{\theta\subscript{2}}(x) &lt; k\subscript{2} p\subscript{\theta\subscript{0}}(x).
\end{cases}
\label{eq2}
\end{equation}
$$</p>

<p>Let $x$ be such that $\phi\subscript{0}(x) = 1$. Now, from the monotonicity of the likelihood ratio, it follows that</p>

<ul>
<li>if $T(y) &gt; T(x)$ then $\phi\subscript{0}(y) = 1$ (by equation $\eqref{eq1}$),</li>
<li>if $T(y) &lt; T(x)$ then $\phi\subscript{0}(y) = 1$ (by equation $\eqref{eq2}$).</li>
</ul>

<p>That is, either $\phi\subscript{0}(y) = 1$ for all $y$ or $\phi\subscript{0}(x) \neq 1$ for all $x$. A contradiction. It follows that $\phi\subscript{0}$ can not be more powerful than $\phi\subscript{1}$ for testing $\theta\subscript{0}$ vs. $\theta\subscript{1}$ and than $\phi\subscript{2}$ for testing $\theta\subscript{0}$ vs. $\theta\subscript{2}$. Thus, a UMP test for $H$ vs. $K$ does not exist.</p>

<div align="right">
$\blacksquare$
</div>

<p>Even though a UMP test for the two-sided hypothesis considered above does not exist, there exist a UMP unbiased test (i.e. a test that is uniformly most powerful among all unbiased tests). For detail see Section 4.2 in TSH.</p>

<p></div></p>

  </div>

  <div class="date">
    Written on October 29, 2015
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'agisga';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          



<a href="http://github.com/agisga"><i class="svg-icon github"></i></a>

<a href="http://linkedin.com/in/alexejgossmann"><i class="svg-icon linkedin"></i></a>


<a href="http://twitter.com/agisga"><i class="svg-icon twitter"></i></a>


        </footer>
      </div>
    </div>

    

  </body>
</html>
