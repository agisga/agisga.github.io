<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alexej Gossmann</title>
    <description>I am interested in math, stats, coding, genetics, ...</description>
    <link>http://www.alexejgossmann.com/</link>
    <atom:link href="http://www.alexejgossmann.com//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>&quot;Testing Statistical Hypotheses&quot; and &quot;Theory of Point Estimation&quot; impressions</title>
        <description>&lt;script type=&quot;text/javascript&quot;&gt;
function toggleMe(a){
var e=document.getElementById(a);
if(!e)return true;
if(e.style.display==&quot;none&quot;){
e.style.display=&quot;block&quot;
}
else{
e.style.display=&quot;none&quot;
}
return true;
}
&lt;/script&gt;

&lt;p&gt;I am currently reading Lehmann &amp;amp; Romano &amp;quot;Testing Statistical Hypotheses&amp;quot; (3rd ed.) and Lehmann &amp;amp; Casella &amp;quot;Theory of Point Estimation&amp;quot; (2nd ed.), abbr. TSH and TPE. The following is a collection of &lt;del&gt;random facts&lt;/del&gt; observations I made while reading TSH and TPE.&lt;/p&gt;

&lt;p&gt;&lt;img class=&#39;emoji&#39; title=&#39;:construction_worker:&#39; alt=&#39;:construction_worker:&#39; src=&#39;https://assets.github.com/images/icons/emoji/unicode/1f477.png&#39; height=&#39;20&#39; width=&#39;20&#39; align=&#39;absmiddle&#39; /&gt; &lt;em&gt;This post will be regularly updated.&lt;/em&gt; &lt;img class=&#39;emoji&#39; title=&#39;:construction_worker:&#39; alt=&#39;:construction_worker:&#39; src=&#39;https://assets.github.com/images/icons/emoji/unicode/1f477.png&#39; height=&#39;20&#39; width=&#39;20&#39; align=&#39;absmiddle&#39; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clickable items&lt;/strong&gt; (each could as well be a separate post)&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt;

&lt;!-- 
* some things that were not clear to me before, 
* some results I found interesting, 
* definitions that are better than their equivalents that I have seen before,
* and other observations I made while reading TSH and TPE.

Brainstorming:

* MLE = posterior mode with a uniform prior; LASSO estimate = posterior mode with a Laplacian prior
* (informal?) fundamental Neyman-Pearson lemma and generalizations?
* something on linear hypotheses as chapter 7, combined with treatment in McCulloch, Searle, Neuhaus

--&gt;

&lt;p&gt;&lt;br&gt;
&lt;input type=&quot;button&quot; onclick=&quot;return toggleMe(&#39;unbiased&#39;)&quot; value=&quot;+ Topic:&quot;&gt; &lt;b&gt;On the notion of unbiasedness of estimators, hypotheses tests, and confidence intervals&lt;/b&gt;&lt;br&gt;
&lt;div id=&quot;unbiased&quot; style=&quot;display:none&quot;&gt;&lt;/p&gt;

&lt;h2&gt;On the notion of unbiasedness of estimators, hypotheses tests, and confidence intervals&lt;/h2&gt;

&lt;p&gt;The following discusses various well-known definitions of unbiasedness, their generalizations and relationships with each other, as well as some of the underlying intuition (such as the relationship between hypotheses tests and confidence intervals).&lt;/p&gt;

&lt;h3&gt;Unbiased estimators&lt;/h3&gt;

&lt;p&gt;The well-known and widely used definition of an unbiased estimator $\hat{\theta}$ of a parameter $\theta$ is&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}(\hat{\theta}) = \theta.$$&lt;/p&gt;

&lt;p&gt;However it can be generalized as follows. Assume that there is a loss function $L(\theta, \hat{\theta})$, which only depends on the correct parameter $\theta$ and the estimate $\hat{\theta}$ (i.e. it measures how far off the estimator is from the parameter that it aims to estimate).
Then $\hat{\theta}$ is said to be unbiased for $\theta$ with respect to $L$, if for all $\theta^\prime$ it holds that&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}(L(\theta^\prime, \hat{\theta})) \geq \mathrm{E}\subscript{\theta}(L(\theta, \hat{\theta})).$$&lt;/p&gt;

&lt;p&gt;That is, if $\hat{\theta}$ is on average closer to the correct parameter $\theta$ than to any wrong parameter $\theta^\prime$ in the parameter space.&lt;/p&gt;

&lt;p&gt;When estimating a real valued $\theta$ with the square of the error as loss, the above condition becomes&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}\left(\left| \theta^\prime - \hat{\theta} \right|^2\right) \geq \mathrm{E}\subscript{\theta}\left(\left| \theta - \hat{\theta}\right|^2\right).$$&lt;/p&gt;

&lt;p&gt;If $\mathrm{E}\subscript{\theta}\hat{\theta}$ is one of the possible values of $\theta$, then by adding and subtracting $\mathrm{E}\subscript{\theta}\hat{\theta}$ inside the parentheses on both sides of the equation it follows that the above unbiasedness condition is satisfied if and only if&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}(\hat{\theta}) = \theta.$$&lt;/p&gt;

&lt;p&gt;This equivalence also holds under somewhat more general assumptions, see exercise 1.2 in TSH.&lt;/p&gt;

&lt;h3&gt;Unbiased tests&lt;/h3&gt;

&lt;p&gt;Consider a level $\alpha$ test $\phi$ of the hypothesis $H : \theta \in \Omega\subscript{H}$ against an alternative $K : \theta \in \Omega\subscript{K}$.
Denote the power function of $\phi$ by $\beta\subscript{\phi}(\theta) = \mathrm{E}\subscript{\theta} \phi(X)$.
Then it is natural to define unbiasedness of $\phi$ by the criterion&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\nonumber
\beta\subscript{\phi}(\theta) &amp;amp;\leq&amp;amp; \alpha \quad \mathrm{if}\, H : \theta \in \Omega\subscript{H}, \\\
\beta\subscript{\phi}(\theta) &amp;amp;\geq&amp;amp; \alpha \quad \mathrm{if}\,  K : \theta \in \Omega\subscript{K}. 
\nonumber
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;In particular, it follows that $\beta\subscript{\phi}(\theta) = \alpha$ on the common boundary of $\Omega\subscript{H}$ and $\Omega\subscript{K}$. In fact, a test that is the most powerful among all such tests, is UMP unbiased (Lemma 4.1.1 in TSH). &lt;/p&gt;

&lt;p&gt;However, the definition of an unbiased test can be generalized in the same way as that of an unbiased estimator shown above.
Assume that there is a loss function $L(\theta, \phi(x))$, which only depends on the true value of $\theta$ and the decision $\phi(x)$ takes by the test $\phi$. Then the hypothesis test is unbiased with respect to $L$, if for all $\theta^\prime$ it holds that&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}(L(\theta^\prime, \phi(X))) \geq \mathrm{E}\subscript{\theta}(L(\theta, \phi(X))).$$&lt;/p&gt;

&lt;p&gt;For the test $\phi$ of $H$ vs. $K$ let the loss function be equal to $\alpha$ if a Type II error is committed and equal $(1-\alpha)$ if a Type I error is committed. Then &lt;/p&gt;

&lt;p&gt;$$
\mathrm{E}\subscript{\theta}(L(\theta^\prime, \phi(X))) = 
\begin{cases}
\alpha (1 - \beta\subscript{\phi}(\theta)) \quad &amp;amp;\mathrm{if}&amp;amp;\, \theta^\prime \in \Omega\subscript{K}\\\ 
(1-\alpha) \beta\subscript{\phi}(\theta) \quad &amp;amp;\mathrm{if}&amp;amp;\, \theta^\prime \in \Omega\subscript{H},
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;It follows that if $\theta \in \Omega\subscript{H}$ then $\alpha (1 - \beta\subscript{\phi}(\theta)) \geq (1-\alpha) \beta\subscript{\phi}(\theta)$, and consequently&lt;/p&gt;

&lt;p&gt;$$\beta\subscript{\phi}(\theta) \leq \alpha.$$&lt;/p&gt;

&lt;p&gt;Similarly, by considering $\theta\in\Omega\subscript{K}$, we get $\beta\subscript{\phi}(\theta) \geq \alpha$. Thus the usual definition is a special case of the more general loss-function-based definition.&lt;/p&gt;

&lt;h3&gt;Unbiased confidence sets&lt;/h3&gt;

&lt;p&gt;As is well-known, the defining condition for a confidence interval $\left(\underline{\theta}, \overline{\theta}\right)$ is&lt;/p&gt;

&lt;p&gt;$$P\subscript{\theta}\left(\underline{\theta}(X) \leq \theta \leq \overline{\theta}(X)\right) \geq 1-\alpha,$$&lt;/p&gt;

&lt;p&gt;for all $\theta$.&lt;/p&gt;

&lt;h4&gt;Hypotheses tests vs. confidence intervals&lt;/h4&gt;

&lt;p&gt;It is well-known that hypotheses tests and confidence intervals generally do exactly the same thing.
However, to describe with mathematical rigour in what sense it is true requires a little thinking.&lt;/p&gt;

&lt;p&gt;Consider a level $\alpha$ test of a two-sided hypothesis test $H : \theta = \theta\subscript{0}$ vs. $K : \theta \neq \theta\subscript{0}$, and denote its acceptance region by $A(\theta\subscript{0})$.
Define the inclusion region of the confidence set to be&lt;/p&gt;

&lt;p&gt;$$S(x) := \{ \theta : x\in A(\theta) \},$$&lt;/p&gt;

&lt;p&gt;that is, $\theta \in S(x)$ if and only if $x\in A(\theta)$. Then $S(x)$ defines a $(1-\alpha) \cdot 100\%$ confidence set, because for all $\theta$ we have&lt;/p&gt;

&lt;p&gt;$$P\subscript{\theta}(\theta \in S(x)) = P\subscript{\theta}(x\in A(\theta)) \geq 1 - \alpha.$$&lt;/p&gt;

&lt;p&gt;Conversely, if we start out with a family of confidence sets $\{S(x) : x\in\mathcal{X}\}$, and define $A(\theta) := \{x : \theta\in S(x)\}$, then for any $\theta$ it holds that&lt;/p&gt;

&lt;p&gt;$$P\subscript{\theta}(x\in A(\theta)) = P\subscript{\theta}(\theta \in S(x)) \geq 1 - \alpha.$$&lt;/p&gt;

&lt;p&gt;It follows that $P\subscript{\theta}(\mathrm{Type\,I\,error}) \leq \alpha$, that is, $A(\theta)$ is the acceptance region of a level $\alpha$ test.&lt;/p&gt;

&lt;h4&gt;Unbiased and uniformly most accurate unbiased confidence sets&lt;/h4&gt;

&lt;p&gt;Now it suggests itself to define an unbiased confidence set as one that stems from an unbiased hypothesis test by the above procedure. 
In the two-sided case discussed above this condition reduces to&lt;/p&gt;

&lt;p&gt;$$P\subscript{\theta}\left(\underline{\theta}(X) \leq \theta^\prime \leq \overline{\theta}(X)\right) \leq 1 - \alpha$$&lt;/p&gt;

&lt;p&gt;for all $\theta^\prime$ and $\theta$ such that $\theta \neq \theta^\prime$. That is, the inclusion probability of the null hypothesis parameter $\theta^\prime$ in the confidence interval, when the alternative $\theta$ is true, is less than the confidence level. Lemma 5.5.1 in TSH shows that the confidence set derived from an unbiased level $\alpha$ hypothesis test has indeed the form of an interval.&lt;/p&gt;

&lt;p&gt;Similarly, uniformly most accurate confidence intervals correspond to uniformly most powerful tests (see section 3.5 in TSH for more detail).
However, UMP tests usually do not exist, which is a reason to concentrate on unbiasedness instead. In particular, UMP unbiased tests correspond to uniformly most accurate unbiased confidence sets, i.e.  $S(x)$ such that for all $\theta^\prime$ and $\theta$ with $\theta\in K(\theta^\prime)$ the probability $P\subscript{\theta}(\theta^\prime\in S(x))$ is minimized.
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;input type=&quot;button&quot; onclick=&quot;return toggleMe(&#39;para2&#39;)&quot; value=&quot;+ Topic:&quot;&gt; &lt;b&gt;Conditional expectation, conditional distribution, sufficiency, decision procedures&lt;/b&gt;&lt;br&gt;
&lt;div id=&quot;para2&quot; style=&quot;display:none&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Conditional expectation, conditional distribution, sufficiency, decision procedures&lt;/h2&gt;

&lt;p&gt;Consider a random variable $X$ with sample space $(\mathcal{X}, \mathcal{A})$ and probability distribution $P^X$, and a statistic $T(X)$ with range space $(\mathcal{T}, \mathcal{B})$.&lt;/p&gt;

&lt;h4&gt;Definition [$\mathrm{E}(f(X)|t)$]&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Let $f(x)$ be a non-negative, $\mathcal{A}$-measurable and $P^X$-integrable function.
A $\mathcal{B}$-measurable function $g(t)$ is the conditional expectation of $X$ for given $t$, i.e. $\mathrm{E}(f(X)|t) = \mathrm{E}(f(X)|T=t) = g(t)$, if for all sets $B\in\mathcal{B}$ it holds that&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$$\int\subscript{T^{-1}(B)} f(x) dP^X(x) = \int\subscript{B} g(t) dP^T(t).$$&lt;/p&gt;

&lt;p&gt;Some observations regarding this definition:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In fact, if we define $f\subscript{0}(x) = g(T(x))$, then by Lemma 2.3.2 in TSH the above formula becomes&lt;/p&gt;

&lt;p&gt;$$\int\subscript{A} f(x) dP^X(x) = \int\subscript{A} f\subscript{0}(x) dP^X(x), \forall A \in \mathcal{A}\subscript{0},$$&lt;/p&gt;

&lt;p&gt;where $\mathcal{A}\subscript{0}$ is the $\sigma$-algebra induced by $T$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The existence and uniqueness $(\mathcal{A}\subscript{0}, P^X)$ of such a function $f\subscript{0}$ follows from Radon-Nikodym Theorem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If $f$ is not non-negative, then we can use the usual decomposition $f = f^+ - f^-$ and define&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}(f(X)|t) = \mathrm{E}(f^+(X)|t) - \mathrm{E}(f^-(X)|t).$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Definition [$P(A|t)$]&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Let $I\subscript{A}(X)$ be a random variable that is equal to one if and only if $X\in A$. The conditional probability of $A$ given $T=t$ can be defined as&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$$P(A|t) = E(I\subscript{A}(X) | t).$$&lt;/p&gt;

&lt;p&gt;This definition seems natural, and in fact, if $T$ has Euclidean domain and range spaces or if $\mathrm{E}|f(X)| &amp;lt; \infty$, then the above defines the &lt;em&gt;conditional probability distribution&lt;/em&gt; $P^{X|t}$ (see Theorems 2.5.2 and 2.5.3 in TSH).&lt;/p&gt;

&lt;h4&gt;Definition [Sufficiency]&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Let $\mathcal{P} = \{P\subscript{\theta} : \theta\in\Omega\}$ be a family of distributions over a sample space $(\mathcal{X}, \mathcal{A})$.&lt;/em&gt;
&lt;em&gt;$T$ is sufficient for $\mathcal{P}$ (or $\theta$) if $P\subscript{\theta}(A|t)$ is independent of $\theta$ for every $A\in\mathcal{A}$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In particular, the class of decision procedures depending on a sufficient statistic $T$ is &lt;em&gt;essentially complete&lt;/em&gt;. To see this, assume that the sample space is Euclidean, then by Theorem 2.5.1 in TSH there exists the conditional probability distribution $P^{X|t}$. Let $\phi(x)$ be a decision procedure. Given only the value of the sufficient statistic $T(X)$ (but not $X$), define another decision procedure $\psi(t)$ as a random sample from the distribution $P^{X|t}$. Then $\phi(X)$ and $\psi(T)$ have identical distributions. Consequently, both decision procedures have the same risk,&lt;/p&gt;

&lt;p&gt;$$R(\theta, \psi) = \mathrm{E}(L(\theta, \psi(T))) = \mathrm{E}(L(\theta, \phi(X))) = R(\theta, \phi).$$&lt;/p&gt;

&lt;p&gt;Thus, for any decision procedure that is based on $X$, there is a decision procedure based on $T$ that is equally good or better.&lt;/p&gt;

&lt;p&gt;For a proof in the general (non-Euclidean) case see exercise 2.13 in TSH.&lt;/p&gt;

&lt;h3&gt;General conditional expectation&lt;/h3&gt;

&lt;p&gt;Let $X$ and $Y$ be two real-valued random variables, which can be written as mappings $X: \Omega \to \mathbb{R}$ and $Y: \Omega \to \mathbb{R}$ over a measurable space $(\Omega, \mathcal{A}, P)$. The above definition of $\mathrm{E}(X|T(X)=t)$ suggests a similar definition of $\mathrm{E}(X|Y=y)$. Namely, $\mathrm{E}(X|Y=y) = g(y)$ if for all Borel sets $A$ it holds that&lt;/p&gt;

&lt;p&gt;$$\int\subscript{Y^{-1}(A)} X(\omega) dP(\omega) = \int\subscript{A} g(y) dP^Y(y).$$&lt;/p&gt;

&lt;p&gt;In fact, a more general version of this definition is given in Feller&amp;#39;s &amp;quot;An Introduction to Probability Theory and its Applications. Volume II&amp;quot; (10.6) as,&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}(X\cdot I\subscript{A}(Y)) = \int\subscript{A} \mathrm{E}(X | y) \mu\{dy\},$$&lt;/p&gt;

&lt;p&gt;for any pair of random variables $X$ and $Y$.&lt;/p&gt;

&lt;p&gt;If $X$ and $Y$ are real-valued one-dimensional, then the pair $(X,Y)$ can be viewed as a random vector in the plane. Each set $\{Y \in A\}$ consists of parallels to the $x$-axis, and we can define a $\sigma$-algebra induced by $Y$ as the collection of all sets $\{Y \in A\}$, where $A$ are Borel sets. Then $\mathrm{E}(X|Y)$ is a random variable, such that $\mathrm{E}(X\cdot I\subscript{B}) = \mathrm{E}(\mathrm{E}(X|Y) \cdot I\subscript{B})$ for all $B=\{Y\in A\}$ with $A$ being a Borel set. This leads to the following general definition.&lt;/p&gt;

&lt;h4&gt;Definition [Conditional expectation]&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Let $\mathcal{A}$ be the underlying $\sigma$-algebra of sets, and let $\mathcal{B}$ be a $\sigma$-algebra contained in $\mathcal{A}$. Let $X$ be a random variable.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;A random variable $U$ is called a conditional expectation of $X$ relative to $\mathcal{B}$ (or $U=\mathrm{E}(X|\mathcal{B})$), if it is $\mathcal{B}$-measurable and for all $B\in\mathcal{B}$ it holds that&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}(X\cdot I\subscript{B}) = \mathrm{E}(U \cdot I\subscript{B}).$$&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;If $\mathcal{B}$ is the $\sigma$-algebra generated by a random variable $Y$, then $\mathrm{E}(X|Y) = \mathrm{E}(X|\mathcal{B})$.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;input type=&quot;button&quot; onclick=&quot;return toggleMe(&#39;neyman-pearson&#39;)&quot; value=&quot;+ Topic:&quot;&gt; &lt;b&gt;An informal summary of Neyman-Pearson and generalizations&lt;/b&gt;&lt;br&gt;
&lt;div id=&quot;neyman-pearson&quot; style=&quot;display:none&quot;&gt;&lt;/p&gt;

&lt;h2&gt;An informal summary of Neyman-Pearson and generalizations&lt;/h2&gt;

&lt;p&gt;The following offers an informal view on the fundamental lemma of Neyman and Pearson and generalizations thereof.
For a mathematically rigorous presentation see the corresponding results in TSH, which are cited in this article.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notation&lt;/em&gt;: MP = &amp;quot;most powerful&amp;quot;, UMP = &amp;quot;uniformly most powerful&amp;quot;, $H$ denotes the null hypothesis, $K$ denotes the alternative hypothesis, $\alpha$ denotes the level of the hypothesis test, lower case Roman letters denote realizations of random variables (upper case).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;What is actually called fundamental lemma of Neyman and Pearson in TSH (Theorem 3.2.1) is concerned with a test of two simple hypotheses. Under the null hypothesis the random variable $X$ is assumed to follow a probability distribution with density $p\subscript{0}$, while under the alternative hypothesis the density is $p\subscript{1}$. &lt;/p&gt;

&lt;p&gt;Consider $H : p\subscript{0}$ vs. $K : p\subscript{1}$. MP test $\phi$ exists. It rejects the null if $\frac{p\subscript{1}}{p\subscript{0}} &amp;gt; k$, accepts the null if $\frac{p\subscript{1}}{p\subscript{0}} &amp;lt; k$, and rejects with probability $\gamma$ if $\frac{p\subscript{1}}{p\subscript{0}} = k$, where $\gamma$ and $k$ are chosen to satisfy $\mathrm{E}\subscript{p\subscript{0}} \phi(X) = \alpha$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For one-parameter families of distributions and one-sided hypotheses, the Neyman-Pearson lemma can be generalized to construct a UMP test if the distributions in question have monotone likelihood ratios. This is Theorem 3.4.1 in TSH.&lt;/p&gt;

&lt;p&gt;Consider $H : \theta \leq \theta\subscript{0}$ vs. $K : \theta &amp;gt; \theta\subscript{0}$ ($\theta \in \mathbb{R}$). If $\frac{p\subscript{\theta^\prime}(x)}{p\subscript{\theta}(x)}$ is nondecreasing in $T(x)$ for any $\theta &amp;lt; \theta^\prime$, then a UMP test $\phi$ exists. It rejects if $T(x) &amp;gt; C$, accepts if $T(x) &amp;lt; C$, and rejects with probability $\gamma$ if $T(x) = C$, where $C$ and $\gamma$ are determined by $\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha$.&lt;/p&gt;

&lt;p&gt;By interchanging the inequalities one obtains a UMP test for the dual problem $H : \theta \geq \theta\subscript{0}$ vs. $K : \theta &amp;lt; \theta\subscript{0}$.&lt;/p&gt;

&lt;p&gt;Additionally, this test minimizes the Type I error subject to $\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An analogous UMP test exists for a two-sided null hypothesis $H : \theta \leq \theta\subscript{1} \,\mathrm{or}\, \theta \geq \theta\subscript{2}$ in one-parameter exponential families. It rejects if $C\subscript{1} &amp;lt; T(x) &amp;lt; C\subscript{2}$, accepts if $T(x) &amp;lt; C\subscript{1}$ or $T(x) &amp;gt; C\subscript{2}$, rejects with probability $\gamma$ if $T(x) = C\subscript{i}$ (for $i=1$ or $i=2$), and satisfies $\mathrm{E}\subscript{\theta\subscript{1}} \phi(X) = \alpha = \mathrm{E}\subscript{\theta\subscript{2}} \phi(X)$. Subject to the last condition, this test minimizes the Type I error. See Theorem 3.7.1 in TSH.&lt;/p&gt;

&lt;p&gt;A UMP test for a two-sided alternative hypothesis $K : \theta \leq \theta\subscript{1} \,\mathrm{or}\, \theta \geq \theta\subscript{2}$ does not exist (e.g. see the corresponding section in this blog post). However, a UMP &lt;em&gt;unbiased&lt;/em&gt; test analogous to the above exists (see Section 4.2 in TSH).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the null hypothesis specifies that $X$ is distributed according to one of finitely many densities $p\subscript{1}, p\subscript{2}, \dots, p\subscript{m}$, and the alternative hypothesis is $p\subscript{m+1}$, then there exists a test $\phi$ that maximizes $\int \phi p\subscript{m+1} d\mu$. For suitable constants $k\subscript{1}, k\subscript{2}, \dots, k\subscript{m}$, this test rejects the null if $p\subscript{m+1}(x) &amp;gt; \sum\subscript{i=1}^m k\subscript{i} p\subscript{i}(x)$, it accepts the null if $p\subscript{m+1}(x) &amp;lt; \sum\subscript{i=1}^m k\subscript{i} p\subscript{i}(x)$, and it satisfies $\int \phi p\subscript{i} d\mu \leq \alpha$ for $i = 1,2,\dots,m$.&lt;/p&gt;

&lt;p&gt;See Theorem 3.6.1 and Corollary 3.6.1 in TSH for more detail.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assume a setting similar to the one in the last point, except that the number of distributions under the null hypothesis does not need to be finite. That is, $H : f\subscript{\theta}, \theta \in \omega$ vs. $K : g$.
One can define a &lt;em&gt;least favorable&lt;/em&gt; distribution $\Lambda$ over $\omega$ and assume that $\theta \sim \Lambda$. As $\Lambda$ is least favorable, one can expect that it leads to a hypothesis test that works best in the works case (i.e. at values $\theta$ closest to $K$). Thus, $\Lambda$ will typically be a distribution of $H$ that is closest to $K$. In particular, one would have $\Lambda(\omega^\prime) = 1$ for some &amp;quot;boundary region&amp;quot; of $\omega$. Then a MP test $\phi$ exists. It rejects if $g(x) &amp;gt; k \int f\subscript{\theta}(x) d\Lambda(\theta)$, accepts if $g(x) &amp;lt; k \int f\subscript{\theta}(x) d\Lambda(\theta)$, and satisfies $\sup\subscript{\theta\in\omega} \mathrm{E}\subscript{\theta} \phi(X) = \alpha$.&lt;/p&gt;

&lt;p&gt;See Theorem 3.8.1 and Corollary 3.8.1 in TSH for rigour and detail.&lt;/p&gt;

&lt;p&gt;This approach can also be generalized to test $H : f\subscript{\theta}, \theta \in \omega$ vs. $K : f\subscript{\theta}, \theta \in \omega^\prime$. Then a test can be established that maximizes $\inf\subscript{\theta\in\omega^\prime} \mathrm{E} \phi(X)$. Such a test is called a &lt;em&gt;maximin&lt;/em&gt; test and is established in Theorem 8.1.1 and Corollary 8.1.1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For multi-parameter exponential families the existence of a UMP test typically cannot be established. However, UMP &lt;em&gt;unbiased&lt;/em&gt; tests can be constructed without great difficulties. Assume that $\theta\in\mathbb{R}$ is the parameter to be tested, and that $(U, T)$ is a sufficient statistic, where $U$ corresponds to $\theta$ and $T$ corresponds to all other parameters. Then UMP unbiased tests exist for most of the usual hypotheses, and can be written in the same way as in the one-parameter case, except that now all constants specifying the rejection region depend on $T$ (e.g. the rejection rule has the form $u &amp;gt; C(t)$, etc.). Also, the size of the test is measured conditional on $T$.&lt;/p&gt;

&lt;p&gt;See Theorem 4.4.1 in TSH.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;UMP unbiased tests for multi-parameter exponential families, as discussed in the last point, are independent of $T$ if a number of additional conditions are satisfied. For example, assume that $V = h(U, T)$ is independent of $T$ (with $\theta = \theta\subscript{1}$ and $\theta = \theta\subscript{2}$) and that $h$ is increasing in $u$. Then a UMP unbiased test for a two-sided null hypothesis rejects if $C\subscript{1} &amp;lt; v &amp;lt; C\subscript{2}$, accepts if $v &amp;lt; C\subscript{1}$ or $v &amp;gt; C\subscript{2}$, etc.&lt;/p&gt;

&lt;p&gt;See Theorem 5.1.1 in TSH for more.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the problem of testing $H : \Omega\subscript{0}$ vs. $K : \Omega\subscript{1}$ remains invariant under a finite group $G = \{g\subscript{1}, g\subscript{2}, \dots, g\subscript{N} \}$, then there exists a UMP invariant test that rejects when $\frac{\sum p\subscript{\overline{g}\subscript{i} \theta\subscript{1}} (x)}{\sum p\subscript{\overline{g}\subscript{i} \theta\subscript{0}} (x)} &amp;gt; C$ (for any $\theta\subscript{0} \in \Omega\subscript{0}$ and any $\theta\subscript{1}$ in $\Omega\subscript{1}$). See Theorem 6.3.1 in TSH.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;input type=&quot;button&quot; onclick=&quot;return toggleMe(&#39;permutation&#39;)&quot; value=&quot;+ Topic:&quot;&gt; &lt;b&gt;Permutation tests&lt;/b&gt;&lt;br&gt;
&lt;div id=&quot;permutation&quot; style=&quot;display:none&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Permutation tests&lt;/h2&gt;

&lt;p&gt;When a parametric probabilistic model cannot be assumed, one can still construct exact level-$\alpha$ hypotheses tests as permutation tests. Here, based on sections 5.8 and 5.9 of TSH, I discuss the concept by considering as an example a permutation test for the difference of two means. &lt;/p&gt;

&lt;p&gt;Assume that each of the random variables $X\subscript{1}, \dots, X\subscript{m}$ has mean $\eta$ and that each of $Y\subscript{1}, ..., Y\subscript{n}$ has mean $\xi$. Additionally assume that the distributions of all those variables differ only with respect to the mean, for example, $X\subscript{i} \sim \mathrm{i.i.d.}\, f(x\subscript{i})$ and $Y\subscript{i} \sim \mathrm{i.i.d.}\, f(y\subscript{i} - \Delta)$ with $\Delta = \eta - \xi$. The density function $f$ is not known apart from the fact that it is continuous a.e. We want to test the hypothesis $H : \Delta = 0$.&lt;/p&gt;

&lt;p&gt;Let $N:=n+m$, denote the random vector containing all $X$s and $Y$s as $Z := (X^T, Y^T)^T$, and let $S(z)$ be the set of all permutations of the entries of a realization $z$ of the random vector $Z$.
Then a level-$\alpha$ test $\phi$ has to satisfy&lt;/p&gt;

&lt;p&gt;$$\int \phi(z) \prod\subscript{i=1}^N f(z\subscript{i}) dz = \alpha.$$&lt;/p&gt;

&lt;p&gt;Interestingly, it turns out that this equality holds if and only if&lt;/p&gt;

&lt;p&gt;$$\frac{1}{N!} \sum\subscript{w\in S(z)} \phi(w) = \alpha.$$&lt;/p&gt;

&lt;p&gt;A more general result that accounts for population stratification is given by theorem 5.8.1 in TSH.&lt;/p&gt;

&lt;p&gt;The power of $\phi$  against an alternative $h(z)$ is given by&lt;/p&gt;

&lt;p&gt;$$\int \phi(z) h(z) dz = \int \mathrm{E}\left(\phi(Z) \middle| T=t\right) dP^T(t).$$&lt;/p&gt;

&lt;p&gt;Let $T(Z)$ be the order statistic. It holds that $S(z) = S(T(z)) = S(t)$, and from the expression of the conditional expectation $\mathrm{E}\left(\phi(Z) \middle| T=t\right)$ (see Example 2.4.1 and Problem 2.6), it can be further derived that the most powerful test $\phi$ maximizes&lt;/p&gt;

&lt;p&gt;$$\sum\subscript{z\in S(t)} \phi(z) \frac{h(z)}{\sum\subscript{w\in S(z)} h(w)}$$&lt;/p&gt;

&lt;p&gt;subject to&lt;/p&gt;

&lt;p&gt;$$\frac{1}{N!} \sum\subscript{z\in S(t)} \phi(z) = \alpha.$$&lt;/p&gt;

&lt;p&gt;Now, the Neyman-Pearson fundamental lemma implies that the hypothesis should be rejected whenever $\frac{h(z)N!}{\sum\subscript{w\in S(z)} h(w)}$ is too large.
This leads to a most powerful test $\phi$ given by&lt;/p&gt;

&lt;p&gt;$$\phi(z) = \begin{cases}
1, \quad\mathrm{if}\, h(z) &amp;gt; C(T(z)), \\\
\gamma, \quad\mathrm{if}\, h(z) = C(T(z)), \\\
0, \quad\mathrm{if}\, h(z) &amp;lt; C(T(z)).
\end{cases}$$&lt;/p&gt;

&lt;p&gt;Thus the test is carried out by... &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ordering the points in $S(z)$ in a decreasing order according to $h$,&lt;/li&gt;
&lt;li&gt;rejecting if $h(z)$ is one of the $k$ largest values and rejecting with probability $\gamma$ if $h(z)$ is $(k+1)$st largest, where $k$ and $\gamma$ are determined by&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$k+\gamma = \alpha \cdot N!$$&lt;/p&gt;

&lt;p&gt;More general versions of this approach, which incorporate population stratification and randomization, are given in section 5.8-5.13 in TSH.&lt;/p&gt;

&lt;p&gt;The above test is not UMP because it depends on $h$. However, it can be shown that if under the null hypothesis each $Z\subscript{i}$ follows the same normal distribution $\mathcal{N}(\xi, \sigma^2)$, then the derived test is most powerful among all unbiased tests of level $\alpha$ against all normal alternatives under consideration (see Lemma 5.9.1 in TSH for an even more general result).
Such an approach is appropriate when the data is assumed to be approximately normal but the assumption is not considered reliable. The permutation test is maximizing the power against all normal alternatives, while still being unbiased against all other alternatives.&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;input type=&quot;button&quot; onclick=&quot;return toggleMe(&#39;two-sided&#39;)&quot; value=&quot;+ Topic:&quot;&gt; &lt;b&gt;UMP tests for two-sided hypotheses&lt;/b&gt;&lt;br&gt;
&lt;div id=&quot;two-sided&quot; style=&quot;display:none&quot;&gt;&lt;/p&gt;

&lt;h2&gt;UMP tests for two-sided hypotheses&lt;/h2&gt;

&lt;p&gt;The (non-) existence of uniformly most powerful (or UMP) tests for two-sided hypotheses is an interesting phenomenon. &lt;/p&gt;

&lt;h4&gt;Example of existence&lt;/h4&gt;

&lt;p&gt;First let&amp;#39;s look at an example when such a test does exist. This is Problem 3.2 in TSH.&lt;/p&gt;

&lt;p&gt;For $i = 1,\dots, n$ let $X\subscript{i}$ be i.i.d. $\mathrm{Uniform}(0,\theta)$ random variables, denote their realizations by lower case $x\subscript{i}$s, and let $X$ denote the vector of the $X\subscript{i}$s. Consider the hypothesis $H : \theta = \theta\subscript{0}$ against the alternative $K : \theta \neq \theta\subscript{0}$.&lt;/p&gt;

&lt;p&gt;Denote $x\subscript{(n)} := \max\{x\subscript{1}, \dots, x\subscript{n}\}$. Let $\phi$ be a hypothesis test which rejects $H : \theta = \theta\subscript{0}$ in favor of a two-sided alternative, if either $x\subscript{(n)} \geq \theta\subscript{0}$ or $x\subscript{(n)} &amp;lt; \theta\subscript{0} \sqrt[n]{\alpha}$.&lt;/p&gt;

&lt;h5&gt;Proof&lt;/h5&gt;

&lt;p&gt;Using the fundamental lemma of Neyman and Pearson, it is straightforward to prove that $\phi$ is UMP. Namely, $\phi$ is a UMP test at level $\alpha$ by Neyman-Pearson, if for any fixed $\theta\subscript{1} \neq \theta\subscript{0}$, the test $\phi$ can be written as &lt;/p&gt;

&lt;p&gt;$$\phi(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &amp;gt; k p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &amp;lt; k p\subscript{\theta\subscript{0}}(x), 
\end{cases}$$&lt;/p&gt;

&lt;p&gt;with a suitable $k$, and if it satisfies&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha.$$&lt;/p&gt;

&lt;p&gt;We have that&lt;/p&gt;

&lt;p&gt;$$\begin{eqnarray}
\nonumber
\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) &amp;amp;=&amp;amp; P\subscript{\theta\subscript{0}}\left(X\subscript{(n)} &amp;gt; \theta\subscript{0}\right) + P\subscript{\theta\subscript{0}}\left(X\subscript{(n)} &amp;lt; \theta\subscript{0}\sqrt[n]{\alpha}\right)\\\
&amp;amp;=&amp;amp; 0 + \left(\frac{\theta\subscript{0} \sqrt[n]{\alpha}}{\theta\subscript{0}}\right)^n = \alpha.
\nonumber
\end{eqnarray}$$&lt;/p&gt;

&lt;p&gt;As for the other Neyman-Pearson condition, we have to consider multiple cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If $\theta\subscript{1} &amp;gt; \theta\subscript{0}$, then $k = \left(\frac{\theta\subscript{0}}{\theta\subscript{1}}\right)^n$ yields the desired result.&lt;/li&gt;
&lt;li&gt;If $\theta\subscript{0}\sqrt[n]{\alpha} &amp;lt; \theta\subscript{1} &amp;lt; \theta\subscript{0}$, then $k = \left(\frac{\theta\subscript{0}}{\theta\subscript{1}}\right)^n$ can be used as well.&lt;/li&gt;
&lt;li&gt;If $\theta\subscript{1} &amp;lt; \theta\subscript{0}\sqrt[n]{\alpha} &amp;lt; \theta\subscript{0}$, then $k = 0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;right&quot;&gt;
$\blacksquare$
&lt;/div&gt;

&lt;h4&gt;Example of non-existence&lt;/h4&gt;

&lt;p&gt;Thus, we saw an example of a UMP test for a two-sided hypothesis. &lt;/p&gt;

&lt;p&gt;However, when the underlying distribution comes from an exponential family, then a UMP test does not exist for $H : \theta = \theta\subscript{0}$ vs. $K : \theta \neq \theta\subscript{0}$ (Problem 3.54 in TSH). This follows quite easily from the consideration of UMP tests for the one-sided hypotheses $H\subscript{1} : \theta \leq \theta\subscript{0}$ vs. $K\subscript{1} : \theta &amp;gt; \theta\subscript{0}$, and $H\subscript{2} : \theta \geq \theta\subscript{0}$ vs. $K\subscript{2} : \theta &amp;lt; \theta\subscript{0}$.
A detailed proof follows.&lt;/p&gt;

&lt;h5&gt;Proof&lt;/h5&gt;

&lt;p&gt;According to Theorem 3.4.1 in TSH, a UMP test of $H\subscript{1}$ exists and can be written as&lt;/p&gt;

&lt;p&gt;$$\phi\subscript{1}(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, T(x) &amp;gt; C\subscript{1},\\\
0, \quad &amp;amp;\mathrm{if}\, T(x) &amp;lt; C\subscript{1}.
\end{cases}$$&lt;/p&gt;

&lt;p&gt;Similarly, a UMP test of $H\subscript{2}$ exists and can be written as&lt;/p&gt;

&lt;p&gt;$$\phi\subscript{2}(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, T(x) &amp;lt; C\subscript{2},\\\
0, \quad &amp;amp;\mathrm{if}\, T(x) &amp;gt; C\subscript{2}.
\end{cases}$$&lt;/p&gt;

&lt;p&gt;Clearly, $\phi\subscript{1}$ and $\phi\subscript{2}$ are level-$\alpha$ tests for $H$ vs. $K$ as well.&lt;/p&gt;

&lt;p&gt;Let $\phi\subscript{0}$ be a level-$\alpha$ test of $H$ vs. $K$. Fix a $\theta\subscript{1} &amp;gt; \theta\subscript{0}$ and a $\theta\subscript{2} &amp;lt; \theta\subscript{0}$. Assume that &lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta\subscript{i}} \phi\subscript{0}(X) \geq \mathrm{E}\subscript{\theta\subscript{i}} \phi\subscript{i}(X)$$&lt;/p&gt;

&lt;p&gt;for $i = 1,2$. Then $\phi\subscript{0}$ is most powerful for testing $\theta\subscript{0}$ vs. $\theta\subscript{1}$ and for testing $\theta\subscript{0}$ vs. $\theta\subscript{2}$. Thus, by the fundamental lemma of Neyman and Pearson the UMP test can be rewritten as&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\phi\subscript{0}(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &amp;gt; k\subscript{1} p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &amp;lt; k\subscript{1} p\subscript{\theta\subscript{0}}(x), 
\end{cases}
\label{eq1}
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\phi\subscript{0}(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{2}}(x) &amp;gt; k\subscript{2} p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{2}}(x) &amp;lt; k\subscript{2} p\subscript{\theta\subscript{0}}(x).
\end{cases}
\label{eq2}
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;Let $x$ be such that $\phi\subscript{0}(x) = 1$. Now, from the monotonicity of the likelihood ratio, it follows that&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;if $T(y) &amp;gt; T(x)$ then $\phi\subscript{0}(y) = 1$ (by equation $\eqref{eq1}$),&lt;/li&gt;
&lt;li&gt;if $T(y) &amp;lt; T(x)$ then $\phi\subscript{0}(y) = 1$ (by equation $\eqref{eq2}$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is, either $\phi\subscript{0}(y) = 1$ for all $y$ or $\phi\subscript{0}(x) \neq 1$ for all $x$. A contradiction. It follows that $\phi\subscript{0}$ can not be more powerful than $\phi\subscript{1}$ for testing $\theta\subscript{0}$ vs. $\theta\subscript{1}$ and than $\phi\subscript{2}$ for testing $\theta\subscript{0}$ vs. $\theta\subscript{2}$. Thus, a UMP test for $H$ vs. $K$ does not exist.&lt;/p&gt;

&lt;div align=&quot;right&quot;&gt;
$\blacksquare$
&lt;/div&gt;

&lt;p&gt;Even though a UMP test for the two-sided hypothesis considered above does not exist, there exist a UMP unbiased test (i.e. a test that is uniformly most powerful among all unbiased tests). For detail see Section 4.2 in TSH.&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//TSH_and_TPE_observations/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//TSH_and_TPE_observations/</guid>
      </item>
    
      <item>
        <title>NMatrix with Intel MKL on my university&#39;s HPC</title>
        <description>&lt;p&gt;In order to use &lt;a href=&quot;https://github.com/SciRuby/nmatrix&quot;&gt;NMatrix&lt;/a&gt; for the statistical analysis of big genomic data, I decided to install it on my university&amp;#39;s high performance computing system (HPC). It is called &lt;a href=&quot;http://crsc.tulane.edu/&quot;&gt;Cypress&lt;/a&gt; (like the &lt;a href=&quot;http://imgc.allpostersimages.com/images/P-473-488-90/64/6420/5OV9100Z/posters/paul-souders-cypress-reflected-in-bayou-along-highway-61-on-stormy-summer-afternoon-new-orleans-louisiana-usa.jpg&quot;&gt;typical New Orleans tree&lt;/a&gt;), and it&amp;#39;s currently the 10th best among all American universities. &lt;/p&gt;

&lt;p&gt;At first, I tried to install the latest development version of &lt;code&gt;nmatrix&lt;/code&gt; and &lt;code&gt;nmatrix-atlas&lt;/code&gt; or &lt;code&gt;nmatrix-lapacke&lt;/code&gt; in the same way as I do it on my laptop or desktop. However, this failed in the compilation stage because the BLAS and LAPACK libraries could not be found.&lt;/p&gt;

&lt;p&gt;Therefore, I decided to put some more effort into it, and install NMatrix with support for Intel MKL.
&lt;a href=&quot;https://software.intel.com/en-us/intel-mkl&quot;&gt;Intel MKL (or Math Kernel Library)&lt;/a&gt; promises BLAS and LAPACK functionality with much better performance on Intel hardware than the alternatives (such as ATLAS). Additionally, on Cypress, automatic offload of some LAPACK routines to the &lt;a href=&quot;http://www.intel.com/content/www/us/en/processors/xeon/xeon-phi-detail.html?gclid=CKrYx9LGtcgCFc2PHwodG9YLuw&amp;amp;gclsrc=aw.ds&quot;&gt;Xeon Phi Coprocessors&lt;/a&gt; can be enabled at run time, when Intel MKL is used.&lt;/p&gt;

&lt;p&gt;I document the installation process in what follows (mainly for myself, in case I need to do it again).&lt;/p&gt;

&lt;h1&gt;Installation&lt;/h1&gt;

&lt;p&gt;Cypress uses the &lt;code&gt;module&lt;/code&gt; utility to manage multiple compilers, set environment variables, etc.
In order to use Ruby as well as Intel MKL (which is contained in the Intel Parallel Studio XE suite), I need to load the corresponding modules:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ module load ruby
$ module load intel-psxe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now I can use Ruby in version 2.2.3 as well as the Intel compiler suite and Intel MKL in my current session on Cypress. However, before installing NMatrix, I need to install its dependencies such as &lt;code&gt;bundler&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Installing gems in the user&amp;#39;s home directory&lt;/h2&gt;

&lt;p&gt;As a student I of course don&amp;#39;t have permission to install software system-wide on my university&amp;#39;s HPC.
The option &lt;code&gt;--user-install&lt;/code&gt; can be used with &lt;code&gt;gem install&lt;/code&gt; to install gems locally in the user&amp;#39;s home directory.
For more convenience one can add the line&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;gem: --user-install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;to &lt;code&gt;.gemrc&lt;/code&gt;. This way I can install &lt;code&gt;bundler&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The remaining dependencies of &lt;code&gt;nmatrix&lt;/code&gt; are installed with &lt;code&gt;bundle install&lt;/code&gt;. In order for it to work, however, I need to add my local gem executable directory to path. In my case this is done with &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;export PATH=$PATH:/home/agossman/.gem/ruby/2.2.0/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, &lt;code&gt;bundle install&lt;/code&gt; needs to be invoked with an option for installation in the home directory, which in my case is: &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;bundle install --path /home/agossman/.gem/ruby/2.2.0/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Installing NMatrix and NMatrix-lapacke with Intel MKL&lt;/h2&gt;

&lt;p&gt;I followed the advice given in a comment in &lt;a href=&quot;https://github.com/SciRuby/nmatrix/blob/b7d367f544a9d48af5f1b9dedb7ef6adcf488091/ext/nmatrix_lapacke/extconf.rb#L178&quot;&gt;&lt;code&gt;nmatix/ext/nmatrix_lapacke/extconf.rb&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;#To use the Intel MKL, comment out the line above, and also comment out the bit above with have_library and dir_config for lapack.
#Then add something like the line below (for exactly what linker flags to use see https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor ):
#$libs += &amp;quot; -L${MKLROOT}/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential &amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, it took me a while to figure out the right linker flags. I used the &lt;a href=&quot;https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor&quot;&gt;MKL link line advisor&lt;/a&gt;. Screen shots of inputs leading to working link lines can be found at the bottom of this page.&lt;/p&gt;

&lt;h3&gt;The &amp;quot;right&amp;quot; link line&lt;/h3&gt;

&lt;p&gt;There are three types of linking &amp;mdash; static, dynamic, and SDL (single dynamic library).&lt;/p&gt;

&lt;h4&gt;Stuff that didn&amp;#39;t work&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;I couldn&amp;#39;t get &lt;code&gt;nmatrix-lapacke&lt;/code&gt; to compile with dynamic linking (it complained that some BLAS function cannot be found). However, static and SDL linking work (see below).&lt;/li&gt;
&lt;li&gt;If the linked interface layer is &lt;a href=&quot;https://software.intel.com/en-us/node/528524&quot;&gt;ILP64 (which uses 64-bit integer type as opposed to 32-bit in the LP64 libraries)&lt;/a&gt;, then &lt;code&gt;nmatrix-lapacke&lt;/code&gt; crashes at runtime (always), even if it compiled and installed without complaints (using either static or SDL linking).&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://wiki.hpc.tulane.edu/trac/wiki/cypress/XeonPhi&quot;&gt;Automatic offloading to Intel Xeon Phi coprocessors&lt;/a&gt; (see below).&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;Linking flags that worked:&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The line given in the above NMatrix code comment does actually work. However, I don&amp;#39;t think that it is the optimal choice for the given system, because it does not use the specific features of the Parallel Studio XE suite employed on Cypress (such as parallelism).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using static linking with the MKL LP64 libraries, &lt;code&gt;nmatrix-lapacke&lt;/code&gt; can be compiled with the support for automatic offload to the Intel Xeon Phi Coprocessor (there are two of those at every cluster node). It compiles, passes the tests, and installs. However, when I tried to &lt;a href=&quot;https://wiki.hpc.tulane.edu/trac/wiki/cypress/XeonPhi&quot;&gt;enable the automatic offload by setting &lt;code&gt;MKL_MIC_ENABLE&lt;/code&gt; to 1&lt;/a&gt;, I couldn&amp;#39;t get my Cholesky factorization toy problem to work (see below). With automatic offload disabled (&lt;code&gt;unset MKL_MIC_ENABLE&lt;/code&gt;), everything works fine.&lt;/p&gt;

&lt;p&gt;In this case, the following link line needs to be added to &lt;a href=&quot;https://github.com/SciRuby/nmatrix/blob/b7d367f544a9d48af5f1b9dedb7ef6adcf488091/ext/nmatrix_lapacke/extconf.rb#L178&quot;&gt;&lt;code&gt;nmatix/ext/nmatrix_lapacke/extconf.rb&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$libs += &amp;quot; -Wl,--start-group ${MKLROOT}/lib/intel64/libmkl_intel_lp64.a ${MKLROOT}/lib/intel64/libmkl_core.a ${MKLROOT}/lib/intel64/libmkl_intel_thread.a -Wl,--end-group -liomp5 -ldl -lpthread &amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using linking via SDL, &lt;code&gt;nmatrix-lapacke&lt;/code&gt; compiles, passes the tests, installs, and works great. However, usage of Intel Xeon Phi Coprocessors is not possible if SDL is used for linking.&lt;/p&gt;

&lt;p&gt;SDL offers further, rather convenient features of &lt;a href=&quot;https://software.intel.com/en-us/node/528522&quot;&gt;selection of the threading and interface layer at run time&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To set the threading layer at run time, use the &lt;code&gt;mkl_set_threading_layer&lt;/code&gt; function or set &lt;code&gt;MKL_THREADING_LAYER&lt;/code&gt; variable to one of the following values: &lt;code&gt;INTEL&lt;/code&gt;, &lt;code&gt;SEQUENTIAL&lt;/code&gt;, &lt;code&gt;PGI&lt;/code&gt;. To set interface layer at run time, use the &lt;code&gt;mkl_set_interface_layer&lt;/code&gt; function or set the &lt;code&gt;MKL_INTERFACE_LAYER&lt;/code&gt; variable to &lt;code&gt;LP64&lt;/code&gt; or &lt;code&gt;ILP64&lt;/code&gt;. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The necessary link line that should be used in &lt;a href=&quot;https://github.com/SciRuby/nmatrix/blob/b7d367f544a9d48af5f1b9dedb7ef6adcf488091/ext/nmatrix_lapacke/extconf.rb#L178&quot;&gt;&lt;code&gt;nmatix/ext/nmatrix_lapacke/extconf.rb&lt;/code&gt;&lt;/a&gt; is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$libs += &amp;quot; -Wl,--no-as-needed -L${MKLROOT}/lib/intel64  -lmkl_rt -lpthread &amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Final steps&lt;/h3&gt;

&lt;p&gt;After the linking flags have been determined and added into the code, the development version of &lt;code&gt;nmatrix&lt;/code&gt; and &lt;code&gt;nmatrix-lapacke&lt;/code&gt; can be compiled, tested, and installed as described in the &lt;a href=&quot;https://github.com/SciRuby/nmatrix&quot;&gt;NMatrix README&lt;/a&gt; with the following lines of terminal input:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ bundle exec rake compile nmatrix_plugins=lapacke
$ bundle exec rake spec nmatrix_plugins=lapacke
$ bundle exec rake install nmatrix_plugins=lapacke
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Simple performance tests&lt;/h2&gt;

&lt;p&gt;I performed some quick tests for different installations of &lt;code&gt;nmatrix&lt;/code&gt; and &lt;code&gt;nmatrix-lapacke&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;SVD test&lt;/h3&gt;

&lt;p&gt;Consider the SVD of a 100 &amp;times; 100 matrix:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;NMatrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gesvd&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# check the result for correctness&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;NMatrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vt&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;unless&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all?&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;abs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;SVD does not work!&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The results are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cypress, NMatrix compiled with static linking to MKL:  180817 milliseconds&lt;/li&gt;
&lt;li&gt;Cypress, NMatrix compiled with linking via SDL, with threading layer &lt;code&gt;INTEL&lt;/code&gt;:  180839 milliseconds &lt;/li&gt;
&lt;li&gt;Cypress, NMatrix compiled with linking via SDL, with threading layer &lt;code&gt;SEQUENTIAL&lt;/code&gt;:  83401 milliseconds &lt;/li&gt;
&lt;li&gt;My laptop (&lt;code&gt;nmatrix-lapacke&lt;/code&gt; with Atlas):  122455 milliseconds&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Cholesky factorization test&lt;/h3&gt;

&lt;p&gt;Consider a Cholesky factorization of a 5000 &amp;times; 5000 matrix:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;NMatrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factorize_cholesky&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The results are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cypress, NMatrix compiled with static linking to MKL:  47016 milliseconds&lt;/li&gt;
&lt;li&gt;Cypress, NMatrix compiled with static linking to MKL, with Intel Xeon Phi Coprocessor automatic offload enabled: runtime error (says matrix not symmetric)&lt;/li&gt;
&lt;li&gt;Cypress, NMatrix compiled with linking via SDL, with threading layer &lt;code&gt;INTEL&lt;/code&gt;:  46549 milliseconds &lt;/li&gt;
&lt;li&gt;Cypress, NMatrix compiled with linking via SDL, with threading layer &lt;code&gt;SEQUENTIAL&lt;/code&gt;:  148072 milliseconds&lt;/li&gt;
&lt;li&gt;My laptop (&lt;code&gt;nmatrix-lapacke&lt;/code&gt; with Atlas): 327146 milliseconds&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In particular, we see that for bigger matrices, multi-threading improves performance, while sequential execution is better for smaller matrices.&lt;/p&gt;

&lt;p&gt;Based on these results I decided to compile &lt;code&gt;nmatrix-lapacke&lt;/code&gt; on Cypress with linking to MKL via SDL, as it offers the flexibility of &lt;a href=&quot;https://software.intel.com/en-us/node/528522&quot;&gt;selecting the threading layer at runtime&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Appendix: &lt;a href=&quot;https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor&quot;&gt;MKL link line advisor&lt;/a&gt; screen shots&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/link-line-1.png?raw=true&quot; alt=&quot;SDL link line&quot;&gt;
&lt;img src=&quot;/images/link-line-2.png?raw=true&quot; alt=&quot;Static linking link line&quot;&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 08 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//nmatrix-on-my-universities-hpc/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//nmatrix-on-my-universities-hpc/</guid>
      </item>
    
      <item>
        <title>Statistical linear mixed models in Ruby with mixed_models (GSoC2015)</title>
        <description>&lt;p&gt;Google Summer of Code 2015 is coming to an end. During this summer, I have learned too many things to list here about statistical modeling, Ruby and software development in general, and I had a lot of fun in the process!&lt;/p&gt;

&lt;h2&gt;Linear mixed models&lt;/h2&gt;

&lt;p&gt;My GSoC project is the Ruby gem &lt;a href=&quot;https://github.com/agisga/mixed_models&quot;&gt;mixed_models&lt;/a&gt;. Mixed models are statistical models which predict the value of a response variable as a result of fixed and random effects. The gem in its current version can be used to fit statistical linear mixed models and perform statistical inference on the model parameters as well as to predict future observations. A number of tutorials/examples in IRuby notebook format are accessible from the &lt;code&gt;mixed_models&lt;/code&gt; &lt;a href=&quot;https://github.com/agisga/mixed_models&quot;&gt;github repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Linear mixed models are implemented in the class &lt;code&gt;LMM&lt;/code&gt;. The constructor method &lt;code&gt;LMM#initialize&lt;/code&gt; provides a flexible model specification interface, where an arbitrary covariance structure of the random effects terms can be passed as a &lt;code&gt;Proc&lt;/code&gt; or a block.  &lt;/p&gt;

&lt;p&gt;A convenient user-friendly interface to the basic model fitting algorithm is &lt;code&gt;LMM#from_formula&lt;/code&gt;, which uses the formula language of the R mixed models package &lt;code&gt;lme4&lt;/code&gt; for model specification. With the &lt;code&gt;#from_formula&lt;/code&gt; method, the user can conveniently fit models with categorical predictor variables, interaction fixed or random effects, as well as multiple crossed or nested random effects, all with just one line of code.&lt;/p&gt;

&lt;p&gt;Examples are given in the sections below.&lt;/p&gt;

&lt;h3&gt;Implementation&lt;/h3&gt;

&lt;p&gt;The parameter estimation in &lt;code&gt;LMM#initialize&lt;/code&gt; is largely based on the approach developed by the authors of the R mixed models package &lt;code&gt;lme4&lt;/code&gt;, which is delineated in the &lt;code&gt;lme4&lt;/code&gt; &lt;a href=&quot;https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf&quot;&gt;vignette&lt;/a&gt;. I have tried to make the code of the model fitting algorithm in &lt;code&gt;LMM#initialize&lt;/code&gt; easy to read, especially compared to the corresponding implementation in &lt;code&gt;lme4&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;The &lt;code&gt;lme4&lt;/code&gt; code is largely written in C++, which is integrated in R via the packages &lt;code&gt;Rcpp&lt;/code&gt; and &lt;code&gt;RcppEigen&lt;/code&gt;. It uses &lt;a href=&quot;https://developer.nvidia.com/cholmod&quot;&gt;CHOLMOD&lt;/a&gt; code for various sparse matrix tricks, and it involves passing pointers to C++ object to R (and vice versa) many times, and passing different R environments from function to function. All this makes the &lt;code&gt;lme4&lt;/code&gt; code rather hard to read. Even Douglas Bates, the main developer of &lt;code&gt;lme4&lt;/code&gt;, admits that &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q4/022791.html&quot;&gt;&amp;quot;The end result is confusing (my fault entirely) and fragile&amp;quot;&lt;/a&gt;, because of all the utilized performance improvements. I have analyzed the &lt;code&gt;lme4&lt;/code&gt; code in three blog posts (&lt;a href=&quot;http://agisga.github.io/Dissect_lmer_part1/&quot;&gt;part 1&lt;/a&gt;, &lt;a href=&quot;http://agisga.github.io/Dissect_lmer_part2/&quot;&gt;part 2&lt;/a&gt; and &lt;a href=&quot;http://agisga.github.io/Dissect_lmer_part3/&quot;&gt;part 3&lt;/a&gt;) before starting to work on my gem &lt;code&gt;mixed_models&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The method &lt;code&gt;LMM#initialize&lt;/code&gt; is written in a more functional style, which makes the code shorter and (I find) easier to follow.  All matrix calculations are performed using the gem &lt;a href=&quot;https://github.com/SciRuby/nmatrix&quot;&gt;&lt;code&gt;nmatrix&lt;/code&gt;&lt;/a&gt;, which has a quite intuitive syntax and contributes to the overall code readability as well.
The Ruby gem loses with respect to memory consumption and speed in comparison to &lt;code&gt;lme4&lt;/code&gt;, because it is written in pure Ruby and does not utilize any sparse matrix tricks. However, for the same reasons the &lt;code&gt;mixed_models&lt;/code&gt; code is much shorter and easier to read than &lt;code&gt;lme4&lt;/code&gt;. Moreover, the linear mixed model formulation in &lt;code&gt;mixed_models&lt;/code&gt; is a little bit more general, because it does not assume that the random effects covariance matrix is sparse. More about the implementation of &lt;code&gt;LMM#initialize&lt;/code&gt; can be found in &lt;a href=&quot;http://agisga.github.io/First-linear-mixed-model-fit/&quot;&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Other existing tools&lt;/h3&gt;

&lt;p&gt;Popular existing software packages for mixed models include the R package &lt;a href=&quot;https://cran.r-project.org/web/packages/lme4/index.html&quot;&gt;&lt;code&gt;lme4&lt;/code&gt;&lt;/a&gt; (which is arguably the standard software for linear mixed models), the R package &lt;a href=&quot;https://cran.r-project.org/web/packages/nlme/index.html&quot;&gt;&lt;code&gt;nlme&lt;/code&gt;&lt;/a&gt; (an older package developed by the same author as &lt;code&gt;lme4&lt;/code&gt;, still widely used), Python&amp;#39;s &lt;a href=&quot;https://github.com/statsmodels/statsmodels/blob/master/statsmodels/regression/mixed_linear_model.py&quot;&gt;&lt;code&gt;statmodels&lt;/code&gt;&lt;/a&gt;, and the Julia package &lt;a href=&quot;https://github.com/dmbates/MixedModels.jl&quot;&gt;&lt;code&gt;MixedModels.jl&lt;/code&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Below, I give a couple of examples illustrating some of the capabilities of &lt;code&gt;mixed_models&lt;/code&gt; and explore how it compares to the alternatives.&lt;/p&gt;

&lt;h3&gt;A usage example and discussion&lt;/h3&gt;

&lt;p&gt;As an example, we use &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/BlogFeedback&quot;&gt;data&lt;/a&gt; from the UCI machine learning repository, which originate from blog posts from various sources in 2010-2012, in order to model (the logarithm of) the number of comments that a blog post receives. The linear predictors are the text length, the log-transform of the average number of comments at the hosting website, the average number of trackbacks at the hosting website, and the parent blog posts. We assume a random effect on the number of comments due to the day of the week on which the blog post was published. In &lt;code&gt;mixed_models&lt;/code&gt; this model can be fit with&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;log_comments ~ log_host_comments_avg + host_trackbacks_avg + length + has_parent_with_comments + (1 | day)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                              &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blog_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and we can display some information about the estimated fixed effects with&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inspect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which produces the following output:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;                                             coef                       sd                  z_score            WaldZ_p_value 
               intercept       1.2847896684307731     0.030380582281933178        42.28983027737477                      0.0 
   log_host_comments_avg        0.415586319225577     0.007848368759350875        52.95193586953086                      0.0 
     host_trackbacks_avg     -0.07551588997745964     0.010915623834434068       -6.918146971979714    4.575895218295045e-12 
                  length   1.8245853808280765e-05    2.981631039432429e-06        6.119420400102211    9.391631916599863e-10 
has_parent_with_comments      -0.4616662830553772      0.13936886611993773      -3.3125496095955715    0.0009244972814528296 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can also display the estimated random effects coefficients and the random effects standard deviation,&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Random effects coefficients:&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ran_ef&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Random effects correlation structure:&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ran_ef_summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inspect&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which produces&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Random effects coefficients:
{:intercept_fr=&amp;gt;0.0, :intercept_mo=&amp;gt;0.0, :intercept_sa=&amp;gt;0.0, :intercept_su=&amp;gt;0.0, :intercept_th=&amp;gt;0.0, :intercept_tu=&amp;gt;0.0, :intercept_we=&amp;gt;0.0}
Random effects standard deviation:

#&amp;lt;Daru::DataFrame:70278348234580 @name = 8e11a27f-81b0-48a0-9771-085a8f30693d @size = 1&amp;gt;
                  day 
       day        0.0 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Interestingly, the estimates of the random effects coefficients and standard deviation are all zero!
That is, we have a singular fit. Thus, our results imply that the day of the week on which a blog post is published has no effect on the number of comments that the blog post will receive. &lt;/p&gt;

&lt;p&gt;It is worth pointing out that such a model fit with a singular covariance matrix is problematic with the current version of Python&amp;#39;s &lt;code&gt;statmodels&lt;/code&gt; (described as &amp;quot;numerically challenging&amp;quot; in the &lt;a href=&quot;http://statsmodels.sourceforge.net/devel/mixed_linear.html&quot;&gt;documentation&lt;/a&gt;) and the R package &lt;code&gt;nlme&lt;/code&gt; (&amp;quot;Singular covariance matrices correspond to infinite parameter values&amp;quot;, a &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q4/022791.html&quot;&gt;mailing list reply&lt;/a&gt; by Douglas Bates, the author of &lt;code&gt;nlme&lt;/code&gt;). However, &lt;code&gt;mixed_models&lt;/code&gt;, &lt;code&gt;lme4&lt;/code&gt; and &lt;code&gt;MixedModels.jl&lt;/code&gt; can handle singular fits without problems.
In fact, like &lt;code&gt;mixed_models&lt;/code&gt; above, &lt;code&gt;lme4&lt;/code&gt; estimates the random effects coefficients and standard deviation to be zero, as we can see from the following R output:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; mod &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;log_comments &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; log_host_comments_avg &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; host_trackbacks_avg &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; length &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; has_parent_with_comments &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;day&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; data &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; df&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
Warning &lt;span class=&quot;kp&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
Some predictor variables are on very different scales&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; consider rescaling 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; ranef&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;mod&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;day
   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Intercept&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
fr           &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
mo           &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
sa           &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
su           &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
th           &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
tu           &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
we           &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; VarCorr&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;mod&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 Groups   Name        Std.Dev.
 day      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Intercept&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.0000&lt;/span&gt;  
 Residual             &lt;span class=&quot;m&quot;&gt;1.2614&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Unfortunately, &lt;code&gt;mixed_models&lt;/code&gt; is rather slow when applied to such a large data set (&lt;code&gt;blog_data&lt;/code&gt; is a data frame of size 22435&amp;times;8), especially when compared to &lt;code&gt;lme4&lt;/code&gt; which uses many sparse matrix tricks and is mostly written in C++ (integrated in R via &lt;code&gt;Rcpp&lt;/code&gt;) to speed up computation. The difference in performance between &lt;code&gt;mixed_models&lt;/code&gt; and &lt;code&gt;lme4&lt;/code&gt; is on the order of hours for large data, and Julia&amp;#39;s &lt;code&gt;MixedModels.jl&lt;/code&gt; promises to be even faster than &lt;code&gt;lme4&lt;/code&gt;. However, there is no noticeable difference in performance speed for smaller data sets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://nbviewer.ipython.org/github/agisga/mixed_models/blob/master/notebooks/blog_data.ipynb&quot;&gt;The full data analysis of the blog post data can be found in this IRuby notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;A second example and statistical inference on the parameter estimates&lt;/h3&gt;

&lt;p&gt;Often, the experimental design or the data suggests a linear mixed model whose random effects are associated with multiple grouping factors. A specification of multiple random effects terms which correspond to multiple grouping factors is often referred to as &lt;em&gt;crossed random effect&lt;/em&gt;, or &lt;em&gt;nested random effects&lt;/em&gt; if the corresponding grouping factors are nested in each other.
A good reference on such models is &lt;a href=&quot;http://lme4.r-forge.r-project.org/book/Ch2.pdf&quot;&gt;Chapter 2&lt;/a&gt; of Douglas Bates&amp;#39; &lt;code&gt;lme4&lt;/code&gt; book.&lt;/p&gt;

&lt;p&gt;Like &lt;code&gt;lme4&lt;/code&gt;, &lt;code&gt;mixed_models&lt;/code&gt; is particularly well suited for models with crossed or nested random effects. The current release of &lt;code&gt;statmodels&lt;/code&gt;, however, does not support crossed or nested random effects (according to the &lt;a href=&quot;http://statsmodels.sourceforge.net/devel/mixed_linear.html&quot;&gt;documentation&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;As an example we fit a linear mixed model with nested random effects to a data frame with 100 rows, of the form:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;c1&quot;&gt;#&amp;lt;Daru::DataFrame:69912847885160 @name = 2b161c5d-00de-4240-be50-8fa84f3aed24 @size = 5&amp;gt;&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;38842531&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10364866&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;44622300&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23307061&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;54993657&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2050404&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;52786614&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;00675&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;95&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;76011212&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20054527&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We consider the following model:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We take &lt;code&gt;y&lt;/code&gt; to be the response and &lt;code&gt;x&lt;/code&gt; its predictor.&lt;/li&gt;
&lt;li&gt;We consider the factor &lt;code&gt;b&lt;/code&gt; to be nested within the factor &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;We assume that the intercept varies due to variable &lt;code&gt;a&lt;/code&gt;; that is, a different (random) intercept term for each level of &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Moreover, we assume that the intercept varies due to the factor &lt;code&gt;b&lt;/code&gt; which is nested in &lt;code&gt;a&lt;/code&gt;; that is, different (random) intercept for each combination of levels of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is, mathematically the model can be expressed as&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;y = beta_0 + beta_1 * x + gamma(a) + delta(a,b) + epsilon
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;gamma(a) ~ N(0, phi**2)&lt;/code&gt; and &lt;code&gt;delta(a,b) ~ N(0, psi**2)&lt;/code&gt; are normally distributed random variables which assume different realizations for different values of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, and where &lt;code&gt;epsilon&lt;/code&gt; is a random Gaussian noise term with variance &lt;code&gt;sigma**2&lt;/code&gt;. The goal is to estimate the parameters &lt;code&gt;beta_0&lt;/code&gt;, &lt;code&gt;beta_1&lt;/code&gt;, &lt;code&gt;phi&lt;/code&gt;, &lt;code&gt;psi&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We fit this model in &lt;code&gt;mixed_models&lt;/code&gt;, and display the estimated random effects correlation structure with&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;y ~ x + (1|a) + (1|a:b)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                       &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;reml&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ran_ef_summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inspect&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which produces the output&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;                    a    a_and_b 
         a 1.34108300        nil 
   a_and_b        nil 0.97697500
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The correlation between the factor &lt;code&gt;a&lt;/code&gt; and the nested random effect &lt;code&gt;a_and_b&lt;/code&gt; is denoted as &lt;code&gt;nil&lt;/code&gt;, because the random effects in the model at hand are assumed to be independent.&lt;/p&gt;

&lt;p&gt;An advantage of &lt;code&gt;mixed_models&lt;/code&gt; over some other tools is the simplicity with which p-values and confidence intervals for the parameter estimates can be calculated using a multitude of available methods. Such methods include a likelihood ratio test implementation, multiple bootstrap based methods (which run in parallel by default), and methods based on the Wald Z statistic.&lt;/p&gt;

&lt;p&gt;We can compute five types of 95% confidence intervals for the fixed effects coefficients with the following line of code:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_conf_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which yields the result&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;                                          intercept                                        x 
    wald_z [-1.0442515623151203, 2.433416817887737]   [4.302419420148841, 5.038899876985704] 
boot_basic [-0.9676586601496888, 2.486799230544233]    [4.30540212917657, 5.028701160534481] 
 boot_norm [-1.0575520080398213, 2.4667867000424115   [4.295959190826356, 5.043382379744274] 
    boot_t [-0.9676586601496886, 2.486799230544233]    [4.30540212917657, 5.028701160534481] 
 boot_perc [-1.0976339749716164, 2.3568239157223054   [4.312618136600064, 5.035917167957975] 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, we see here that the intercept term is likely not significantly different from zero. We could proceed now by performing hypotheses tests using &lt;code&gt;#fix_ef_p&lt;/code&gt; or &lt;code&gt;#likelihood_ratio_test&lt;/code&gt;, or by refitting a model without an intercept using &lt;code&gt;#drop_fix_ef&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can also test the nested random effect for significance, in order to decide whether we should drop that term from the model to reduce model complexity. We can use a bootstrap based version of likelihood ratio test as follows.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ran_ef_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;grouping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
             &lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:bootstrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We get a p-value of 9.99e-4, suggesting that we probably should keep the term &lt;code&gt;(1|a:b)&lt;/code&gt; in the model formula.&lt;/p&gt;

&lt;h3&gt;A third example &amp;mdash; a less conventional model fit&lt;/h3&gt;

&lt;p&gt;Another advantage of &lt;code&gt;mixed_models&lt;/code&gt; against comparable tools is the ease of fitting models with arbitrary covariance structures of the random effects, which are not covered by the formula interface of &lt;code&gt;lme4&lt;/code&gt;. This can be done in a user-friendly manner by providing a block or a &lt;code&gt;Proc&lt;/code&gt; to the &lt;code&gt;LMM&lt;/code&gt; constructor. This unique feature of the Ruby language makes the implementation and usage of the method incredibly convenient. A danger of allowing for arbitrary covariance structures is, of course, that such a flexibility gives the user the freedom to specify degenerate and computationally unstable  models.&lt;/p&gt;

&lt;p&gt;As an example we look at an application to genetics, namely to SNP data (&lt;a href=&quot;https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism&quot;&gt;single-nucleotide polymorphism&lt;/a&gt;) with known pedigree structures (family relationships of the subjects). The family information is prior knowledge that we can model in the random effects of a linear mixed effects model.&lt;/p&gt;

&lt;p&gt;We model the quantitative trait &lt;code&gt;y&lt;/code&gt; (a vector of length 1200) as&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;y = X * beta + b + epsilon,
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;X&lt;/code&gt; is a &lt;code&gt;1200 x 130&lt;/code&gt; matrix containing the genotypes (i.e. 130 SNPs for each of the 1200 subjects); &lt;code&gt;epsilon&lt;/code&gt; is a vector of independent random noise terms with variances equal to &lt;code&gt;sigma**2&lt;/code&gt;; &lt;code&gt;beta&lt;/code&gt; is a vector of unknown fixed effects coefficients measuring the contribution of each SNP to the quantitative trait &lt;code&gt;y&lt;/code&gt;; and &lt;code&gt;b&lt;/code&gt; is a vector of random effects.&lt;/p&gt;

&lt;p&gt;If we denote the kinship matrix by &lt;code&gt;K&lt;/code&gt;, then we can express the probability distribution of &lt;code&gt;b&lt;/code&gt; as &lt;code&gt;b ~ N(0, delta**2 * 2 * K)&lt;/code&gt;, where we multiply &lt;code&gt;K&lt;/code&gt; by &lt;code&gt;2&lt;/code&gt; because the diagonal of &lt;code&gt;K&lt;/code&gt; is constant &lt;code&gt;0.5&lt;/code&gt;, and where &lt;code&gt;delta**2&lt;/code&gt; is a unknown scaling factor.&lt;/p&gt;

&lt;p&gt;The goal is to estimate the unknown parameters &lt;code&gt;beta&lt;/code&gt;, &lt;code&gt;sigma&lt;/code&gt;, and &lt;code&gt;delta&lt;/code&gt;, and to determine which of the fixed effects coefficients are significantly different from 0 (i.e. which SNPs are possibly causing the variability in the trait &lt;code&gt;y&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;In order to specify the covariance structure of the random effects, we need to pass a block or &lt;code&gt;Proc&lt;/code&gt; that produces the upper triangular Cholesky factor of the covariance matrix of the random effects from an input Array. In this example, that would be the multiplication of the prior known Cholesky factor of the kinship matrix with a scaling factor.&lt;/p&gt;

&lt;p&gt;Having all the model matrices and vectors, we compute the Cholesky factor of the kinship matrix and fit the model with&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# upper triangulat Cholesky factor&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kinship_mat_cholesky_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kinship_mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factorize_cholesky&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# Fit the model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;zt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;ss&quot;&gt;x_col_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                    &lt;span class=&quot;ss&quot;&gt;start_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                    &lt;span class=&quot;ss&quot;&gt;lower_bound&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kinship_mat_cholesky_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we can use the available hypotheses test and confidence interval methods to determine which SNPs are significant predictors of the quantitative trait. Out of the 130 SNPs in the model, we find 24 to be significant as linear predictors. &lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;http://agisga.github.io/mixed_models_applied_to_family_SNP_data/&quot;&gt;this blog post&lt;/a&gt; for a full analysis of this data with &lt;code&gt;mixed_models&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Room for improvement and future work&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Writing the formula language interpretation code used by &lt;code&gt;LMM#from_formula&lt;/code&gt; from scratch was not easy. Much of the code can be reorganized to be easier to read and to use in other projects. Possibly, the formula interface should be separated out, similar to how it is done with the Python package &lt;a href=&quot;https://github.com/pydata/patsy&quot;&gt;patsy&lt;/a&gt;. Also, some shortcut symbols (namely &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, and &lt;code&gt;||&lt;/code&gt;) in the model specification formula language are currently not implemented. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I plan to add linear mixed models for high-dimensional data (i.e. more predictors than observations) to &lt;code&gt;mixed_models&lt;/code&gt;, because that work would be in line with my current PhD research.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I plan to add generalized linear mixed models capabilities to &lt;code&gt;mixed_models&lt;/code&gt;, which can be used to fit mixed models to discrete data (such as binary or count data).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;I want to thank Google and the &lt;a href=&quot;sciruby.com&quot;&gt;Ruby Science Foundation&lt;/a&gt; for giving me this excellent opportunity! I especially want to thank &lt;a href=&quot;http://thebird.nl/&quot;&gt;Pjotr Prins&lt;/a&gt; who was my mentor for the project for much helpful advice and suggestions as well as his prompt responses to any of my concerns. I also want to thank my fellow GSoC participants &lt;a href=&quot;https://github.com/wlevine&quot;&gt;Will&lt;/a&gt;, &lt;a href=&quot;https://github.com/dilcom&quot;&gt;Ivan&lt;/a&gt;, and &lt;a href=&quot;https://github.com/v0dro&quot;&gt;Sameer&lt;/a&gt; for their help with certain aspects of my project.&lt;/p&gt;
</description>
        <pubDate>Wed, 19 Aug 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//gsoc-2015-mixed_models/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//gsoc-2015-mixed_models/</guid>
      </item>
    
      <item>
        <title>Bootstrapping and bootstrap confidence intervals for linear mixed models</title>
        <description>&lt;p&gt;During the last couple of days, I have added some parametric bootstrap capabilities to &lt;a href=&quot;https://github.com/agisga/mixed_models.git&quot;&gt;&lt;code&gt;mixed_models&lt;/code&gt;&lt;/a&gt;. The following demonstrates first how to resample fixed effects coefficient estimates via bootstrapping. Then various types of bootstrap confidence intervals are presented. Examples of computation and a brief comparison of different types of bootstrap confidence intervals are given.&lt;/p&gt;

&lt;p&gt;Implementation of bootstrap methods is motivated by the fact that the only alternatives currently available in &lt;code&gt;mixed_models&lt;/code&gt; are based on the Wald Z test statistic, which is far from being optimal in many settings, as I have briefly delineated in a &lt;a href=&quot;http://agisga.github.io/MixedModels_p_values_and_CI/&quot;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Parametric bootstrap&lt;/h2&gt;

&lt;p&gt;Parametric bootstrap for linear mixed models is performed via &lt;code&gt;LMM#bootstrap&lt;/code&gt;. Behind the scenes, for the model formulation which is summarized in a &lt;a href=&quot;http://agisga.github.io/First-linear-mixed-model-fit/&quot;&gt;previous blog post&lt;/a&gt;, bootstrap samples of the parameter estimates are obtained by the following procedure (as outlines in &lt;a href=&quot;http://personal.bgsu.edu/%7Ejshang/AICb_assumption.pdf&quot;&gt;this paper&lt;/a&gt;).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Fit a linear mixed model to obtain the estimated fixed effects $\hat{\beta}$, the estimated random effects covariance matrix $\hat{\Sigma}$, and the estimated scaling factor (or residual variance) $\hat{\sigma}^2$. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Generate a bootstrap sample as $y^{\ast} = X\hat{\beta} + Zb^{\ast} + o + \varepsilon^{\ast}$, where we randomly sample $b^{\ast} \sim N(0, \hat{\Sigma})$ and $\varepsilon^{\ast} \sim N(0, \hat{\sigma}^2 W^{-1})$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Re-fit the linear mixed model to the bootstrap data to obtain bootstrap parameter estimates.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat steps 2-3 &lt;code&gt;nsim&lt;/code&gt; times.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This resampling procedure can be performed in parallel using the Ruby gem &lt;a href=&quot;https://github.com/grosser/parallel&quot;&gt;parallel&lt;/a&gt; or single-threaded (more on that below).&lt;/p&gt;

&lt;p&gt;By default &lt;code&gt;LMM#bootstrap&lt;/code&gt; returns a bootstrap sample of the fixed effects coefficient estimates, but estimates of any other parameter can be returned as a bootstrap sample if an appropriate &lt;code&gt;Proc&lt;/code&gt; is passed as argument &lt;code&gt;what_to_collect&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As an example let&amp;#39;s generate a bootstrap sample of the intercept term for the &lt;a href=&quot;http://agisga.github.io/MixedModels_from_formula/&quot;&gt;alien species data&lt;/a&gt;. The following code does the job:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;mixed_models&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Daru&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_csv&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;./data/alien_species.csv&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Aggression ~ Age + Species + (Age | Location)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bootstrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Notice that I didn&amp;#39;t pass the argument &lt;code&gt;what_to_collect&lt;/code&gt; to &lt;code&gt;#bootstrap&lt;/code&gt;, because the intercept is one of the fixed effects terms which are collected by default.&lt;/p&gt;

&lt;p&gt;Using the gem &lt;code&gt;gnuplotrb&lt;/code&gt;, I can plot a histogram of the obtained bootstrap sample, in order to see the shape of its distribution. As expected, it looks approximately like a normal density.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/histogram_of_intercept.png?raw=true&quot; alt=&quot;histogram&quot;&gt;&lt;/p&gt;

&lt;p&gt;The image was produced by the following code.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;gnuplotrb&amp;#39;&lt;/span&gt;
&lt;span class=&quot;kp&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;GnuplotRB&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bin_width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bin_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_a&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rel_freq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;rel_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bins_center&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bin_width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Plot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins_center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rel_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;boxes&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;notitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;ss&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;fill solid 0.5&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Bootstrap confidence intervals&lt;/h2&gt;

&lt;p&gt;Of course, confidence intervals can be constructed based on the bootstrap samples obtained via &lt;code&gt;LMM#bootstrap&lt;/code&gt;. This functionality is now included in &lt;code&gt;LMM#fix_ef_conf_int&lt;/code&gt;. For example, still using the alien species data, basic bootstrap confidence intervals with confidence level of 95% for the fixed effects coefficient estimates can be computed with:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_conf_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:bootstrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;boottype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:basic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The result is a Hash of interval bounds for each fixed effects term:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;{:intercept=&amp;gt;[901.1323777908297, 1127.8830074251803], 
:Age=&amp;gt;[-0.24497613247630068, 0.10778076861255856], 
:Species_lvl_Human=&amp;gt;[-500.23358644022164, -499.1708950551018], 
:Species_lvl_Ood=&amp;gt;[-900.1587789864999, -899.0059837383694], 
:Species_lvl_WeepingAngel=&amp;gt;[-200.1445476541481, -199.0552908625565]}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Bootstrap interval types&lt;/h3&gt;

&lt;p&gt;There are currently four types of bootstrap confidence intervals implemented: &lt;em&gt;basic&lt;/em&gt;, &lt;em&gt;normal&lt;/em&gt;, &lt;em&gt;percentile&lt;/em&gt; and &lt;em&gt;studentized&lt;/em&gt; (default). All methods are taken from Chapter 5 in A. C. Davison and D. V. Hinkley, &lt;em&gt;Bootstrap Methods and their Application&lt;/em&gt; (Cambridge Series in Statistical and Probabilistic Mathematics, 1997).&lt;/p&gt;

&lt;p&gt;Denote the $p$ percentile of the bootstrap sample of a parameter $\theta$ as $\theta\subscript{p}^{\ast}$. Denote the point estimate of $\theta$ as $\hat{\theta}$, and the point estimate of the variance $v$ of $\theta$ as $\hat{v}$.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Basic bootstrap intervals are computed according to (5.6) in Chapter 5 of Davison &amp;amp; Hinkley as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$(2\hat{\theta} -\theta\subscript{(1-\alpha/2)}^{\ast}, 2\hat{\theta} -\theta\subscript{(\alpha/2)}^{\ast}).$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Normal bootstrap intervals are based on the normal distribution using resampling estimates $b\subscript{R}$ and $v\subscript{R}$
for bias correction and variance estimation, as given in (5.5) in Chapter 5 of Davison &amp;amp; Hinkley. The corresponding formula is&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$(\hat{\theta} - b\subscript{R} - \sqrt{v\subscript{R}}z\subscript{(1-\alpha/2)}, \hat{\theta} - b\subscript{R} + \sqrt{v\subscript{R}}z\subscript{(1-\alpha/2)}).$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Percentile bootstrap intervals are computed according to (5.18) in Davison &amp;amp; Hinkley as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$(\theta\subscript{(\alpha/2)}^{\ast}, \theta\subscript{(1-\alpha/2)}^{\ast}).$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Studentized bootstrap confidence intervals, also known as bootstrap-t, are based on the normal approximation confidence limits but use a bootstrapped version of the $N(0,1)$ variable $z$, as given in (5.7) in Chapter 5 of Davison &amp;amp; Hinkley. The studentized interval can be written as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$(\hat{\theta} - \sqrt{\hat{v}} \cdot z\subscript{(1-\alpha/2)}^{\ast}, \hat{\theta} - \sqrt{\hat{v}} \cdot z\subscript{(\alpha/2)}^{\ast}), \quad\mathrm{with}\, z^{\ast} = \frac{\theta^{\ast} - \hat{\theta}}{\sqrt{v^{\ast}}}.$$&lt;/p&gt;

&lt;p&gt;For more detail on the computation of the confidence intervals we refer to Davison &amp;amp; Hinkley and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29#Deriving_confidence_intervals_from_the_bootstrap_distribution&quot;&gt;this wikipedia article&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Comparison of confidence interval methods&lt;/h3&gt;

&lt;p&gt;Which confidence interval method to use is determined by the arguments &lt;code&gt;method&lt;/code&gt; (possible are &lt;code&gt;:wald&lt;/code&gt;, &lt;code&gt;:bootstrap&lt;/code&gt; and &lt;code&gt;:all&lt;/code&gt;) and &lt;code&gt;boottype&lt;/code&gt; (possible are &lt;code&gt;:basic&lt;/code&gt;, &lt;code&gt;:normal&lt;/code&gt;, &lt;code&gt;:studentized&lt;/code&gt; and &lt;code&gt;:percentile&lt;/code&gt;). &lt;/p&gt;

&lt;p&gt;The method &lt;code&gt;:all&lt;/code&gt; returns a &lt;code&gt;Daru::DataFrame&lt;/code&gt; containing the confidence intervals obtained by each of the available methods. The data frame can be printed in form of a nice looking table for inspection. For example for the alien species data we obtain all types of 95% confidence intervals with&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;ci&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_conf_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and print them to screen as a table with&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# round all results to two decimal places&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ci&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each_vector&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each_index&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Confidence intervals obtained with each of the available methods:&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ci&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inspect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which yields&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Confidence intervals obtained with each of the available methods:

#&amp;lt;Daru::DataFrame:70245799731260 @name = 1a212339-2e38-4c4c-8f08-f2edc3668f30 @size = 5&amp;gt;
                                intercept                  Age    Species_lvl_Human      Species_lvl_Ood Species_lvl_WeepingA 
              wald_z     [898.3, 1134.27]        [-0.24, 0.11]   [-500.22, -499.17]   [-900.12, -899.02]   [-200.13, -199.05] 
          boot_basic    [902.48, 1136.56]        [-0.24, 0.11]   [-500.22, -499.18]    [-900.1, -899.03]   [-200.13, -199.06] 
           boot_norm    [897.88, 1131.31]        [-0.23, 0.11]    [-500.2, -499.16]   [-900.09, -899.01]   [-200.12, -199.05] 
              boot_t    [902.48, 1136.56]        [-0.24, 0.11]   [-500.22, -499.18]    [-900.1, -899.03]   [-200.13, -199.06] 
           boot_perc     [896.02, 1130.1]         [-0.25, 0.1]   [-500.21, -499.17]   [-900.11, -899.04]   [-200.12, -199.05]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since here we are dealing with data that was simulated according to the assumptions of the linear mixed model, all parameters end up approximately meeting the normality assumptions, and therefore all confidence interval methods turn out to be pretty much equivalent. Often when analyzing less ideal data, this will not be the case. Then it might be necessary to compare different types of confidence intervals in order to draw the right conclusions.&lt;/p&gt;

&lt;p&gt;Theoretical results given in Chapter 5 of A. C. Davison and D. V. Hinkley, &lt;em&gt;Bootstrap Methods and their Application&lt;/em&gt; guarantee that for statistics which are approximately normal, the studentized bootstrap confidence intervals are second order accurate, meaning that a confidence interval with confidence level of $(1-\alpha)\cdot 100$ contains the true value with a probability of $(1-\alpha) + \mathcal{O}(n^{-1})$, where $n$ is the sample size. The basic and percentile bootstrap methods however are only first order accurate, that is, the interval coverage is correct only up to an order of $n^{-1/2}$. Nevertheless, for equi-tailed confidence intervals (as are all intervals considered above), the basic and percentile methods are second order accurate as well. The normal bootstrap and Wald Z confidence intervals are first order even when they are equi-tailed. Also note that all theoretical results here assume that the bootstrap sample is sufficiently large.&lt;/p&gt;

&lt;p&gt;In general, it appears that basic, percentile and studentized intervals are superior in accuracy compared to the normal bootstrap and Wald Z intervals in all circumstances. However, the normal bootstrap interval adjusts for the bias, and only the studentized bootstrap method adjusts for nonconstant variance and skewness as well as bias.&lt;/p&gt;

&lt;p&gt;Of course, the Wald Z method has the advantage of being computationally efficient and convenient. All bootstrap intervals are computationally very heavy, especially for big data sets.
Thus, it is probably best to use the Wald Z intervals in the data exploration phase, and compare different kinds of bootstrap intervals once it is more clear what to look for.&lt;/p&gt;

&lt;h2&gt;Parallel execution&lt;/h2&gt;

&lt;p&gt;Finally I also want to examine the parallel computing capabilities of the bootstrap confidence interval methods a little.
The following code benchmarks the computation of studentized bootstrap confidence intervals in parallel and single-threaded.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;benchmark&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;nil&lt;/span&gt;
&lt;span class=&quot;no&quot;&gt;Benchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;bm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;single-threaded&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_conf_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:bootstrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;bm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;parallel&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_conf_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:bootstrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The obtained results are given in this table.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;                        user     system      total        real
single-threaded   101.540000   0.000000 101.540000 (101.452211)
parallel           16.150000   0.030000 170.980000 ( 55.285422)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The parallel execution does in fact utilize all threads on both cores of my laptop (as I can simply observe by watching &lt;code&gt;htop&lt;/code&gt;). As expected, the parallel execution turns out to be about twice as fast as single-threaded. &lt;/p&gt;

&lt;!--The reason is that even though the bootstrap sample is obtained by `LMM#bootstrap` in parallel, the computation of the intervals from the bootstrap sample (like finding percentiles or transformations of the bootstrap sample) is always single-threaded. Better performance can be achieved for example by writing methods specifically adapted to a given data analysis, which would utilize the argument `what_to_collect` in the method `LMM#bootstrap` in a way optimal for the given setting. --&gt;
</description>
        <pubDate>Thu, 06 Aug 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//bootstap_confidence_intervals/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//bootstap_confidence_intervals/</guid>
      </item>
    
      <item>
        <title>A (naive) application of linear mixed models to genetics</title>
        <description>&lt;p&gt;The following shows an application of class &lt;code&gt;LMM&lt;/code&gt; from the Ruby gem &lt;a href=&quot;https://github.com/agisga/mixed_models.git&quot;&gt;&lt;code&gt;mixed_models&lt;/code&gt;&lt;/a&gt; to SNP data (&lt;a href=&quot;https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism&quot;&gt;single-nucleotide polymorphism&lt;/a&gt;) with known pedigree structures. The family information is prior knowledge that we can model in the random effects of a linear mixed effects model.&lt;/p&gt;

&lt;h2&gt;Data&lt;/h2&gt;

&lt;p&gt;I have simulated realistic SNP data with the simulation software &lt;a href=&quot;http://seqsimla.sourceforge.net/&quot;&gt;SeqSIMLA&lt;/a&gt;, using the software &lt;a href=&quot;http://www.broadinstitute.org/%7Esfs/cosi/&quot;&gt;cosi&lt;/a&gt; to generate a reference sequence, as advised in the SeqSIMLA &lt;a href=&quot;http://seqsimla.sourceforge.net/tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The response variable is a quantitative trait with mean 10 and variance 1. In total there are 130 SNPs in the data set, and SNPs 1, 3, 5 and 11 are selected to be causal, explaining 10%, 20%, 20% and 10% of the variance in the quantitative trait. Additionally, 35% of the variance is explained by shared environmental effects, and the remaining 5% by individual environmental effects. The correlation coefficient between spouses for the shared environmental effects is set to 0.8, and the respective correlation coefficients between parent and offspring as well as siblings is set to be 0.5. The data is available for ten families of twelve individuals each (i.e. 1200 subjects total). All families have identical pedigrees, which look like this:
&lt;img src=&quot;/images/pedigree.jpeg?raw=true&quot; alt=&quot;Image of pedigree&quot;&gt;&lt;/p&gt;

&lt;p&gt;The exact parameters provided to the SeqSIMLA software can be found in a &lt;a href=&quot;https://github.com/agisga/mixed_models/blob/master/examples/genetics/data/data_generation_and_preprocessing/SeqSIMLA_Call.txt&quot;&gt;text file in the repository&lt;/a&gt;. Additionally, I have preprocessed the SeqSIMLA output slightly and extracted the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kinship&quot;&gt;kinship&lt;/a&gt; matrix, both using a short &lt;a href=&quot;https://github.com/agisga/mixed_models/blob/master/examples/genetics/data/data_generation_and_preprocessing/preprocessing.R&quot;&gt;R script&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;The model&lt;/h2&gt;

&lt;p&gt;We model the quantitative trait $y$ (a vector of length 1200) as,
$$y = X\beta + b + \epsilon,$$
where $X$ is a $1200\times 130$ matrix containing the genotypes, $\epsilon$ is a vector of i.i.d. random residuals with variances equal to $\sigma\subscript{e}^2$, $\beta$ is a vector of unknown fixed effects coefficients, and $b$ is a vector of random effects.&lt;/p&gt;

&lt;p&gt;If we denote the kinship matrix by $K$, then we can express the probability distribution of $b$ as
$$b\sim N(0, \sigma\subscript{b}^2 2K),$$
where we multiply $K$ by $2$ because the diagonal of $K$ is constant $0.5$, and where $\sigma\subscript{b}^2$ is a scaling factor.&lt;/p&gt;

&lt;p&gt;The goal is to estimate the unknown parameters $\beta$, $\sigma\subscript{e}^2$ and $\sigma\subscript{b}^2$, and to determine which of the fixed effects coefficients are significantly different from 0 (i.e. which SNPs are possibly causing the variability in the phenotype).&lt;/p&gt;

&lt;h2&gt;Fit the model in Ruby&lt;/h2&gt;

&lt;p&gt;First, we need to load the generated design matrix $X$, the response vector $y$, and the kinship matrix $K$.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;read_csv_into_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each_line&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each_index&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each_index&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_f&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_array&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# fixed effects design matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_csv_into_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;./data/design_matrix.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unshift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# intercept&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;NMatrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# response vector&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_csv_into_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;./data/phenotype.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;NMatrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# kinship matrix, which determines the random effects covariance matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kinship_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_csv_into_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;./data/kinship_matrix.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kinship_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;NMatrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kinship_array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we can try to fit the model.&lt;/p&gt;

&lt;p&gt;Instead of using the user-friendly method &lt;code&gt;LMM#from_formula&lt;/code&gt; to fit the model, we will fit the model with raw model matrices directly using &lt;code&gt;LMM#initialize&lt;/code&gt;. While &lt;code&gt;LMM#from_formula&lt;/code&gt; mimics the behaviour of the function &lt;code&gt;lmer&lt;/code&gt; from the popular &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;lme4&lt;/code&gt; (see my &lt;a href=&quot;http://agisga.github.io/MixedModels_from_formula/&quot;&gt;previous&lt;/a&gt; blog &lt;a href=&quot;http://agisga.github.io/MixedModels_p_values_and_CI/&quot;&gt;posts&lt;/a&gt;), &lt;code&gt;LMM#initialize&lt;/code&gt; gives more flexibility to the user and allows for less conventional fits, which (to my knowledge) are not directly covered by &lt;code&gt;lme4&lt;/code&gt;. This flexibility comes in form of an interface, where the user can supply the parametrization for the triangular Cholesky factor of the covariance matrix of the random effects in form of a &lt;code&gt;Proc&lt;/code&gt; object or a block (which probably would not be as nice syntactically in most other languages as it is in Ruby).&lt;/p&gt;

&lt;p&gt;In this case, the Cholesky factor of the covariance matrix is $\sqrt{2} \sigma\subscript{b} \Lambda$, where $\Lambda$ is the Cholesky factor of the kinship matrix $K$. For convenience, we use the transformation $\theta = \sqrt{2} \sigma\subscript{b}$.&lt;/p&gt;

&lt;p&gt;Before we call &lt;code&gt;LMM.new&lt;/code&gt;, we also need to define the random effects model matrix $Z$ (which is the identity matrix in this case), find the Cholesky factor $\Lambda$ of the kinship matrix $K$, and specify the column names for the SNP matrix $X$. These steps and the model fit are performed by the following Ruby code.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;mixed_models&amp;#39;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;NMatrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# upper triangulat Cholesky factor&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kinship_mat_cholesky_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kinship_mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factorize_cholesky&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# fixed effects names&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:Intercept&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;SNP&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Fit the model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;zt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;ss&quot;&gt;x_col_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                    &lt;span class=&quot;ss&quot;&gt;start_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                    &lt;span class=&quot;ss&quot;&gt;lower_bound&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kinship_mat_cholesky_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It takes a couple of minutes to run.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;We can start by looking at some parameters describing the model fit:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Optimal theta: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;REML criterion: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deviance&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;yields&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Optimal theta:  [2.508012294769287]
REML criterion:     3919.756682815396
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(I know not very meaningful to look at... At least, we see that the optimization method converged.)&lt;/p&gt;

&lt;p&gt;Now, we might be interested to see which of the SNPs explain the variation in the quantitative trait best. To this end, we print those SNPs to the screen, which have a Wald p-value less than 0.05 (&lt;a href=&quot;http://agisga.github.io/MixedModels_p_values_and_CI/&quot;&gt;I have written before about Wald Z tests not being adequate&lt;/a&gt;, also see &lt;strong&gt;update&lt;/strong&gt; below).&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;p_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_p&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_signif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_vals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each_key&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_signif&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_vals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;05&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Fixed effects with Wald p-values &amp;lt;0.05:&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_signif&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;, &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We get:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;SNP2, SNP7, SNP10, SNP11, SNP13, SNP15, SNP24, SNP25, SNP26, 
SNP40, SNP41, SNP42, SNP51, SNP52, SNP53, SNP55, SNP62, SNP85, 
SNP96, SNP100, SNP102, SNP106, SNP107, SNP125, SNP127
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Because the data is simulated, we know that the true causal SNPs are #1, #3, #5 and #11. The model only picked up SNP #11 among those. However, this is not surprising, because SNP data is highly correlated. The selected SNPs are probably highly correlated to the true casual ones, and because of random fluctuations, in this particular data set they probably happened to explain the response better than the true causal SNPs. &lt;/p&gt;

&lt;p&gt;Also, it might be of interest to see just how much of the remaining (not-explained-by-SNPs) variability of the response is explained by family relatedness as compared to individual random fluctuations of each subject. We address this question by comparing the estimates of $\sigma\subscript{b}^2$ and $\sigma\subscript{e}^2$.&lt;/p&gt;

&lt;p&gt;Because $\theta$ is the scaling factor of the Cholesky factor $\Lambda$ of the kinship matrix $K$, and the covariance of the random effects $b$ is given by
$$\Sigma = \sigma\subscript{b}^2 2K = (\theta \Lambda) (\theta \Lambda^T),$$
it follows that
$$\sigma\subscript{b}^2 = \theta^2 / 2.$$ &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Variance due to family relatedness: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Residual variance: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that as expected from the SeqSIMLA input parameters mentioned above, the family relatedness influences the total variance of the trait a lot more than individual non-genetic factors.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Variance due to family relatedness:     3.1450628353569527
Residual variance:  0.3189292035466212
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr&gt;

&lt;h2&gt;Update&lt;/h2&gt;

&lt;p&gt;As an alternative to the Wald Z tests performed above we can make use of the equivalence of confidence intervals and significance tests. That is, if the 95% confidence interval of a fixed effect does not include zero, then the fixed effect coefficient in question differs from zero with a p-value of 0.05.
I have summarized different types of bootstrap confidence intervals available in &lt;code&gt;mixed_models&lt;/code&gt; in a &lt;a href=&quot;http://agisga.github.io/bootstap_confidence_intervals/&quot;&gt;blog post&lt;/a&gt;.
We can compute studentized bootstrap confidence intervals with 95% coverage using the following code.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_conf_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:bootstrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Due to the size of the data (1200 observations of 130 variables) the 1000 performed bootstrap simulations ran for more then 10 hours on my laptop (Intel Core i5 processor of fifth generation). Instead of examining all confidence intervals, I only print those which do not contain zero, using the following code.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;ci_signif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Hash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each_key&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# check if the CI contains 0&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ci_signif&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ci_bootstrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Studentized bootstrap confidence intervals not containing zero:&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ci_signif&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which yields:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Studentized bootstrap confidence intervals not containing zero:
{:SNP2=&amp;gt;[0.02102004268473849, 0.19723382621979196], 
:SNP7=&amp;gt;[0.024429922300370194, 0.2050671130468381], 
:SNP10=&amp;gt;[0.03459252399249489, 0.21426082763890741], 
:SNP11=&amp;gt;[0.08911647130840533, 0.2734691844585601], 
:SNP13=&amp;gt;[0.0456727118052626, 0.22425364463936825], 
:SNP15=&amp;gt;[0.03181623273320287, 0.21173723808641784], 
:SNP25=&amp;gt;[0.01986561195551105, 0.19729703681731675], 
:SNP26=&amp;gt;[0.016293023488597055, 0.20877970806848856], 
:SNP40=&amp;gt;[0.041118439272559176, 0.21138427319736441], 
:SNP41=&amp;gt;[0.020204375347575798, 0.20732073954994756], 
:SNP42=&amp;gt;[0.03651940469509106, 0.2140755606301379], 
:SNP51=&amp;gt;[0.052484169516553617, 0.24343113789170623], 
:SNP52=&amp;gt;[0.03286133268033546, 0.2095456450883188], 
:SNP53=&amp;gt;[0.017970201451067855, 0.2106064389500535], 
:SNP55=&amp;gt;[0.027856517149911247, 0.2048122682390223], 
:SNP62=&amp;gt;[0.049097681406648525, 0.23149634299539168], 
:SNP85=&amp;gt;[0.014276199187678293, 0.1942775690073427], 
:SNP96=&amp;gt;[0.001038435570539148, 0.18219718091237833], 
:SNP100=&amp;gt;[0.0028378036113550775, 0.19694912960692745], 
:SNP102=&amp;gt;[0.029119574583789387, 0.20291901799020484], 
:SNP106=&amp;gt;[0.0071595998238537795, 0.18495009016791536], 
:SNP107=&amp;gt;[0.008993896956614525, 0.19396696934223592], 
:SNP125=&amp;gt;[0.04701107230060422, 0.22696673801664247], 
:SNP127=&amp;gt;[0.006116174706046959, 0.1803563547638151]}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As the Wald Z tests above, the studentized bootstrap methods detects only one of the true causal SNPs #1, #3, #5 and #11. As explained above, this happens because of very high correlations between the SNPs. &lt;/p&gt;

&lt;p&gt;Another question that we may want to answer is how similar the results of the studentized bootstrap method are to those of the Wald Z tests. To answer that question we look at the intersection of the two sets of selected SNPs. The code&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;SNPs that have Wald p-values &amp;lt;.05 and studentized bootstrap confidence intervals not containing zero:&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ci_signif&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_signif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;produces the output&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;SNPs that have Wald p-values &amp;lt;.05 and studentized bootstrap confidence intervals not containing zero:
SNP2
SNP7
SNP10
SNP11
SNP13
SNP15
SNP25
SNP26
SNP40
SNP41
SNP42
SNP51
SNP52
SNP53
SNP55
SNP62
SNP85
SNP96
SNP100
SNP102
SNP106
SNP107
SNP125
SNP127
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that 24 out of the 25 SNPs detected by the Wald Z tests were also detected by the bootstrap method. In fact, it turns out that the set of SNPs detected by the studentized bootstrap is a subset of the SNP set identified by the Wald Z tests. The reason for this behaviour is probably that the fixed effects coefficient estimates are approximately normally distributed for the given data (which in itself is a interesting discovery).&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Jul 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//mixed_models_applied_to_family_SNP_data/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//mixed_models_applied_to_family_SNP_data/</guid>
      </item>
    
      <item>
        <title>P-values and confidence intervals</title>
        <description>&lt;p&gt;A few days ago I started working on hypotheses tests and confidence intervals for my project &lt;a href=&quot;https://github.com/agisga/mixed_models&quot;&gt;&lt;code&gt;mixed_models&lt;/code&gt;&lt;/a&gt;, and I got pretty surprised by certain things.&lt;/p&gt;

&lt;h1&gt;Methods&lt;/h1&gt;

&lt;p&gt;There does not seem to be an agreement on a method to compute p-values (or whether to compute them at all) and confidence intervals for (generalized) linear mixed models in the scientific community. See for example the multitude of discussions on Cross Validated (&lt;a href=&quot;http://stats.stackexchange.com/questions/118416/getting-p-value-with-mixed-effect-with-lme4-package&quot;&gt;(1)&lt;/a&gt;, 
&lt;a href=&quot;http://stats.stackexchange.com/questions/95054/how-to-get-the-overall-effect-for-linear-mixed-model-in-lme4-in-r&quot;&gt;(2)&lt;/a&gt;,
&lt;a href=&quot;http://stats.stackexchange.com/questions/65489/how-do-i-get-a-a-p-value-for-the-output-of-an-lme-model-with-lme4&quot;&gt;(3)&lt;/a&gt;, 
&lt;a href=&quot;http://stats.stackexchange.com/questions/22988/significant-effect-in-lme4-mixed-model&quot;&gt;(4)&lt;/a&gt; among others), or the longish &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html&quot;&gt;statement&lt;/a&gt; on the topic by the creator of &lt;code&gt;lme4&lt;/code&gt; and &lt;code&gt;nlme&lt;/code&gt; Douglas Bates.&lt;/p&gt;

&lt;p&gt;There are many ways to perform hypothesis tests and to compute confidence intervals for the fixed effects coefficients of a linear mixed model.
For a list see for example &lt;a href=&quot;http://glmm.wikidot.com/faq&quot;&gt;this entry from the wiki of the r-sig-mixed-models mailing list&lt;/a&gt;.
Unfortunately, the more accurate and universally applicable among the methods are computationally expensive and difficult to implement within Ruby&amp;#39;s current infrastructure of gems. &lt;/p&gt;

&lt;p&gt;The method that is most convenient to compute is the Wald Z-test. However, its validity is often questionable. &lt;a href=&quot;http://glmm.wikidot.com/faq&quot;&gt;The wiki of the r-sig-mixed-models mailing list&lt;/a&gt; names the following reasons:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;[Wald Z-statistics] are asymptotic approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal (or equivalently that the log-likelihood surface is quadratic) and that (2) the sampling distribution of the log-likelihood is (proportional to) $\chi^2$. The second approximation is discussed further under &amp;quot;Degrees of freedom&amp;quot;. The first assumption usually requires an even greater leap of faith, and is known to cause problems in some contexts (for binomial models failures of this assumption are called the Hauck-Donner effect), especially with extreme-valued parameters.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nevertheless, for now I decided to implement the Wald method only. It is still useful as a computationally light method for the initial data analysis, before falling back on the heavy weaponry. The &lt;code&gt;LMM&lt;/code&gt; class provides methods to access all parameter estimates and information required in order to implement other methods to compute p-values or confidence intervals by the user, applicable to her specific situation.&lt;/p&gt;

&lt;p&gt;For future extensibility I have included an argument &lt;code&gt;:method&lt;/code&gt; in all of the methods.&lt;/p&gt;

&lt;h1&gt;Implementation and usage&lt;/h1&gt;

&lt;h2&gt;Example data&lt;/h2&gt;

&lt;p&gt;For purposes of illustration, I use the same data as in my previous &lt;a href=&quot;http://agisga.github.io/MixedModels_from_formula/&quot;&gt;blog post&lt;/a&gt;.
The simulated data set contains two numeric variables &lt;em&gt;Age&lt;/em&gt; and &lt;em&gt;Aggression&lt;/em&gt;, and two categorical variables &lt;em&gt;Location&lt;/em&gt; and &lt;em&gt;Species&lt;/em&gt;. These data are available for 100 individuals.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alien_species&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;#&amp;lt;Daru::DataFrame:70197332524760 @name = 1cd9d732-526b-49ae-8cb1-35cd69541c87 @size = 10&amp;gt;&lt;/span&gt;
                  &lt;span class=&quot;no&quot;&gt;Age&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Aggression&lt;/span&gt;   &lt;span class=&quot;no&quot;&gt;Location&lt;/span&gt;    &lt;span class=&quot;no&quot;&gt;Species&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;204&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;95&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;877&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;542420&lt;/span&gt;     &lt;span class=&quot;no&quot;&gt;Asylum&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Dalek&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;39&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;88&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;852&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;528392&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;WeepingAng&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;107&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;34&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;388&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;791416&lt;/span&gt;     &lt;span class=&quot;no&quot;&gt;Asylum&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Human&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;210&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;01&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;170&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;010124&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt;        &lt;span class=&quot;no&quot;&gt;Ood&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;270&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1078&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31219&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Dalek&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;157&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;65&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;164&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;924992&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt;        &lt;span class=&quot;no&quot;&gt;Ood&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;136&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;865&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;838374&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;WeepingAng&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;241&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1052&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;36035&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Earth&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Dalek&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;86&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;84&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5725199&lt;/span&gt;     &lt;span class=&quot;no&quot;&gt;Asylum&lt;/span&gt;        &lt;span class=&quot;no&quot;&gt;Ood&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;206&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1070&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;71900&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Dalek&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We model the &lt;em&gt;Aggression&lt;/em&gt; level of an individual as a linear function of the &lt;em&gt;Age&lt;/em&gt; (&lt;em&gt;Aggression&lt;/em&gt; decreases with &lt;em&gt;Age&lt;/em&gt;), with a different constant added for each &lt;em&gt;Species&lt;/em&gt; (i.e. each species has a different base level of aggression). Moreover, we assume that there is a random fluctuation in &lt;em&gt;Aggression&lt;/em&gt; due to the &lt;em&gt;Location&lt;/em&gt; that an individual is at. Additionally, there is a random fluctuation in how &lt;em&gt;Age&lt;/em&gt; affects &lt;em&gt;Aggression&lt;/em&gt; at each different &lt;em&gt;Location&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;We fit this model in Ruby using &lt;code&gt;mixed_models&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;mixed_models&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alien_species&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Daru&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_csv&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;./data/alien_species.csv&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Aggression ~ Age + Species + (Age | Location)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                             &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alien_species&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Test statistics&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Wald_test#Test_on_a_single_parameter&quot;&gt;Wald Z test statistics&lt;/a&gt; for the fixed effects coefficients can be computed with:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_z&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; {:intercept=&amp;gt;16.882603431075875, :Age=&amp;gt;-0.7266646548258817, &lt;/span&gt;
&lt;span class=&quot;ss&quot;&gt;:Species_lvl_Human&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1862&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7747813759402&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:Species_lvl_Ood&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3196&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2289922406044&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
&lt;span class=&quot;ss&quot;&gt;:Species_lvl_WeepingAngel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;723&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7158917283754&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that the variable &lt;code&gt;Species&lt;/code&gt; seems to have a huge influence on &lt;code&gt;Aggression&lt;/code&gt;, while &lt;code&gt;Age&lt;/code&gt; not so much.&lt;/p&gt;

&lt;h2&gt;P-values&lt;/h2&gt;

&lt;p&gt;Based on the above test statistics, we can carry out hypotheses tests for each fixed effects term $\beta\subscript{i}$, testing the null
$$H\subscript{0} : \beta\subscript{i} = 0$$
against the alternative
$$H\subscript{a} : \beta\subscript{i} \neq 0.$$&lt;/p&gt;

&lt;p&gt;The corresponding (approximate) p-values are obtained with:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:wald&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; {:intercept=&amp;gt;0.0, :Age=&amp;gt;0.4674314106158888, &lt;/span&gt;
&lt;span class=&quot;ss&quot;&gt;:Species_lvl_Human&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:Species_lvl_Ood&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
&lt;span class=&quot;ss&quot;&gt;:Species_lvl_WeepingAngel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that indeed the aggression level of each species is highly significantly different from the base level (which is the species &lt;code&gt;Dalek&lt;/code&gt; in this model), while statistically we don&amp;#39;t have enough evidence to conclude that the age of an individual is a good predictor of his/her/its aggression level.&lt;/p&gt;

&lt;p&gt;I have specified &lt;code&gt;method: :wald&lt;/code&gt; above for illustration purposes only, because the Wald method is currently the default and the only available method.
In the future I might implement other methods which are more reliable and more computationally difficult at the same time.&lt;/p&gt;

&lt;h2&gt;Confidence intervals&lt;/h2&gt;

&lt;p&gt;We can use the Wald method for confidence intervals as well. For example 90% confidence intervals for each fixed effects coefficient estimate can be computed as follows.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef_conf_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:wald&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; {:intercept=&amp;gt;[917.2710135369496, 1115.302428002405],&lt;/span&gt;
 &lt;span class=&quot;ss&quot;&gt;:Age&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;[-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2131635992213468&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08253129235199347&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;ss&quot;&gt;:Species_lvl_Human&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;[-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13493113101106&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;499&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25245944940696&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;ss&quot;&gt;:Species_lvl_Ood&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;[-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;900&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;032260611745&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;899&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1063820954081&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;ss&quot;&gt;:Species_lvl_WeepingAngel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;[-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;0425&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8166587766&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;199&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13533441813757&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As for the p-values, the Wald method is currently the only option and the default.&lt;/p&gt;

&lt;!-- # Predictions

Recently, I have also implemented method for predictions on new data by the fitted linear mixed model.
Data can be supplied to `LMM#predict` either in form of a `Daru::DataFrame`, or as model matrices for the fixed and random effects (which is far less convenient but might be necessary for unconventional models).

Assume, we have captured ten new individuals of different ages and species and at different locations, and we want to estimate their aggression levels.

We can put their data in a new Daru::DataFrame:

```Ruby
                  Age   Location    Species 
         0        209  OodSphere      Dalek 
         1         90      Earth        Ood 
         2        173     Asylum        Ood 
         3        153     Asylum      Human 
         4        255  OodSphere WeepingAng 
         5        256     Asylum WeepingAng 
         6         37      Earth      Dalek 
         7        146      Earth WeepingAng 
         8        127     Asylum WeepingAng 
         9         41     Asylum        Ood
```

Then we estimate the aggression level of each individual (conditional on the obtained parameter estimates) using our fitted model:

```Ruby
model_fit.predict(newdata: newdata)

# =&gt; [1070.9125752531213, 182.45206492790766, -17.064468754763425, 384.78815861991046, 876.1240725686444, 674.711339114886, 1092.6985606350875, 871.150885526236, 687.4629975728096, -4.0162601001437395]
```

We can also exclude the estimated random effects from the predictions. For our example that would make sense, if we had observed individuals in previously unobserved locations.

```Ruby
model_fit.predict(newdata: newdata, with_ran_ef: false)

# =&gt; [1002.6356447018298, 110.83894560697945, 105.41770487190126, 506.59965400396266, 800.0421436018271, 799.9768274483924, 1013.8700230925942, 807.1616043262068, 808.4026112414656, 114.0394371252786]
```

### Prediction intervals

Additionally, confidence intervals for the predictions (i.e. prediction intervals) can be computed. The methods are the same as used for the confidence intervals of fixed effects. The prediction intervals are implemented only for the population level predictions (i.e. without inclusion of the random effects estimates).
Continuing our example, we have:
--&gt;
</description>
        <pubDate>Fri, 03 Jul 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//MixedModels_p_values_and_CI/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//MixedModels_p_values_and_CI/</guid>
      </item>
    
      <item>
        <title>MixedModels Formula Interface and Categorical Variables</title>
        <description>&lt;p&gt;I made some more progress on my &lt;a href=&quot;https://github.com/agisga/MixedModels&quot;&gt;Google Summer of Code project MixedModels&lt;/a&gt;.
The linear mixed models fitting method is now capable of handling non-numeric (i.e. categorical) predictor variables, as well as interaction effects. Moreover, I gave the method a user friendly R-formula-like interface.
I will present these new capabilities of the Ruby gem with an example. Then I will briefly describe their implementation.&lt;/p&gt;

&lt;h1&gt;Example&lt;/h1&gt;

&lt;h2&gt;Data and mathematical model formulation&lt;/h2&gt;

&lt;p&gt;The data is supplied to the model fitting method &lt;code&gt;LMM#from_formula&lt;/code&gt; as a &lt;code&gt;Daru::DataFrame&lt;/code&gt; (from the excellent Ruby gem &lt;a href=&quot;https://github.com/v0dro/daru.git&quot;&gt;daru&lt;/a&gt;). In order to test &lt;code&gt;LMM#from_formula&lt;/code&gt;, I have generated a data set of the following form:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alien_species&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;#&amp;lt;Daru::DataFrame:70197332524760 @name = 1cd9d732-526b-49ae-8cb1-35cd69541c87 @size = 10&amp;gt;&lt;/span&gt;
                  &lt;span class=&quot;no&quot;&gt;Age&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Aggression&lt;/span&gt;   &lt;span class=&quot;no&quot;&gt;Location&lt;/span&gt;    &lt;span class=&quot;no&quot;&gt;Species&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;204&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;95&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;877&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;542420&lt;/span&gt;     &lt;span class=&quot;no&quot;&gt;Asylum&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Dalek&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;39&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;88&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;852&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;528392&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;WeepingAng&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;107&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;34&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;388&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;791416&lt;/span&gt;     &lt;span class=&quot;no&quot;&gt;Asylum&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Human&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;210&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;01&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;170&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;010124&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt;        &lt;span class=&quot;no&quot;&gt;Ood&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;270&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1078&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31219&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Dalek&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;157&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;65&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;164&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;924992&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt;        &lt;span class=&quot;no&quot;&gt;Ood&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;136&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;865&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;838374&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;WeepingAng&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;241&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1052&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;36035&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Earth&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Dalek&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;86&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;84&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5725199&lt;/span&gt;     &lt;span class=&quot;no&quot;&gt;Asylum&lt;/span&gt;        &lt;span class=&quot;no&quot;&gt;Ood&lt;/span&gt; 
         &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;      &lt;span class=&quot;mi&quot;&gt;206&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1070&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;71900&lt;/span&gt;  &lt;span class=&quot;no&quot;&gt;OodSphere&lt;/span&gt;      &lt;span class=&quot;no&quot;&gt;Dalek&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As we can see, the data set contains two numeric variables &lt;em&gt;Age&lt;/em&gt; and &lt;em&gt;Aggression&lt;/em&gt;, and two categorical variables &lt;em&gt;Location&lt;/em&gt; and &lt;em&gt;Species&lt;/em&gt;. These data are available for 100 individuals.&lt;/p&gt;

&lt;p&gt;We model the &lt;em&gt;Aggression&lt;/em&gt; level of an individual as a linear function of the &lt;em&gt;Age&lt;/em&gt; (&lt;em&gt;Aggression&lt;/em&gt; decreases with &lt;em&gt;Age&lt;/em&gt;), with a different constant added for each &lt;em&gt;Species&lt;/em&gt; (i.e. each species has a different base level of aggression). Moreover, we assume that there is a random fluctuation in &lt;em&gt;Aggression&lt;/em&gt; due to the &lt;em&gt;Location&lt;/em&gt; that an individual is at. Additionally, there is a random fluctuation in how &lt;em&gt;Age&lt;/em&gt; affects &lt;em&gt;Aggression&lt;/em&gt; at each different &lt;em&gt;Location&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;Thus, the &lt;em&gt;Aggression&lt;/em&gt; level of an individual of &lt;em&gt;Species&lt;/em&gt; $spcs$ who is at the &lt;em&gt;Location&lt;/em&gt; $lctn$ can be expressed as:
$$Aggression = \beta\subscript{0} + \gamma\subscript{spcs} + Age \cdot \beta\subscript{1} + b\subscript{lctn,0} + Age \cdot b\subscript{lctn,1} + \epsilon,$$
where $\epsilon$ is a random residual, and the random vector $(b\subscript{lctn,0}, b\subscript{lctn,1})^T$ follows a multivariate normal distribution (the same distribution but different realizations of the random vector for each &lt;em&gt;Location&lt;/em&gt;). That is, we have a linear mixed model with fixed effects $\beta\subscript{0}, \beta\subscript{1}, \gamma\subscript{Dalek}, \gamma\subscript{Ood}, \dots$ and random effects $b\subscript{Asylum,0}, b\subscript{Asylum,1}, b\subscript{Earth,0},\dots$.&lt;/p&gt;

&lt;h2&gt;Model fit&lt;/h2&gt;

&lt;p&gt;We fit this model in Ruby using &lt;code&gt;MixedModels&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Aggression ~ Age + Species + (Age | Location)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                             &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alien_species&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where the argument &lt;code&gt;formula&lt;/code&gt; takes in a &lt;code&gt;String&lt;/code&gt; that contains a formula written in the formula language that is used in the R-package &lt;code&gt;lme4&lt;/code&gt; (&lt;code&gt;MixedModels&lt;/code&gt; currently supports most of the formula language except some shortcuts).
Since &lt;code&gt;lme4&lt;/code&gt; is currently the most commonly used package for linear mixed models, a lot of documentation and tutorials to the formula interface can be found online. &lt;/p&gt;

&lt;p&gt;We print some of the results that we have obtained:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;REML criterion: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dev_optimal&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Fixed effects:&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix_ef&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Standard deviation: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which produces the output:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;REML criterion:     333.71553910151437
Fixed effects:
{&amp;quot;x0&amp;quot;=&amp;gt;1016.2867207023437, &amp;quot;x1&amp;quot;=&amp;gt;-0.06531615342788923, 
&amp;quot;x2&amp;quot;=&amp;gt;-499.69369529020815, &amp;quot;x3&amp;quot;=&amp;gt;-899.569321353576, 
&amp;quot;x4&amp;quot;=&amp;gt;-199.5889580420067}
Standard deviation:     0.9745169802141329
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Comparison with R lme4&lt;/h3&gt;

&lt;p&gt;We fit the same model in R using the package &lt;code&gt;lme4&lt;/code&gt;, and print out the estimates for the same quantities as previously:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; mod &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Aggression &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; Age &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; Species &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Age &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; Location&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; data &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; alien.species&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; REMLcrit&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;mod&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;333.7155&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; fixef&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;mod&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Intercept&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                 Age        SpeciesHuman          SpeciesOod 
      &lt;span class=&quot;m&quot;&gt;1016.28672021&lt;/span&gt;         &lt;span class=&quot;m&quot;&gt;-0.06531615&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;-499.69369521&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;-899.56932076&lt;/span&gt; 
SpeciesWeepingAngel 
      &lt;span class=&quot;m&quot;&gt;-199.58895813&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; sigma&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;mod&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.9745324&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We observe that the parameter estimates from Ruby and R agree up to at least four digits behind the floating point. &lt;/p&gt;

&lt;h1&gt;A brief comment on the implementation&lt;/h1&gt;

&lt;h2&gt;Categorical predictor variables&lt;/h2&gt;

&lt;p&gt;If a predictor variable is categorical and no intercept term or other categorical variables are included in the design matrix, then the design matrix must contain a column of zeros and ones for each different level of the categorical variable. If the design matrix includes an intercept term or already contains another set of 0-1-indicators for a categorical variable, then one of the levels of the categorical variable, that we want to add to the model, must be excluded (or other so-called contrasts can be used).&lt;/p&gt;

&lt;p&gt;In the current implementation of &lt;code&gt;MixedModels&lt;/code&gt; this is handled by the method &lt;code&gt;Daru::DataFrame::create_indicator_vectors_for_categorical_vectors!&lt;/code&gt; (defined &lt;a href=&quot;https://github.com/agisga/MixedModels/blob/master/lib/MixedModels/daru_methods.rb#L90&quot;&gt;here&lt;/a&gt;). It adds a set of 0-1-valued vectors for each non-numeric vector in the &lt;code&gt;Daru::DataFrame&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Daru&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;d&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;ss&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;char&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_indicator_vectors_for_categorical_vectors!&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# =&amp;gt; &amp;lt;Daru::DataFrame:70212314363900 @name = 1a2a49d9-35d3-4adf-a993-5266d7d79442 @size = 7&amp;gt;&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;int&lt;/span&gt;       &lt;span class=&quot;n&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;char_lvl_b&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;char_lvl_c&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;char_lvl_d&lt;/span&gt; 
    &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;          &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
    &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;          &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
    &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;          &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
    &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;          &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
    &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;          &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
    &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;          &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
    &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;          &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;        &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(where it didn&amp;#39;t add a vector for level &amp;quot;a&amp;quot; of &amp;quot;char&amp;quot;, because it assumes a model with intercept by default)&lt;/p&gt;

&lt;p&gt;After the data frame is extended, &lt;code&gt;LMM#from_daru&lt;/code&gt; checks which of the specified terms are non-numeric, and replaces them with the names of the 0-1-valued indicator columns (e.g. if a fixed effects term &lt;code&gt;char&lt;/code&gt; were defined, &lt;code&gt;LMM#from_daru&lt;/code&gt; would replace it with &lt;code&gt;char_lvl_b&lt;/code&gt;, &lt;code&gt;char_lvl_c&lt;/code&gt; and &lt;code&gt;char_lvl_d&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;I will probably end up restructuring the current implementation, in order to better accommodate interaction effects between categorical variables...&lt;/p&gt;

&lt;h2&gt;Formula interface&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;LMM#from_formula&lt;/code&gt; takes in a &lt;code&gt;String&lt;/code&gt; containing a formula specifying the model, for example &lt;/p&gt;

&lt;p&gt;&amp;quot;z ~ x + y + x:y + (x | u)&amp;quot;.&lt;/p&gt;

&lt;p&gt;It transforms this formula into another &lt;code&gt;String&lt;/code&gt;, for the above example:&lt;/p&gt;

&lt;p&gt;&amp;quot;lmm_formula(:intercept) + lmm_variable(:x) + lmm_variable(:y) + lmm_variable(:x) * lmm_variable(:y) + (lmm_variable(:intercept) + lmm_variable(:x) | lmm_variable(:u)))&amp;quot;,&lt;/p&gt;

&lt;p&gt;adding intercept terms and wrapping all variables in &lt;code&gt;lmm_variable()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The Ruby expression in the &lt;code&gt;String&lt;/code&gt; is evaluated with &lt;code&gt;eval&lt;/code&gt;. This evaluation uses a specially defined class &lt;code&gt;LMMFormula&lt;/code&gt; (defined &lt;a href=&quot;https://github.com/agisga/MixedModels/blob/master/lib/MixedModels/LMMFormula.rb&quot;&gt;here&lt;/a&gt;), which overloads the &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt; operators, in order to combine the variable names into arrays, which can be fed into &lt;code&gt;LMM#from_daru&lt;/code&gt;. The class &lt;code&gt;LMMFormula&lt;/code&gt; was an idea that I got from Will Levine (&lt;a href=&quot;https://github.com/wlevine&quot;&gt;wlevine&lt;/a&gt;). In particular, the method &lt;code&gt;LMMFormula#to_input_for_lmm_from_daru&lt;/code&gt; transforms an &lt;code&gt;LMMFormula&lt;/code&gt; object into a number of &lt;code&gt;Array&lt;/code&gt;, which have the form required by &lt;code&gt;LMM#from_daru&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finally, &lt;code&gt;LMM#from_daru&lt;/code&gt; constructs the model matrices, vectors and the covariance function &lt;code&gt;Proc&lt;/code&gt;, which are passed on to &lt;code&gt;LMM#initialize&lt;/code&gt; that performs the actual model fit.&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Jun 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//MixedModels_from_formula/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//MixedModels_from_formula/</guid>
      </item>
    
      <item>
        <title>Model specification for linear mixed model</title>
        <description>&lt;p&gt;Last week I wrote about my implementation of an algorithm that fits a linear mixed model in Ruby using the gem &lt;a href=&quot;https://github.com/agisga/MixedModels&quot;&gt;MixedModels&lt;/a&gt;, that I am working on right now. See, &lt;a href=&quot;http://agisga.github.io/First-linear-mixed-model-fit/&quot;&gt;first rudimentary LMM fit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The currently available interface to the method is rather unfriendly to the user. First, I was planning on reproducing the interface of the R mixed models library lme4, but that appears to be too complicated and too time consuming. I spend some time thinking about what to do instead. Below, I present an idea and it&amp;#39;s comparison to the lme4 interface in R. In particular, I want to write a more user-friendly initialization method &lt;code&gt;LMM#from_daru&lt;/code&gt; that will work on &lt;a href=&quot;https://github.com/v0dro/daru&quot;&gt;daru&lt;/a&gt; data sets.&lt;/p&gt;

&lt;h3&gt;Random intercept model&lt;/h3&gt;

&lt;p&gt;Simplest model. The i&amp;#39;th observation of &amp;quot;yield&amp;quot; in the j&amp;#39;th batch is modeled as:
$$Yield\subscript{ij} = Intercept + BatchEffect\subscript{j} + RandomError\subscript{ij},$$
where &amp;quot;Intercept&amp;quot; is the overall mean, and &amp;quot;BatchEffect&amp;quot; denotes a random effect due to the batch that the i&amp;#39;th observation was in.&lt;/p&gt;

&lt;p&gt;In lme4 (R):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;fm01 &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Yield &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;Batch&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; Dyestuff&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In MixedModels (Ruby):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;fm01&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_daru&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:yield&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;ss&quot;&gt;fixed_effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;ss&quot;&gt;random_effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;ss&quot;&gt;grouping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dyestuff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Crossed random effects&lt;/h3&gt;

&lt;p&gt;Simple crossed random effects. The i&amp;#39;th observation of &amp;quot;diameter&amp;quot; in the j&amp;#39;th &amp;quot;sample&amp;quot; from the k&amp;#39;th &amp;quot;plate&amp;quot; is modeled as:
$$diameter\subscript{ijk} = Intercept + SampleIntercept\subscript{j} + PlateIntercept\subscript{k} + RandomError\subscript{ij},$$
where &amp;quot;Intercept&amp;quot; is the overall average, and &amp;quot;SampleIntercept&amp;quot; as well as &amp;quot;PlateIntercept&amp;quot; are random intercept terms, due to the sample and plate that a particular observation comes from.&lt;/p&gt;

&lt;p&gt;In R, lme4:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;fm03 &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;diameter &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;plate&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; Penicillin&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In Ruby, MixedModels:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;fm03&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_daru&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:diameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;ss&quot;&gt;fixed_effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;ss&quot;&gt;random_effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;ss&quot;&gt;grouping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;penicillin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Nested random effects&lt;/h3&gt;

&lt;p&gt;The i&amp;#39;th observation of &amp;quot;BoneGrowth&amp;quot; in the m&amp;#39;th &amp;quot;digit&amp;quot; of the k&amp;#39;th &amp;quot;foot&amp;quot; of the j&amp;#39;th &amp;quot;mouse&amp;quot; can be modelled as:
$$BoneGrowth\subscript{ijkm} = Intercept +  MouseIntercept\subscript{j} + FootIntercept\subscript{kj} + RandomError\subscript{ijkm},$$
i.e. the random effect &amp;quot;foot&amp;quot; only appears as nested within &amp;quot;mouse&amp;quot; (i.e. the intercept due to foot 1 in mouse 1 is different than the intercept due to foot 1 in mouse 2).&lt;/p&gt;

&lt;p&gt;In R, lme4:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;bone.growth.lmer &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;BoneGrowth &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;mouse&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;mouse&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;foot&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                         data &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; dat&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or more succinct:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;bone.growth.lmer &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;BoneGrowth &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;mouse&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;foot&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; data &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; dat&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In Ruby, MixedModels: Construct additional data frame columns, describing the interaction of foot and mouse, by hand. Then fit a model as shown above for crossed random effects...&lt;/p&gt;

&lt;h3&gt;Random slopes.&lt;/h3&gt;

&lt;p&gt;The i&amp;#39;th observation of &amp;quot;politeness&amp;quot; in the j&amp;#39;th subject and the k&amp;#39;th scenario is modeled as:&lt;/p&gt;

&lt;p&gt;$$\begin{eqnarray} 
Politeness\subscript{ijk} &amp;amp;=&amp;amp; Intercept + SubjectIntercept\subscript{j} + ScenarioIntercept\subscript{k} \\
 &amp;amp;+&amp;amp; IsFemale\subscript{ijk} \cdot FixedEffect + IQ\subscript{ijk} \cdot AnotherFixedEffect \\
&amp;amp;+&amp;amp; Attitude\subscript{ijk} \cdot SubjectSlope\subscript{j} + Attitude\subscript{ijk} \cdot ScenarioSlope\subscript{k} \\
 &amp;amp;+&amp;amp; RandomError\subscript{ijk},
\end{eqnarray}$$&lt;/p&gt;

&lt;p&gt;where we assume a random intercept and slope due to &amp;quot;subject&amp;quot;, and a random intercept and slope due to &amp;quot;scenario&amp;quot;.&lt;/p&gt;

&lt;p&gt;This can be expressed in R as:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;politeness.model &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;politeness &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; gender &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; IQ &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;attitude&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;subject&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;attitude&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;scenario&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;politeness&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And the equivalent in Ruby would be:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;politeness_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_daru&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:politeness&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;ss&quot;&gt;fixed_effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:gender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:IQ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;ss&quot;&gt;random_efffects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:attitude&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:attitude&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;ss&quot;&gt;grouping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:subject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:scenario&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;politeness&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Interaction effects, and transformations of the fixed effects&lt;/h3&gt;

&lt;p&gt;We model the i&amp;#39;th observation in the j&amp;#39;th batch as:
$$z\subscript{ij} = \beta\subscript{0} + x\subscript{ij} \cdot \beta\subscript{1} + x\subscript{ij}^2 \cdot \beta\subscript{2}
+ y\subscript{ij} \cdot \beta\subscript{3} + x\subscript{ij}y\subscript{ij} \cdot \beta\subscript{4} + u\subscript{j} + \epsilon\subscript{ij},$$
where $u\subscript{j}$ denotes the random intercept of the j&amp;#39;th batch and $\epsilon\subscript{ij}$ is the random error.&lt;/p&gt;

&lt;p&gt;In R, lme4:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;modfit &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;z &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; x&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;y &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;u&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In Ruby, MixedModels: By hand, generate new columns to the &lt;code&gt;daru&lt;/code&gt; data frame. Namely, a column containing the squares of $x$ and a column containing the products $xy$. Then call &lt;code&gt;LMM#from_daru&lt;/code&gt; as,&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;modfit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_daru&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;ss&quot;&gt;fixed_effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:xy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;ss&quot;&gt;random_effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;ss&quot;&gt;grouping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;ss&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;your_xyzu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Summary&lt;/h3&gt;

&lt;p&gt;This list of examples is most likely far from exhaustive, but it presents some of the most common types of linear mixed model fits.&lt;/p&gt;

&lt;p&gt;Disadvantages of my proposed model specification interface for Ruby compared to R:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lengthy&lt;/li&gt;
&lt;li&gt;No interaction effects&lt;/li&gt;
&lt;li&gt;No nested effects&lt;/li&gt;
&lt;li&gt;No transformations of predictors&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 12 Jun 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//LMM-model-specification/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//LMM-model-specification/</guid>
      </item>
    
      <item>
        <title>A rudimentary first linear mixed model fit</title>
        <description>&lt;p&gt;During the last two weeks I made some progress on my &lt;a href=&quot;https://github.com/agisga/MixedModels&quot;&gt;Google Summer of Code project&lt;/a&gt;.
The Ruby gem is now capable of fitting linear mixed models.
In this short blog post I want to give an example, and compare the results I get in Ruby to those obtained by &lt;code&gt;lme4&lt;/code&gt; in R.&lt;/p&gt;

&lt;h1&gt;LMM Mathematical Basics&lt;/h1&gt;

&lt;p&gt;Mathematically, a &lt;a href=&quot;http://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf&quot;&gt;linear mixed model&lt;/a&gt;
has the general form&lt;/p&gt;

&lt;p&gt;$$(y | b = \hat{b}) \sim \mathrm{N}(X\beta + Z\hat{b} + o, \sigma^2 W^{-1}), \mathrm{\,with\,} b \sim \mathrm{N}(0, \Sigma\subscript{\theta})$$&lt;/p&gt;

&lt;p&gt;where $y\in\mathbb{R}^n$ and $b\in\mathbb{R}^q$ are random vectors (response and random effects), $\beta\in\mathbb{R}^p$ is the vector of fixed effects, $o\in\mathbb{R}^n$ is a vector of known prior offset terms, $W\in\mathbb{R}^{n\times n}$ is a diagonal matrix of known prior weights. The random effects covariance matrix $\Sigma\subscript{\theta}\in\mathbb{R}^{q\times q}$ depends on the variance component parameter vector $\theta\in\mathbb{R}^l$.
Additionally, via the Cholesky decomposition we write 
$$\Sigma\subscript{\theta} = \sigma^2 \Lambda\subscript{\theta} \Lambda\subscript{\theta}^T,$$
where $\Lambda\subscript{\theta}$ is a lower triangular matrix, which is parametrized by $\theta$ in a way that is known a priori. &lt;/p&gt;

&lt;p&gt;The goal of the model fitting process is to find parameter estimates $\hat{\theta}$, $\hat{\beta}$ and $\hat{b}$ that fit the observed data best. Then the LMM fit can be used for prediction and inference.&lt;/p&gt;

&lt;h1&gt;Model Fit Example&lt;/h1&gt;

&lt;p&gt;The only currently available user interface is rudimentary. It requires the user to set up the design matrix $X$, the random effects model matrix $Z$, a &lt;code&gt;Proc&lt;/code&gt; that generates a $\Lambda\subscript{\theta}$ matrix from a $\theta$ input, etc. by hand, before calling &lt;code&gt;LMM#initialize&lt;/code&gt;. On the bright side, this adds a lot of flexibility to the model specification. In the future, I will write a more user-friendly R-like method &lt;code&gt;LMM#from_formula&lt;/code&gt;. &lt;/p&gt;

&lt;h2&gt;The Data&lt;/h2&gt;

&lt;p&gt;Now, let&amp;#39;s look at the simulated data that I use to test the implemented method. &lt;/p&gt;

&lt;p&gt;I generate a 50x2 design matrix $X$ with one column of ones (for the intercept) and one column of numbers 1,2,3,...,50.
The data is divided into five groups of equal size (consecutive blocks of 10 rows of $X$). Each of these groups has its own random intercept and random slope. Thus, the random effects model matrix $Z$ is of size 50x10, and has the form&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1 1  0 0  0 ...
1 2  0 0  0 ...
   ....
1 10 0 0  0 ...
0 0  1 11 0 ...
0 0  1 12 0 ...
   ....
0 0  1 20 0 ...
   ....
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and $\Lambda\subscript{\theta}$ is 10x10 block-diagonal with five square blocks of the form &lt;code&gt;[ [theta[0], 0], [theta[1], theta[2] ]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The random effects &lt;code&gt;b&lt;/code&gt; are generated from the multivariate distribution with mean 0 and covariance matrix &lt;code&gt;[ [1, 0.5], [0.5, 1] ]&lt;/code&gt;, and 50 random error terms &lt;code&gt;epsilon&lt;/code&gt; are generated from the standard normal distribution. Both, the fixed intercept and slope are set to be equal to one. &lt;/p&gt;

&lt;p&gt;Finally I generate the response vector &lt;code&gt;y&lt;/code&gt; as&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;The model fit can be performed with &lt;a href=&quot;https://github.com/agisga/MixedModels&quot;&gt;&lt;code&gt;MixedModels&lt;/code&gt;&lt;/a&gt; in Ruby via:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;zt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;lambdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lambdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                    &lt;span class=&quot;ss&quot;&gt;start_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;lower_bound&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;INFINITY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parametrization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;My entire Ruby code for this example can be found on github &lt;a href=&quot;https://github.com/agisga/MixedModels/blob/master/examples/LMM.rb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Behind the scenes, &lt;code&gt;LMM#initialize&lt;/code&gt; essentially performs the following three steps:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Ruby&quot; data-lang=&quot;Ruby&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# (1) Create the data structure in a LMMData object&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@model_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LMMData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;zt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;lambdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lambdat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                          &lt;span class=&quot;ss&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thfun&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# (2) Set up the profiled deviance/REML function&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@dev_fun&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;MixedModels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mk_lmm_dev_fun&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;vi&quot;&gt;@model_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;vi&quot;&gt;@reml&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# (3) Optimize the deviance/REML&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@optimization_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;MixedModels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;NelderMead&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;start_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                        &lt;span class=&quot;ss&quot;&gt;lower_bound&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower_bound&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                        &lt;span class=&quot;ss&quot;&gt;upper_bound&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upper_bound&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                        &lt;span class=&quot;ss&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                        &lt;span class=&quot;ss&quot;&gt;max_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                        &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dev_fun&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can fit a linear mixed model in R to the same matrix and vector data with:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;dat.frm &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;cbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;y&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; X&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;each&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;kp&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;dat.frm&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;x&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;grp&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
lmer.fit &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; lmer&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;y&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;grp&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; dat.frm&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let&amp;#39;s look at some of the results.&lt;/p&gt;

&lt;p&gt;In Ruby:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/rudimentary-lmm-fit-ruby.png?raw=true&quot; alt=&quot;Rudimentary-LMM-fit-Ruby PNG&quot; title=&quot;rudimentary-lmm-fit-ruby.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;And in R:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/rudimentary-lmm-fit-R.png?raw=true&quot; alt=&quot;Rudimentary-LMM-fit-R PNG&quot; title=&quot;rudimentary-lmm-fit-R.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;We see that all values agree between Ruby and R up to at least one digit behind the floating point (these values are the REML criterion, fixed effects estimates, random effects standard deviations and correlation, and residual variance and standard deviation, to be more precise). The slight differences are probably due to different optimization routines.&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jun 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//First-linear-mixed-model-fit/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//First-linear-mixed-model-fit/</guid>
      </item>
    
      <item>
        <title>Dissecting lme4&#39;s lmer function. Part 3.</title>
        <description>&lt;p&gt;This is the final part of my analysis of the function &lt;code&gt;lmer&lt;/code&gt;, which is used to fit linear mixed models in the R package &lt;code&gt;lme4&lt;/code&gt;.
In two previous blog posts, we have seen the general layout of the function &lt;code&gt;lmer&lt;/code&gt;, the dealings with the R model formula, and the setting up of the objective function for the optimization (see &lt;a href=&quot;http://agisga.github.io/Dissect_lmer_part1/&quot;&gt;part 1&lt;/a&gt; and &lt;a href=&quot;http://agisga.github.io/Dissect_lmer_part2/&quot;&gt;part 2&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;After the user-specified R model formula is evaluated to model matrices, vectors and parameters, and the objective function is generated, the function &lt;code&gt;optimizeLmer&lt;/code&gt; is called from within &lt;code&gt;lmer&lt;/code&gt; to carry out the optimization. We analyse &lt;code&gt;optimizeLmer&lt;/code&gt; below.&lt;/p&gt;

&lt;h1&gt;Minimizing the deviance - &lt;code&gt;optimizeLmer&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;The function takes as input arguments the previously generated deviance function &lt;code&gt;devfun&lt;/code&gt;, the provided (or previously computed by &lt;code&gt;mkLmerDevfun&lt;/code&gt;) starting values &lt;code&gt;start&lt;/code&gt; for the optimization, and other optimization parameters (such as the method to be used) bundled in the &lt;code&gt;merControl&lt;/code&gt; object &lt;code&gt;control&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then an environment is defined as &lt;code&gt;rho &amp;lt;- environment(devfun)&lt;/code&gt;. That is, &lt;code&gt;rho&lt;/code&gt; contains all the parameters defined by &lt;code&gt;mkLmerDevfun&lt;/code&gt; during the generation of &lt;code&gt;devfun&lt;/code&gt;; and additionally, the parent environment of &lt;code&gt;rho&lt;/code&gt; is the environment from which &lt;code&gt;mkLmerDevfun&lt;/code&gt; was called (so, there is access to variables from there as well).&lt;/p&gt;

&lt;p&gt;Eventually the function &lt;code&gt;optwrap&lt;/code&gt; (which is defined in &lt;code&gt;lmer.R&lt;/code&gt;) is called to carry out the actual optimization. We dissect &lt;code&gt;optwrap&lt;/code&gt; below. The returned object is saved as &lt;code&gt;opt&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;&lt;code&gt;optwrap&lt;/code&gt;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Use &lt;code&gt;getOptfun&lt;/code&gt; in order to check that the user-specified optimizer is supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deal with the peculiarities regarding the input arguments of the supported optimizer functions (e.g. modify &lt;code&gt;control&lt;/code&gt; so that the verbose argument will be passed on correctly), and set up other input arguments for the optimizer with &lt;code&gt;arglist &amp;lt;- list(fn = fn, par = par, lower = lower, upper = upper, control = control)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Call the optimizer:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;opt &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;withCallingHandlers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;do.call&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;optfun&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; arglist&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                           warning &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;w&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                               curWarnings &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;curWarnings&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;w&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                           &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Do some post optimization tweaking: rename the parameters in &lt;code&gt;opt&lt;/code&gt; in a consistent way and pass on all warnings.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the gradient for the objective function at the estimated minimal values using the function &lt;code&gt;deriv12&lt;/code&gt;, which uses a central finite difference method.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Store all auxiliary information and return &lt;code&gt;opt&lt;/code&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;&lt;span class=&quot;kp&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;optimizer&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; optimizer
&lt;span class=&quot;kp&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;control&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; control
&lt;span class=&quot;kp&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;warnings&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; curWarnings
&lt;span class=&quot;kp&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;derivs&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; derivs
opt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Extended convergence checking - &lt;code&gt;checkConv&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;If the optimization yields a result, then it is checked against additional convergence criteria by the function &lt;code&gt;checkConv&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute a scaled gradient as the solution to the linear system with the Cholesky factor of the Hessian as the matrix on the left hand side, and the gradient on the right hand side:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;scgrad &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;tryCatch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;derivs&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;chol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Hessian&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;gradient&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                        error&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;e&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;e&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Find the parallel minimum of the gradient and the scaled gradient of the objective function as &lt;code&gt;mingrad &amp;lt;- pmin(abs(scgrad),abs(derivs$gradient))&lt;/code&gt;. Check whether the maximal entry of &lt;code&gt;mingrad&lt;/code&gt; is above a specified threshold (default is 2e-3).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Similarly check the relative gradient against a specified relative tolerance (disabled by default).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check whether the variance of any random effect is below a specified tolerance (i.e. equal to 0), that is, whether we have a singular fit. The default tolerance level here is 1e-4.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check the Hessian of the objective function for convergence. This check is implemented in the function &lt;code&gt;checkHess&lt;/code&gt;, which performs the following steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Check that the Hessian has no negative eigenvalues (less that &lt;code&gt;-tol&lt;/code&gt;, where tol is 1e-6 by default).&lt;/li&gt;
&lt;li&gt;Check that the Hessian does not have very large eigenvalues, determined by $\rho(H) \cdot \mathrm{tol} &amp;gt; 1$ (where $\rho(H)$ is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Spectral_radius&quot;&gt;spectral radius&lt;/a&gt; of the Hessian, and tol is 1e-6 by default).&lt;/li&gt;
&lt;li&gt;Check that the ratio of the minimal to the maximal eigenvalues is not below &lt;code&gt;tol&lt;/code&gt;; which is equivalent to the &lt;a href=&quot;http://en.wikipedia.org/wiki/Condition_number&quot;&gt;conditional number&lt;/a&gt; of the Hessian being smaller than &lt;code&gt;1/tol&lt;/code&gt;. &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return all messages and warnings.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Prepare an output object - &lt;code&gt;mkMerMod&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;This function takes as inputs the environment of the objective function, the parameter estimates obtained from the optimization, the fixed effects and random effects model matrices etc., the original function call, and the messages generated from the convergence check in &lt;code&gt;checkConv&lt;/code&gt;. 
It checks, reorganizes and renames the parameters, and finally returns everything in an object of class &lt;code&gt;lmerMod&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;new&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;switch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;rcl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; lmerResp&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;lmerMod&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; glmResp&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;glmerMod&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; nlsResp&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;nlmerMod&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    call&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mc&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; frame&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;fr&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; flist&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;reTrms&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;flist&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; cnms&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;reTrms&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;cnms&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    Gp&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;reTrms&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Gp&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; theta&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pp&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;theta&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; beta&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    u&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;trivial.y&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA_real_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;pp&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Zt&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;else&lt;/span&gt; pp&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;u&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;fac&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    lower&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;reTrms&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;lower&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; devcomp&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;cmp&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cmp&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; dims&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dims&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    pp&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pp&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; resp&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;resp&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    optinfo &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;optimizer&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;optimizer&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                    control  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;control&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                    derivs   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;derivs&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                    conv  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;conv&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; lme4&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;lme4conv&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                    feval &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;is.null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;feval&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;else&lt;/span&gt; opt&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;feval&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    warnings &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;opt&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;warnings&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; val &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; opt&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;par&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        <pubDate>Sun, 17 May 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Dissect_lmer_part3/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Dissect_lmer_part3/</guid>
      </item>
    
  </channel>
</rss>