<!DOCTYPE html>
<html>
  <head>
    <title>NMatrix with Intel MKL on my university's HPC – Alexej Gossmann – Math PhD student at Tulane University</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="In order to use NMatrix for the statistical analysis of big genomic data, I decided to install it on my university’s high performance computing system (HPC). It is called Cypress (like the typical New Orleans tree), and it’s currently the 10th best among all American universities.

" />
    <meta property="og:description" content="In order to use NMatrix for the statistical analysis of big genomic data, I decided to install it on my university’s high performance computing system (HPC). It is called Cypress (like the typical New Orleans tree), and it’s currently the 10th best among all American universities.

" />
    
    <meta name="author" content="Alexej Gossmann" />

    
    <meta property="og:title" content="NMatrix with Intel MKL on my university's HPC" />
    <meta property="twitter:title" content="NMatrix with Intel MKL on my university's HPC" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Alexej Gossmann - Math PhD student at Tulane University" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
 
    <!-- MathJax interation-->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"]]}});
      MathJax.Hub.Config({TeX: {Macros:{subscript:['_{#1}',1],superscript:['^{#1}',1]}}});
    </script> 
    <!-- Turn on equation numbering -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
    </script>
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
 </head>

  <body>
    <div class="wrapper-masthead">
      <header class="masthead clearfix">
        <a href="/" class="site-avatar"><img src="https://avatars0.githubusercontent.com/u/11449372?v=3&s=460" /></a>

        <div class="site-info">
          <h1 class="site-name"><a href="/">Alexej Gossmann</a></h1>
          <p class="site-description">Math PhD student at Tulane University</p>
        </div>

        <nav>
          <a href="/about" class="blue">About</a>
          <a href="/" class="yellow">Blog</a>
          <a href="/talks" class="green">Presentations</a>
          <a href="/publications" class="magenta">Publications</a>
          <a href="/software" class="cyan">Software</a>
        </nav>
      </header>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>NMatrix with Intel MKL on my university's HPC</h1>

  <div class="entry">
    <p>In order to use <a href="https://github.com/SciRuby/nmatrix">NMatrix</a> for the statistical analysis of big genomic data, I decided to install it on my university’s high performance computing system (HPC). It is called <a href="http://crsc.tulane.edu/">Cypress</a> (like the <a href="http://imgc.allpostersimages.com/images/P-473-488-90/64/6420/5OV9100Z/posters/paul-souders-cypress-reflected-in-bayou-along-highway-61-on-stormy-summer-afternoon-new-orleans-louisiana-usa.jpg">typical New Orleans tree</a>), and it’s currently the 10th best among all American universities.</p>

<p>At first, I tried to install the latest development version of <code class="highlighter-rouge">nmatrix</code> and <code class="highlighter-rouge">nmatrix-atlas</code> or <code class="highlighter-rouge">nmatrix-lapacke</code> in the same way as I do it on my laptop or desktop. However, this failed in the compilation stage because the BLAS and LAPACK libraries could not be found.</p>

<p>Therefore, I decided to put some more effort into it, and install NMatrix with support for Intel MKL. <a href="https://software.intel.com/en-us/intel-mkl">Intel MKL (or Math Kernel Library)</a> promises BLAS and LAPACK functionality with much better performance on Intel hardware than the alternatives (such as ATLAS). Additionally, on Cypress, automatic offload of some LAPACK routines to the <a href="http://www.intel.com/content/www/us/en/processors/xeon/xeon-phi-detail.html?gclid=CKrYx9LGtcgCFc2PHwodG9YLuw&amp;gclsrc=aw.ds">Xeon Phi Coprocessors</a> can be enabled at run time, when Intel MKL is used.</p>

<p>I document the installation process in what follows (mainly for myself, in case I need to do it again).</p>

<h1 id="installation">Installation</h1>

<p>Cypress uses the <code class="highlighter-rouge">module</code> utility to manage multiple compilers, set environment variables, etc. In order to use Ruby as well as Intel MKL (which is contained in the Intel Parallel Studio XE suite), I need to load the corresponding modules:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>$ module load ruby
$ module load intel-psxe
</code></pre>
</div>

<p>Now I can use Ruby in version 2.2.3 as well as the Intel compiler suite and Intel MKL in my current session on Cypress. However, before installing NMatrix, I need to install its dependencies such as <code class="highlighter-rouge">bundler</code>.</p>

<h2 id="installing-gems-in-the-users-home-directory">Installing gems in the user’s home directory</h2>

<p>As a student I of course don’t have permission to install software system-wide on my university’s HPC. The option <code class="highlighter-rouge">--user-install</code> can be used with <code class="highlighter-rouge">gem install</code> to install gems locally in the user’s home directory. For more convenience one can add the line</p>

<div class="highlighter-rouge"><pre class="highlight"><code>gem: --user-install
</code></pre>
</div>

<p>to <code class="highlighter-rouge">.gemrc</code>. This way I can install <code class="highlighter-rouge">bundler</code>.</p>

<p>The remaining dependencies of <code class="highlighter-rouge">nmatrix</code> are installed with <code class="highlighter-rouge">bundle install</code>. In order for it to work, however, I need to add my local gem executable directory to path. In my case this is done with</p>

<div class="highlighter-rouge"><pre class="highlight"><code>export PATH=$PATH:/home/agossman/.gem/ruby/2.2.0/bin/
</code></pre>
</div>

<p>Also, <code class="highlighter-rouge">bundle install</code> needs to be invoked with an option for installation in the home directory, which in my case is:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>bundle install --path /home/agossman/.gem/ruby/2.2.0/
</code></pre>
</div>

<h2 id="installing-nmatrix-and-nmatrix-lapacke-with-intel-mkl">Installing NMatrix and NMatrix-lapacke with Intel MKL</h2>

<p>I followed the advice given in a comment in <a href="https://github.com/SciRuby/nmatrix/blob/b7d367f544a9d48af5f1b9dedb7ef6adcf488091/ext/nmatrix_lapacke/extconf.rb#L178"><code class="highlighter-rouge">nmatix/ext/nmatrix_lapacke/extconf.rb</code></a>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>#To use the Intel MKL, comment out the line above, and also comment out the bit above with have_library and dir_config for lapack.
#Then add something like the line below (for exactly what linker flags to use see https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor ):
#$libs += " -L${MKLROOT}/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential "
</code></pre>
</div>

<p>However, it took me a while to figure out the right linker flags. I used the <a href="https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor">MKL link line advisor</a>. Screen shots of inputs leading to working link lines can be found at the bottom of this page.</p>

<h3 id="the-right-link-line">The “right” link line</h3>

<p>There are three types of linking — static, dynamic, and SDL (single dynamic library).</p>

<h4 id="stuff-that-didnt-work">Stuff that didn’t work</h4>

<ol>
  <li>I couldn’t get <code class="highlighter-rouge">nmatrix-lapacke</code> to compile with dynamic linking (it complained that some BLAS function cannot be found). However, static and SDL linking work (see below).</li>
  <li>If the linked interface layer is <a href="https://software.intel.com/en-us/node/528524">ILP64 (which uses 64-bit integer type as opposed to 32-bit in the LP64 libraries)</a>, then <code class="highlighter-rouge">nmatrix-lapacke</code> crashes at runtime (always), even if it compiled and installed without complaints (using either static or SDL linking).</li>
  <li><a href="https://wiki.hpc.tulane.edu/trac/wiki/cypress/XeonPhi">Automatic offloading to Intel Xeon Phi coprocessors</a> (see below).</li>
</ol>

<h4 id="linking-flags-that-worked">Linking flags that worked:</h4>

<ol>
  <li>
    <p>The line given in the above NMatrix code comment does actually work. However, I don’t think that it is the optimal choice for the given system, because it does not use the specific features of the Parallel Studio XE suite employed on Cypress (such as parallelism).</p>
  </li>
  <li>
    <p>Using static linking with the MKL LP64 libraries, <code class="highlighter-rouge">nmatrix-lapacke</code> can be compiled with the support for automatic offload to the Intel Xeon Phi Coprocessor (there are two of those at every cluster node). It compiles, passes the tests, and installs. However, when I tried to <a href="https://wiki.hpc.tulane.edu/trac/wiki/cypress/XeonPhi">enable the automatic offload by setting <code class="highlighter-rouge">MKL_MIC_ENABLE</code> to 1</a>, I couldn’t get my Cholesky factorization toy problem to work (see below). With automatic offload disabled (<code class="highlighter-rouge">unset MKL_MIC_ENABLE</code>), everything works fine.</p>

    <p>In this case, the following link line needs to be added to <a href="https://github.com/SciRuby/nmatrix/blob/b7d367f544a9d48af5f1b9dedb7ef6adcf488091/ext/nmatrix_lapacke/extconf.rb#L178"><code class="highlighter-rouge">nmatix/ext/nmatrix_lapacke/extconf.rb</code></a>:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>$libs += " -Wl,--start-group ${MKLROOT}/lib/intel64/libmkl_intel_lp64.a ${MKLROOT}/lib/intel64/libmkl_core.a ${MKLROOT}/lib/intel64/libmkl_intel_thread.a -Wl,--end-group -liomp5 -ldl -lpthread "
</code></pre>
    </div>
  </li>
  <li>
    <p>Using linking via SDL, <code class="highlighter-rouge">nmatrix-lapacke</code> compiles, passes the tests, installs, and works great. However, usage of Intel Xeon Phi Coprocessors is not possible if SDL is used for linking.</p>

    <p>SDL offers further, rather convenient features of <a href="https://software.intel.com/en-us/node/528522">selection of the threading and interface layer at run time</a>:</p>

    <blockquote>
      <p>To set the threading layer at run time, use the <code class="highlighter-rouge">mkl_set_threading_layer</code> function or set <code class="highlighter-rouge">MKL_THREADING_LAYER</code> variable to one of the following values: <code class="highlighter-rouge">INTEL</code>, <code class="highlighter-rouge">SEQUENTIAL</code>, <code class="highlighter-rouge">PGI</code>. To set interface layer at run time, use the <code class="highlighter-rouge">mkl_set_interface_layer</code> function or set the <code class="highlighter-rouge">MKL_INTERFACE_LAYER</code> variable to <code class="highlighter-rouge">LP64</code> or <code class="highlighter-rouge">ILP64</code>.</p>
    </blockquote>

    <p>The necessary link line that should be used in <a href="https://github.com/SciRuby/nmatrix/blob/b7d367f544a9d48af5f1b9dedb7ef6adcf488091/ext/nmatrix_lapacke/extconf.rb#L178"><code class="highlighter-rouge">nmatix/ext/nmatrix_lapacke/extconf.rb</code></a> is:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>$libs += " -Wl,--no-as-needed -L${MKLROOT}/lib/intel64  -lmkl_rt -lpthread "
</code></pre>
    </div>
  </li>
</ol>

<h3 id="final-steps">Final steps</h3>

<p>After the linking flags have been determined and added into the code, the development version of <code class="highlighter-rouge">nmatrix</code> and <code class="highlighter-rouge">nmatrix-lapacke</code> can be compiled, tested, and installed as described in the <a href="https://github.com/SciRuby/nmatrix">NMatrix README</a> with the following lines of terminal input:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>$ bundle exec rake compile nmatrix_plugins=lapacke
$ bundle exec rake spec nmatrix_plugins=lapacke
$ bundle exec rake install nmatrix_plugins=lapacke
</code></pre>
</div>

<h2 id="simple-performance-tests">Simple performance tests</h2>

<p>I performed some quick tests for different installations of <code class="highlighter-rouge">nmatrix</code> and <code class="highlighter-rouge">nmatrix-lapacke</code>.</p>

<h3 id="svd-test">SVD test</h3>

<p>Consider the SVD of a 100 × 100 matrix:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="mi">10000</span><span class="p">.</span><span class="nf">times</span> <span class="k">do</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span>
  <span class="n">a</span> <span class="o">=</span> <span class="no">NMatrix</span><span class="p">.</span><span class="nf">random</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span> <span class="ss">dtype: :float64</span><span class="p">)</span>
  <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">vt</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">gesvd</span>

  <span class="c1"># check the result for correctness</span>
  <span class="n">s</span> <span class="o">=</span> <span class="no">NMatrix</span><span class="p">.</span><span class="nf">diagonal</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
  <span class="n">svd</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span><span class="p">.</span><span class="nf">dot</span> <span class="n">s</span><span class="p">).</span><span class="nf">dot</span> <span class="n">vt</span>
  <span class="k">unless</span> <span class="p">(</span><span class="n">svd</span> <span class="o">-</span> <span class="n">a</span><span class="p">).</span><span class="nf">all?</span> <span class="p">{</span> <span class="o">|</span><span class="n">entry</span><span class="o">|</span> <span class="n">entry</span><span class="p">.</span><span class="nf">abs</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">12</span> <span class="p">}</span>
    <span class="nb">puts</span> <span class="s2">"SVD does not work!"</span>
  <span class="k">end</span> 
<span class="k">end</span>
</code></pre>
</div>

<p>The results are:</p>

<ul>
  <li>Cypress, NMatrix compiled with static linking to MKL:  180817 milliseconds</li>
  <li>Cypress, NMatrix compiled with linking via SDL, with threading layer <code class="highlighter-rouge">INTEL</code>:  180839 milliseconds</li>
  <li>Cypress, NMatrix compiled with linking via SDL, with threading layer <code class="highlighter-rouge">SEQUENTIAL</code>:  83401 milliseconds</li>
  <li>My laptop (<code class="highlighter-rouge">nmatrix-lapacke</code> with Atlas):  122455 milliseconds</li>
</ul>

<h3 id="cholesky-factorization-test">Cholesky factorization test</h3>

<p>Consider a Cholesky factorization of a 5000 × 5000 matrix:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="mi">10</span><span class="p">.</span><span class="nf">times</span> <span class="k">do</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span>
  <span class="n">a</span> <span class="o">=</span> <span class="no">NMatrix</span><span class="p">.</span><span class="nf">random</span><span class="p">([</span><span class="mi">5000</span><span class="p">,</span><span class="mi">5000</span><span class="p">],</span> <span class="ss">dtype: :float64</span><span class="p">)</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">dot</span> <span class="n">a</span><span class="p">.</span><span class="nf">transpose</span>
  <span class="n">b</span><span class="p">.</span><span class="nf">factorize_cholesky</span>
<span class="k">end</span>
</code></pre>
</div>

<p>The results are:</p>

<ul>
  <li>Cypress, NMatrix compiled with static linking to MKL:  47016 milliseconds</li>
  <li>Cypress, NMatrix compiled with static linking to MKL, with Intel Xeon Phi Coprocessor automatic offload enabled: runtime error (says matrix not symmetric)</li>
  <li>Cypress, NMatrix compiled with linking via SDL, with threading layer <code class="highlighter-rouge">INTEL</code>:  46549 milliseconds</li>
  <li>Cypress, NMatrix compiled with linking via SDL, with threading layer <code class="highlighter-rouge">SEQUENTIAL</code>:  148072 milliseconds</li>
  <li>My laptop (<code class="highlighter-rouge">nmatrix-lapacke</code> with Atlas): 327146 milliseconds</li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>In particular, we see that for bigger matrices, multi-threading improves performance, while sequential execution is better for smaller matrices.</p>

<p>Based on these results I decided to compile <code class="highlighter-rouge">nmatrix-lapacke</code> on Cypress with linking to MKL via SDL, as it offers the flexibility of <a href="https://software.intel.com/en-us/node/528522">selecting the threading layer at runtime</a>.</p>

<h3 id="appendix-mkl-link-line-advisorhttpssoftwareintelcomen-usarticlesintel-mkl-link-line-advisor-screen-shots">Appendix: <a href="https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor">MKL link line advisor</a> screen shots</h3>

<p><img src="/images/link-line-1.png?raw=true" alt="SDL link line" /><br />
<img src="/images/link-line-2.png?raw=true" alt="Static linking link line" /></p>

  </div>

  <div class="date">
    Written on October  8, 2015
  </div>

  <div class="date">
    Tags:
		
		<a href="/tag/ruby">#ruby</a>
		
		<a href="/tag/nmatrix">#nmatrix</a>
		
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'agisga';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          



<a href="https://github.com/agisga"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/alexejgossmann"><i class="svg-icon linkedin"></i></a>

<a href="/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/agisga"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>

    

  </body>
</html>
