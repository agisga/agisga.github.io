<!DOCTYPE html>
<html>
  <head>
    <title>Understanding the CANDECOMP/PARAFAC Tensor Decomposition, aka CP; with R code – Alexej Gossmann</title>

        <meta charset="utf-8">
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

    
    <meta name="description" content="A tensor is essentially a multi-dimensional array:

">
    <meta property="og:description" content="A tensor is essentially a multi-dimensional array:

">
    
    <meta name="author" content="Alexej Gossmann">

    
    <meta property="og:title" content="Understanding the CANDECOMP/PARAFAC Tensor Decomposition, aka CP; with R code">
    <meta property="twitter:title" content="Understanding the CANDECOMP/PARAFAC Tensor Decomposition, aka CP; with R code">
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css">
    <link rel="alternate" type="application/rss+xml" title="Alexej Gossmann - PhD student with (severely sleep deprived) focus on statistics, machine learning, and programming" href="/feed.xml">

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->

    <!-- MathJax interation-->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"]]}});
      MathJax.Hub.Config({TeX: {Macros:{subscript:['_{#1}',1],superscript:['^{#1}',1]}}});
    </script>
    <!-- Turn on equation numbering -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
    </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
 </head>

  <body>
    <div class="wrapper-masthead">
      <header class="masthead clearfix">
        <a href="/" class="site-avatar"><img src="/images/avatar.png"></a>

        <div class="site-info">
          <h1 class="site-name"><a href="/">Alexej Gossmann</a></h1>
          <p class="site-description">PhD student with (severely sleep deprived) focus on statistics, machine learning, and programming</p>
        </div>

        <nav>
          <a href="/">/blog/</a>
          <a href="/publications">/publications/</a>
          <a href="/code">/code/</a>
          <a href="/about">/about/</a>
        </nav>
      </header>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Understanding the CANDECOMP/PARAFAC Tensor Decomposition, aka CP; with R code</h1>

  <div class="entry">
    <p>A tensor is essentially a multi-dimensional array:</p>

<ul>
  <li>a tensor of order one is a vector, which simply is a column of numbers,</li>
  <li>a tensor of order two is a matrix, which is basically numbers arranged in a rectangle,</li>
  <li>a tensor of order three looks like numbers arranged in rectangular box (or a cube, if all modes have the same dimension),</li>
  <li>an <em>n</em>th order (or <em>n</em>-way) tensor looks like numbers arranged in… an $n$-hyperrectangle or something like that… you get the idea…</li>
</ul>

<p>In many applications, data naturally form an <em>n</em>-way tensor with <em>n &gt; 2</em>, rather than a “tidy” table.</p>

<p>So, what’s a tensor decomposition?</p>

<p>Well, there are several types of tensor decomposition, but in this blog post I will introduce only the CANDECOMP/PARAFAC decomposition. Following <a href="http://epubs.siam.org/doi/abs/10.1137/07070111X?journalCode=siread&amp;">Kolda &amp; Bader (2009)</a> I will refer to it as <em>CP decomposition</em>. But before spelling it out in mathematical terms, let’s start with a simple toy example using the R language.</p>

<h2 id="rank-1-approximation-to-a-3-way-tensor-toy-example">Rank-1 approximation to a 3-way tensor (toy example)</h2>

<p>Assume that we observe spatio-temporal<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> data that, apart from random noise, correspond to the following three classes:</p>

<p><img src="../images/CP_tensor_decomposition/cases_for_rank-1_approx.png" alt="Three classes for rank-1 tensor approximation toy example"></p>

<p>As you can see from the above figures, the essential difference between the three cases is that they behave differently with respect to time. The spatial component is just a Gaussian curve, while the temporal component is piecewise constant with a sudden jump at time 50, which differs in magnitude and in direction between the three classes.</p>

<p>Now assume we have collected samples that correspond to the three classes above (with some added noise). <em>But we don’t know which sample falls in what class, how many classes there are, and how they differ</em> <img class="emoji" title=":confused:" alt=":confused:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f615.png" height="20" width="20" align="absmiddle">. In an unsupervised fashion we want to learn the different classes and their differences with respect to time and space.</p>

<p>We can arrange the samples in a 3-way tensor, <em>sample-by-space-by-time</em>. For simplicity, however, assume that the samples are already grouped according to their class within the tensor (but the algorithm doesn’t know that!), so that the resulting tensor looks like this:</p>

<p><img src="../images/CP_tensor_decomposition/3-modal_tensor.png" alt="3-way tensor"></p>

<p>In R the above tensor (let’s call it <code class="highlighter-rouge">X</code>) can be generated with the following lines of code:</p>

<div class="language-R highlighter-rouge">
<pre class="highlight"><code><span class="n">space_index</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">bell_curve</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">space_index</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w">
</span><span class="n">case1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="n">bell_curve</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">case2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="n">bell_curve</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">case3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="n">bell_curve</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="n">case2</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="m">51</span><span class="o">:</span><span class="m">100</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">case2</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="m">51</span><span class="o">:</span><span class="m">100</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.1</span><span class="w">
</span><span class="n">case3</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="m">51</span><span class="o">:</span><span class="m">100</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">case3</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="m">51</span><span class="o">:</span><span class="m">100</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">0.1</span><span class="w">

</span><span class="n">X</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">array</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">90</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">))</span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">30</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">    </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">case1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">),</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
  </span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="m">+30</span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">case2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">),</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
  </span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="m">+60</span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">case3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">),</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
</div>

<p>Using the excellent R package <a href="https://cran.r-project.org/package=rTensor"><code class="highlighter-rouge">rTensor</code></a> we obtain the CP decomposition with one component per mode of the tensor:</p>

<div class="language-R highlighter-rouge">
<pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">rTensor</span><span class="p">)</span><span class="w">
</span><span class="n">cp_decomp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cp</span><span class="p">(</span><span class="n">as.tensor</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="w"> </span><span class="n">num_components</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">str</span><span class="p">(</span><span class="n">cp_decomp</span><span class="o">$</span><span class="n">U</span><span class="p">)</span><span class="w">
</span><span class="c1"># List of 3
#  $ : num [1:90, 1] 0.0111 0.0111 0.0111 0.0111 0.0112 ...
#  $ : num [1:100, 1] -0.00233 -0.00251 -0.00271 -0.00292 -0.00314 ...
#  $ : num [1:100, 1] -0.00996 -0.00994 -0.00996 -0.00993 -0.00997 ...
# NULL
</span></code></pre>
</div>

<p>Visualizing the three components, we get the following figures:</p>

<p><img src="../images/CP_tensor_decomposition/rank-1_approx.png" alt="components of a rank-1 approximation to a 3-way tensor"></p>

<p>We can clearly see that</p>
<ol>
  <li>The sample-specific component has correctly separated the samples into three groups (in fact, <code class="highlighter-rouge">1:30</code> correspond to case 1, <code class="highlighter-rouge">31:60</code> correspond to case 2, <code class="highlighter-rouge">61:90</code> correspond to case 3).</li>
  <li>The spatial component is bell-shaped, just as the input data with respect to the spatial dimension.</li>
  <li>The temporal component has correctly picked up a change at time 50, which is where the three classes differ.</li>
</ol>

<p>So, what did just happen? <img class="emoji" title=":astonished:" alt=":astonished:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f632.png" height="20" width="20" align="absmiddle"></p>

<h2 id="cp-decomposition-quick-summary-of-the-math-behind-it">CP decomposition (quick summary of the math behind it)</h2>

<p>The CP decomposition factorizes a tensor into a sum of outer products of vectors. For example, for a 3-way tensor $X$, the CP decomposition can be written as</p>

<script type="math/tex; mode=display">X \approx \sum\subscript{r = 1}^R u\subscript{r} \circ v\subscript{r} \circ w\subscript{r} =: \widehat{X},</script>

<p>where $R&gt;0$ and $u\subscript{r}$, $v\subscript{r}$, $w\subscript{r}$ are vectors of appropriate dimensions, and where the notation “$\circ$” denotes the outer product for tensors, i.e.,</p>

<script type="math/tex; mode=display">x\subscript{ijk} \approx \hat{x}\subscript{ijk} = \sum\subscript{r = 1}^R u\subscript{ri} v\subscript{rj} w\subscript{rk}.</script>

<p>In case of the previous toy example we have that $R = 1$. Thus, the corresponding CP decomposition has the following form:</p>

<p><img src="../images/CP_tensor_decomposition/rank-1_decomposition_cartoon.png" alt="Rank-1 CP decomposition cartoon"></p>

<p>I hope that explains, why the components <code class="highlighter-rouge">u</code>, <code class="highlighter-rouge">v</code>, and <code class="highlighter-rouge">w</code> in the toy example look the way they do! <img class="emoji" title=":bowtie:" alt=":bowtie:" src="https://assets-cdn.github.com/images/icons/emoji/bowtie.png" height="20" width="20" align="absmiddle"></p>

<p>Now, how do you solve for the components $u\subscript{r}$, $v\subscript{r}$, $w\subscript{r}$ ($r = 1, 2, \ldots, R$)? You need to solve the following optimization problem:</p>

<script type="math/tex; mode=display">\min\subscript{\widehat{X}} \lVert X - \widehat{X} \rVert \quad\mathrm{with}\, \widehat{X} = \sum\subscript{r = 1}^R \lambda\subscript{r} u\subscript{r} \circ v\subscript{r} \circ w\subscript{r},</script>

<p>where $\lVert \cdot \rVert$ is the Frobenius norm.</p>

<p>The simplest way to do it is via an alternating least squares approach, where we would regard certain components as fixed while solving for others, and then iterate while alternating the components regarded as fixed… For much more rigour and detail see <a href="http://epubs.siam.org/doi/abs/10.1137/07070111X?journalCode=siread&amp;">Kolda &amp; Bader (2009) <em>Tensor Decompositions and Applications</em></a>.</p>

<h2 id="a-higher-rank-approximation-via-cp-toy-example-cont">A higher-rank approximation via CP (toy example cont.)</h2>

<p>Since in the previous toy example, there are no differentiating features between the three classes, apart from a jump in the temporal component, it makes perfect sense to set $R = 1$ in CP. In order to try out CP with more than one component per mode, I generated data with a more complex structure, and with further differentiation between the three groups with respect to their temporal and spatial makeup:</p>

<ol>
  <li>The three classes still have a bell-shaped component in the spatial mode, but now each class has a different mean.</li>
  <li>In the temporal mode the data is shaped like a sine wave, with different scaling per class.</li>
  <li>As before, there is a sudden jump in the temporal mode at time 50.</li>
</ol>

<p>The three classes in the resulting dataset have the following means.</p>

<p><img src="../images/CP_tensor_decomposition/cases_for_rank-3_approx.png" alt="Three classes for rank-3 tensor approximation toy example"></p>

<p>As before, we generate a tensor <code class="highlighter-rouge">X</code> of dimensions 90 × 100 × 100, with 30 samples per class obscured with random noise.</p>

<!---
```R
# bell-shaped spatial component with different means
space_index <- seq(-1, 1, l = 100)
case1 <- matrix(rep(dnorm(space_index, mean = 0, sd = 0.3), 10), 100, 100)
case2 <- matrix(rep(dnorm(space_index, mean = 0.5, sd = 0.3), 10), 100, 100)
case3 <- matrix(rep(dnorm(space_index, mean = -0.5, sd = 0.3), 10), 100, 100)
# sine-shaped temporal component
sine_wave <- sin(seq(-4*pi, 4*pi, l = 100))
sine_mat  <- matrix(rep(sine_wave, each = 100), 100, 100)
case1 <- case1 + 0.3 * sine_mat
case2 <- case2 + 0.6 * sine_mat
case3 <- case3 + 0.9 * sine_mat
# suddent drops in the temporal component
case2[ , 51:100] <- case2[ , 51:100] + 0.1
case3[ , 51:100] <- case3[ , 51:100] - 0.1

X <- array(NA, dim = c(90, 100, 100))
for(i in 1:30) {
  X[i, , ] <- case1 + matrix(rnorm(10000, sd = 0.1), 100, 100)
  X[i+30, , ] <- case2 + matrix(rnorm(10000, sd = 0.1), 100, 100)
  X[i+60, , ] <- case3 + matrix(rnorm(10000, sd = 0.1), 100, 100)
}
```
--->

<p>We use a CP decomposition in order to obtain a rank-3 approximation to that tensor:</p>

<div class="language-R highlighter-rouge">
<pre class="highlight"><code><span class="n">cp_decomp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cp</span><span class="p">(</span><span class="n">as.tensor</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="w"> </span><span class="n">num_components</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">max_iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<p>Here, we increase <code class="highlighter-rouge">max_iter</code> to 100, in order to ensure convergence, as can be checked with the <code class="highlighter-rouge">conv</code> attribute:</p>

<div class="language-R highlighter-rouge">
<pre class="highlight"><code><span class="n">cp_decomp</span><span class="o">$</span><span class="n">conv</span><span class="w">
</span><span class="c1"># [1] TRUE
</span></code></pre>
</div>

<p>Since we set <code class="highlighter-rouge">num_components = 3</code>, the solution now has three components per mode, organized in a three-column matrix for each mode:</p>

<div class="language-R highlighter-rouge">
<pre class="highlight"><code><span class="n">str</span><span class="p">(</span><span class="n">cp_decomp</span><span class="o">$</span><span class="n">U</span><span class="p">)</span><span class="w">
</span><span class="c1"># List of 3
#  $ : num [1:90, 1:3] 0.00131 0.00137 0.00141 0.0014 0.00135 ...
#  $ : num [1:100, 1:3] 0.000926 0.001345 0.001799 0.002228 0.002755 ...
#  $ : num [1:100, 1:3] 0.00969 0.0097 0.00974 0.0097 0.00971 ...
# NULL
</span></code></pre>
</div>

<p>And we can even check the percentage of the Frobenius norm of $X$ explained by the rank-3 approximation $\widehat{X}$:</p>

<div class="language-R highlighter-rouge">
<pre class="highlight"><code><span class="n">cp_decomp</span><span class="o">$</span><span class="n">norm_percent</span><span class="w">
</span><span class="c1"># [1] 83.13865
</span></code></pre>
</div>

<p>83% isn’t too bad!</p>

<p><img class="emoji" title=":grinning:" alt=":grinning:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f600.png" height="20" width="20" align="absmiddle"> Let’s look at a visualization of the obtained components!</p>

<p><img src="../images/CP_tensor_decomposition/rank-3_approx.png" alt="components of a rank-3 approximation to a 3-way tensor"></p>

<p>Indeed, we observe that,</p>

<ul>
  <li>the sample-specific components $u\subscript{r}$ clearly distinguish between the three groups of samples (<code class="highlighter-rouge">1:30</code>, <code class="highlighter-rouge">31:60</code>, and <code class="highlighter-rouge">61:90</code>),</li>
  <li>the spatial components $v\subscript{r}$ clearly picks up the Gaussian shapes with the three different means (at -0.5, 0, and 0.5),</li>
  <li>the temporal components $w\subscript{r}$ clearly show the sudden jump at time 50, as well as a sine wave.</li>
</ul>

<p>That’s it for now. In the next couple of weeks I am planning to write a couple blog posts on other types of tensor decompositions and tensor regression methods, as I am learning about them.</p>

<hr>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The two data modes can correspond to many types of measurements, other than space and time. Here, I use space and time for example purposes only because those are very familiar concepts. I am neither suggesting that specifically spatio-temporal data should be analyzed in this way, nor that tensor decomposition is generally a good approach for spatio-temporal data (I actually have no idea). <a href="#fnref:1" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

  </div>

  <div class="date">
    Written on April  2, 2017
  </div>

  <div class="date">
    Tags:
		
		<a href="/tag/r">#r</a>
		
		<a href="/tag/math">#math</a>
		
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'agisga';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:alexej.go@gmail.com"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/alexej.yexela"><i class="svg-icon facebook"></i></a>

<a href="https://github.com/agisga"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/alexejgossmann"><i class="svg-icon linkedin"></i></a>

<a href="/feed.xml"><i class="svg-icon rss"></i></a>




        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-94080131-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/tensor_decomposition_CP/',
		  'title': 'Understanding the CANDECOMP/PARAFAC Tensor Decomposition, aka CP; with R code'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
