<!DOCTYPE html>
<html>
  <head>
    <title>Understanding the Tucker decomposition, and compressing tensor-valued data (with R code) – Alexej Gossmann</title>

        <meta charset="utf-8">
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

    
    <meta name="description" content="In many applications, data naturally form an n-way tensor with n &gt; 2, rather than a “tidy” table.
As mentioned in the beginning of my last blog post, a tensor is essentially a multi-dimensional array:

  a tensor of order one is a vector, which simply is a column of numbers,
  a tensor of order two is a matrix, which is basically numbers arranged in a rectangle,
  a tensor of order three looks like numbers arranged in rectangular box (or a cube, if all modes have the same dimension),
  an nth order (or n-way) tensor looks like numbers arranged in an n-hyperrectangle… you get the idea…


">
    <meta property="og:description" content="In many applications, data naturally form an n-way tensor with n &gt; 2, rather than a “tidy” table.
As mentioned in the beginning of my last blog post, a tensor is essentially a multi-dimensional array:

  a tensor of order one is a vector, which simply is a column of numbers,
  a tensor of order two is a matrix, which is basically numbers arranged in a rectangle,
  a tensor of order three looks like numbers arranged in rectangular box (or a cube, if all modes have the same dimension),
  an nth order (or n-way) tensor looks like numbers arranged in an n-hyperrectangle… you get the idea…


">
    
    <meta name="author" content="Alexej Gossmann">

    
    <meta property="og:title" content="Understanding the Tucker decomposition, and compressing tensor-valued data (with R code)">
    <meta property="twitter:title" content="Understanding the Tucker decomposition, and compressing tensor-valued data (with R code)">
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css">
    <link rel="alternate" type="application/rss+xml" title="Alexej Gossmann - PhD student with focus on statistics, machine learning, and programming, among other things" href="/feed.xml">

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->

    <!-- MathJax integration and configuration-->
    <!-- Use standard LaTeX delimiters and turn on equation numbering -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"] ],
        displayMath: [ ["$$","$$"] ],
        processEscapes: true
      },
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
        Macros: {
          subscript: ['_{#1}', 1],
          superscript: ['^{#1}', 1]
        }
      }
    });
    </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
 </head>

  <body>
    <div class="wrapper-masthead">
      <header class="masthead clearfix">
        <a href="/" class="site-avatar"><img src="/images/avatar.png"></a>

        <div class="site-info">
          <h1 class="site-name"><a href="/">Alexej Gossmann</a></h1>
          <p class="site-description">PhD student with focus on statistics, machine learning, and programming, among other things</p>
        </div>

        <nav>
          <a href="/">/blog/</a>
          <a href="/publications">/publications/</a>
          <a href="/code">/code/</a>
          <a href="/about">/about/</a>
        </nav>
      </header>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Understanding the Tucker decomposition, and compressing tensor-valued data (with R code)</h1>

  <div class="entry">
    <p>In many applications, data naturally form an <em>n</em>-way tensor with <em>n &gt; 2</em>, rather than a “tidy” table.
As mentioned in the beginning of <a href="http://www.alexejgossmann.com/tensor_decomposition_CP/">my last blog post</a>, a tensor is essentially a multi-dimensional array:</p>
<ul>
  <li>a tensor of order one is a vector, which simply is a column of numbers,</li>
  <li>a tensor of order two is a matrix, which is basically numbers arranged in a rectangle,</li>
  <li>a tensor of order three looks like numbers arranged in rectangular box (or a cube, if all modes have the same dimension),</li>
  <li>an <em>n</em>th order (or <em>n</em>-way) tensor looks like numbers arranged in an <em>n</em>-hyperrectangle… you get the idea…</li>
</ul>

<p>In this post I introduce the Tucker decomposition (<a href="https://link.springer.com/article/10.1007%2FBF02289464">Tucker (1966) “Some mathematical notes on three-mode factor analysis”</a>). The Tucker decomposition family includes methods such as</p>

<ol>
  <li>the <em>higher-order SVD</em>, or HOSVD, which is a generalization of the matrix SVD to tensors (<a href="http://www.sandia.gov/~tgkolda/tdw2004/ldl-94-31.pdf">De Lathauwer, De Moor, and Vanderwalle (2000) “A multilinear singular value decomposition”</a>),</li>
  <li>the <em>higher order orthogonal iteration</em>, or HOOI, which delivers the best approximation to a given tensor by another tensor with prescribed mode-1 rank, mode-2 rank, etc. (<a href="http://epubs.siam.org/doi/abs/10.1137/S0895479898346995?journalCode=sjmael">De Lathauwer, De Moor, and Vanderwalle (2000) “On the Best Rank-1 and Rank-(R1,R2,…,RN) Approximation of Higher-Order Tensors”</a>).</li>
</ol>

<p>I introduce both approaches, and in order to demonstrate the usefulness of these concepts, I present a simple data compression example using <a href="http://data.worldbank.org/data-catalog/world-development-indicators">The World Bank’s World Development Indicators dataset</a> (though I use the <a href="https://www.kaggle.com/worldbank/world-development-indicators">version available on Kaggle</a>).</p>

<p>However, before we can get started with the decompositions, we need to look at and understand the <em>k</em>-mode tensor product.</p>

<p>Throughout this post, I will also introduce the R functions from the package <a href="https://cran.r-project.org/package=rTensor"><code class="highlighter-rouge">rTensor</code></a>, which can be used to perform all of the presented computations.</p>

<h2 id="tensor-times-matrix-the-k-mode-product">Tensor times matrix: the <em>k</em>-mode product</h2>

<p>The $k$-mode product of a tensor $X \in \mathbb{R}^{I\subscript{1} \times I\subscript{2} \times \ldots \times I\subscript{N}}$ with a matrix $A \in \mathbb{R}^{J \times I\subscript{k}}$ is written as</p>

<script type="math/tex; mode=display">Y = X \times\subscript{k} A.</script>

<p>The resulting tensor $Y$ is of size $I\subscript{1} \times \ldots \times I\subscript{k-1} \times J \times I\subscript{k+1} \times \ldots \times I\subscript{N}$, and contains the elements</p>

<script type="math/tex; mode=display">y\subscript{i\subscript{1}  \cdots  i\subscript{k-1}  j  i\subscript{k+1}  \cdots  i\subscript{N}} = \sum\subscript{i\subscript{k} = 1}^{I\subscript{k}} x\subscript{i\subscript{1}  i\subscript{2}  \cdots  i\subscript{N}} a\subscript{ji\subscript{k}}.</script>

<p>It can be hard, at first, to understand what that definition really means, or to visualize it in your mind. I find that it becomes easier once you realize that the <em>k</em>-mode product amounts to multiplying each mode-<em>k</em> fiber of $X$ by the matrix $A$.</p>

<p>We can demonstrate that in R:</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">rTensor</span><span class="p">)</span><span class="w">

</span><span class="n">tnsr</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.tensor</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)))</span><span class="w">
</span><span class="n">mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">

</span><span class="c1"># 1-mode product performed via the function ttm in rTensor</span><span class="w">
</span><span class="n">tnsr_times_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ttm</span><span class="p">(</span><span class="n">tnsr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tnsr</span><span class="p">,</span><span class="w"> </span><span class="n">mat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Now, for example, the first slice of <code class="highlighter-rouge">tnsr_times_mat</code> is the same as the matrix product of <code class="highlighter-rouge">mat</code> with the first slice of <code class="highlighter-rouge">tnsr</code>:</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tnsr_times_mat</span><span class="o">@</span><span class="n">data</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="c1">#      [,1] [,2]</span><span class="w">
</span><span class="c1"># [1,]    9   19</span><span class="w">
</span><span class="c1"># [2,]   12   26</span><span class="w">
</span><span class="c1"># [3,]   15   33</span><span class="w">
</span><span class="n">mat</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">tnsr</span><span class="o">@</span><span class="n">data</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">])</span><span class="w">
</span><span class="c1">#      [,1] [,2]</span><span class="w">
</span><span class="c1"># [1,]    9   19</span><span class="w">
</span><span class="c1"># [2,]   12   26</span><span class="w">
</span><span class="c1"># [3,]   15   33</span><span class="w">
</span></code></pre></div></div>

<p>You might want to play around some more with the function <code class="highlighter-rouge">ttm</code> in R to get a better understanding of the <em>k</em>-mode product.</p>

<p>A few important facts about the <em>k</em>-mode product:</p>
<ul>
  <li>$X \times\subscript{m} A \times\subscript{n} B = X \times\subscript{n} B \times\subscript{m} A$ if $n \neq m$,</li>
  <li>but $X \times\subscript{n} A \times\subscript{n} B = X \times\subscript{n} (BA)$ (in general $\neq X \times\subscript{n} B \times\subscript{n} A$).</li>
</ul>

<h2 id="tucker-decomposition">Tucker decomposition</h2>

<p>The Tucker decomposition (<a href="https://link.springer.com/article/10.1007%2FBF02289464">Tucker (1966)</a>) decomposes a tensor into a core tensor multiplied by a matrix along each mode (i.e., transformed via a $k$-mode product for every $k = 1, 2, \ldots, N$):</p>

<script type="math/tex; mode=display">X = G \times\subscript{1} A^{(1)} \times\subscript{2} A^{(2)} \times\subscript{3} \ldots \times\subscript{N} A^{(N)}.</script>

<p>Note that $G$ might be much smaller than the original tensor $X$ if we accept an approximation instead of an exact equality.</p>

<p>In case of three-way tensors, we can hold on to the following mental image:</p>

<p><img src="../images/Tucker_decomposition/3-modal_Tucker_decomposition.jpg" alt="Figure showing a Tucker decomposition of a 3-way tensor"></p>

<p>It is interesting to note that the CP decomposition, that I introduced in <a href="http://www.alexejgossmann.com/tensor_decomposition_CP/">a previous blog post</a>, is a special case of the Tucker decomposition, where the core tensor $G$ is constrained to be superdiagonal.</p>

<h3 id="higher-order-svd-hosvd">Higher-order SVD (HOSVD)</h3>

<p>So, how do you compute the Tucker decomposition?</p>

<p>Many algorithms rely on the following fundamental equivalence:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
Y &=& X \times\subscript{1} A^{(1)} \times\subscript{2} A^{(2)} \times\subscript{3} \ldots \times\subscript{N} A^{(N)} \nonumber\\
\Leftrightarrow\quad Y\subscript{(k)} &=& A^{(k)} X\subscript{(k)} \left(A^{(N)} \otimes \cdots \otimes A^{(k+1)} \otimes A^{(k-1)} \otimes \cdots A^{(1)} \right)^T. \nonumber
\end{eqnarray} %]]></script>

<p>The above equation uses some notation that was not introduced yet:</p>
<ul>
  <li>$\otimes$ denotes the <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker product</a>.</li>
  <li>
    <p>$X\subscript{(k)}$ is the mode-$k$ unfolding (or mode-$k$ matricization) of the tensor $X$. The mode-$k$ unfolding arranges the mode-$k$ fibers (a <em>fiber</em> is a generalization of <em>column</em> to tensors) of $X$ as columns into a matrix. The concept may be easiest to understand by looking at an example. The following R code shows a 3-way tensor and all three of its mode-$k$ unfoldings (using the <code class="highlighter-rouge">k_unfold</code> function from the <code class="highlighter-rouge">rTensor</code> package):</p>

    <div class="language-R highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">tnsr</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.tensor</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)))</span><span class="w">
</span><span class="n">tnsr</span><span class="o">@</span><span class="n">data</span><span class="w">
</span><span class="c1"># , , 1</span><span class="w">
</span><span class="c1">#</span><span class="w">
</span><span class="c1">#      [,1] [,2] [,3]</span><span class="w">
</span><span class="c1"># [1,]    1    3    5</span><span class="w">
</span><span class="c1"># [2,]    2    4    6</span><span class="w">
</span><span class="c1">#</span><span class="w">
</span><span class="c1"># , , 2</span><span class="w">
</span><span class="c1">#</span><span class="w">
</span><span class="c1">#      [,1] [,2] [,3]</span><span class="w">
</span><span class="c1"># [1,]    7    9   11</span><span class="w">
</span><span class="c1"># [2,]    8   10   12</span><span class="w">

</span><span class="c1"># mode-1 unfolding:</span><span class="w">
</span><span class="n">k_unfold</span><span class="p">(</span><span class="n">tnsr</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="o">@</span><span class="n">data</span><span class="w">
</span><span class="c1">#      [,1] [,2] [,3] [,4] [,5] [,6]</span><span class="w">
</span><span class="c1"># [1,]    1    3    5    7    9   11</span><span class="w">
</span><span class="c1"># [2,]    2    4    6    8   10   12</span><span class="w">

</span><span class="c1"># mode-2 unfolding:</span><span class="w">
</span><span class="n">k_unfold</span><span class="p">(</span><span class="n">tnsr</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="o">@</span><span class="n">data</span><span class="w">
</span><span class="c1">#      [,1] [,2] [,3] [,4]</span><span class="w">
</span><span class="c1"># [1,]    1    2    7    8</span><span class="w">
</span><span class="c1"># [2,]    3    4    9   10</span><span class="w">
</span><span class="c1"># [3,]    5    6   11   12</span><span class="w">

</span><span class="c1"># mode-3 unfolding:</span><span class="w">
</span><span class="n">k_unfold</span><span class="p">(</span><span class="n">tnsr</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="o">@</span><span class="n">data</span><span class="w">
</span><span class="c1">#      [,1] [,2] [,3] [,4] [,5] [,6]</span><span class="w">
</span><span class="c1"># [1,]    1    2    3    4    5    6</span><span class="w">
</span><span class="c1"># [2,]    7    8    9   10   11   12</span><span class="w">
</span></code></pre></div>    </div>
  </li>
</ul>

<p>A straightforward approach to solve the Tucker decomposition would be to solve each mode-$k$ matricized form of the Tucker decomposition (shown in the equivalence above) for $A^{(k)}$. This approach is known as <em>higher order SVD</em>, or HOSVD. It can be regarded as a generalization of the matrix SVD, because the matrices $A^{(k)}$ are orthogonal, while the tensor $G$ is “ordered” and “all-orthogonal” (see <a href="http://www.sandia.gov/~tgkolda/tdw2004/ldl-94-31.pdf">De Lathauwer et. al. (2000)</a> for detail). The resulting algorithm is shown below.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
&\,&\mathrm{for}\, k = 1, 2, ..., N \,\mathrm{do} \nonumber\\
&\,&\quad A^{(k)} \leftarrow \,\mathrm{left\,orthogonal\,matrix\,of\,SVD\,of}\,X\subscript{(k)} \nonumber\\
&\,&\mathrm{end\,for} \nonumber\\
&\,&G \leftarrow X \times\subscript{1} (A^{(1)})^T \times\subscript{2} (A^{(2)})^T \times\subscript{3} \cdots \times\subscript{N} (A^{(N)})^T  \nonumber
\end{eqnarray} %]]></script>

<p>In R we can perform HOSVD using the function <code class="highlighter-rouge">hosvd</code> from <code class="highlighter-rouge">rTensor</code>:</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tnsr</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rand_tensor</span><span class="p">(</span><span class="n">modes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">30</span><span class="p">,</span><span class="w"> </span><span class="m">40</span><span class="p">,</span><span class="w"> </span><span class="m">50</span><span class="p">))</span><span class="w">
</span><span class="n">hosv_decomp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hosvd</span><span class="p">(</span><span class="n">tnsr</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Now <code class="highlighter-rouge">hosv_decomp$Z</code> is our matrix $G$, and <code class="highlighter-rouge">hosv_decomp$U</code> is a list containing all the matrices $A^{(k)}$. We can use the function <code class="highlighter-rouge">ttl</code>, which performs multiple <em>k</em>-mode products on multiple modes successively given a tensor and a list of matrices, to check that up to numerical error the equation
$X = G \times\subscript{1} A^{(1)} \times\subscript{2} A^{(2)} \times\subscript{3} \ldots \times\subscript{N} A^{(N)}$
is satisfied:</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">HOSVD_prod</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ttl</span><span class="p">(</span><span class="n">hosv_decomp</span><span class="o">$</span><span class="n">Z</span><span class="p">,</span><span class="w"> </span><span class="n">hosv_decomp</span><span class="o">$</span><span class="n">U</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="n">error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tnsr</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">HOSVD_prod</span><span class="w">
</span><span class="n">table</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">error</span><span class="o">@</span><span class="n">data</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">1e-12</span><span class="p">)</span><span class="w">
</span><span class="c1">#</span><span class="w">
</span><span class="c1">#  TRUE</span><span class="w">
</span><span class="c1"># 60000</span><span class="w">
</span></code></pre></div></div>

<h3 id="higher-order-orthogonal-iteration-hooi">Higher order orthogonal iteration (HOOI)</h3>

<p>Note that we can also use HOSVD to compress $X$ by truncating the matrices $A^{(k)}$. The truncated HOSVD, however, is known to not give the best fit, as measured by the norm of the difference</p>

<script type="math/tex; mode=display">\lVert X - G \times\subscript{1} A^{(1)} \times\subscript{2} A^{(2)} \times\subscript{3} \ldots \times\subscript{N} A^{(N)} \rVert.</script>

<p>The <em>higher order orthogonal iteration</em>, or HOOI, algorithm finds the optimal approximation $\widehat{X}$ (with respect to the Frobenius norm loss) by, essentially, iterating the alternating truncation and SVD until convergence. If we truncate $A^{(k)}$ to have $r\subscript{k}$ columns, then the HOOI solution can be obtained by the following algorithm.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
&\,&\mathrm{initialize\,via\,HOSVD} \nonumber\\
&\,&\mathrm{while\,not\,converged\,do} \nonumber\\
&\,&\quad \mathrm{for}\, k = 1, 2, ..., N \nonumber\\
&\,&\quad\quad Y \leftarrow X \times\subscript{1} (A^{(1)})^T \times\subscript{2} \cdots \times\subscript{k-1} (A^{(k-1)})^T \times\subscript{k+1} (A^{(k+1)})^T \times\subscript{k+2} \cdots \times\subscript{N} (A^{(N)})^T  \nonumber\\
&\,&\quad\quad A^{(k)} \leftarrow r\subscript{k}\,\mathrm{leading\,left\,singular\,vectors\,of}\,Y\subscript{(k)} \nonumber\\
&\,&\quad \mathrm{end\,for} \nonumber\\
&\,&\mathrm{end\,while} \nonumber\\
&\,&G \leftarrow X \times\subscript{1} (A^{(1)})^T \times\subscript{2} (A^{(2)})^T \times\subscript{3} \cdots \times\subscript{N} (A^{(N)})^T  \nonumber\\
\end{eqnarray} %]]></script>

<h3 id="application-of-hooi-to-data-compression">Application of HOOI to data compression</h3>

<p>The example considered below is somewhat silly, given that the tensor I’m compressing isn’t very big, and thus there isn’t much of a point in compressing it. However, I think that the example still shows off very well how the algorithm can be very useful when the data size is much bigger (or the available storage much smaller).</p>

<p>I have downloaded from Kaggle the World Development Indicators <a href="https://www.kaggle.com/worldbank/world-development-indicators">dataset</a>, originally collected and published by The World Bank (the original dataset is available <a href="http://data.worldbank.org/data-catalog/world-development-indicators">here</a>).</p>

<p>The data can be arranged into a three-way tensor with the three modes corresponding to <em>country</em> (<a href="https://www.kaggle.com/benhamner/d/worldbank/world-development-indicators/countries-in-the-wdi-data">list of available countries</a>), <em>indicator</em> (<a href="https://www.kaggle.com/benhamner/d/worldbank/world-development-indicators/indicators-in-data">list of available indicators</a>), and <em>year</em> (1960-2014). Since I didn’t have any time to deal with NA values in any creative way, I have kept only three indicators in the dataset. And I have replaced the remaining NAs with a country-wise average value for each particular indicator. Also, I have forgotten to normalize the data <img class="emoji" title=":disappointed:" alt=":disappointed:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f61e.png" height="20" width="20">. The preprocessing resulted in a tensor of size 247-countries-by-3-indicators-by-55-years, that looks sort of like this:</p>

<p><img src="../images/Tucker_decomposition/3-way_tensor_small.png" alt="Figure: 3-way tensor obtained from the WDI data"></p>

<p>In particular, large stretches of the data within a given country tend to be nearly constant, or nearly piece-wise constant.</p>

<p>We use the function <code class="highlighter-rouge">tucker</code> from <code class="highlighter-rouge">rTensor</code> to obtain a Tucker decomposition via HOOI, where we set the ranks to the value 3 at each mode.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">dim</span><span class="p">(</span><span class="n">wdi_tnsr</span><span class="p">)</span><span class="w">
</span><span class="c1"># [1] 247   3  55</span><span class="w">

</span><span class="n">tucker_decomp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tucker</span><span class="p">(</span><span class="n">wdi_tnsr</span><span class="p">,</span><span class="w"> </span><span class="n">ranks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span><span class="w">

</span><span class="n">str</span><span class="p">(</span><span class="n">tucker_decomp</span><span class="p">)</span><span class="w">
</span><span class="c1"># List of 7</span><span class="w">
</span><span class="c1">#  $ Z           :Formal class 'Tensor' [package "rTensor"] with 3 slots</span><span class="w">
</span><span class="c1">#   .. ..@ num_modes: int 3</span><span class="w">
</span><span class="c1">#   .. ..@ modes    : int [1:3] 3 3 3</span><span class="w">
</span><span class="c1">#   .. ..@ data     : num [1:3, 1:3, 1:3] -6.60e+10 -1.13e+05 6.24e+05 -7.76e+05 -1.93e+08 ...</span><span class="w">
</span><span class="c1">#  $ U           :List of 3</span><span class="w">
</span><span class="c1">#   ..$ : num [1:247, 1:3] -0.02577 -0.00065 -0.01146 -0.19637 -0.17317 ...</span><span class="w">
</span><span class="c1">#   ..$ : num [1:3, 1:3] -1.00 -6.97e-10 -2.08e-02 2.08e-02 -4.70e-08 ...</span><span class="w">
</span><span class="c1">#   ..$ : num [1:55, 1:3] -0.0762 -0.0772 -0.0785 -0.0802 -0.082 ...</span><span class="w">
</span><span class="c1">#  $ conv        : logi TRUE</span><span class="w">
</span><span class="c1">#  $ est         :Formal class 'Tensor' [package "rTensor"] with 3 slots</span><span class="w">
</span><span class="c1">#   .. ..@ num_modes: int 3</span><span class="w">
</span><span class="c1">#   .. ..@ modes    : int [1:3] 247 3 55</span><span class="w">
</span><span class="c1">#   .. ..@ data     : num [1:247, 1:3, 1:55] 9.83e+07 4.44e+06 8.81e+07 1.05e+09 8.97e+08 ...</span><span class="w">
</span><span class="c1">#  $ norm_percent: num 99.4</span><span class="w">
</span><span class="c1">#  $ fnorm_resid : num 3.9e+08</span><span class="w">
</span><span class="c1">#  $ all_resids  : num [1:2] 3.9e+08 3.9e+08</span><span class="w">
</span><span class="c1"># NULL</span><span class="w">
</span></code></pre></div></div>

<p>To see how well the tensor decomposition approximates the original tensor, we can look at the relative error</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wdi_tnsr_approx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ttl</span><span class="p">(</span><span class="n">tucker_decomp</span><span class="o">$</span><span class="n">Z</span><span class="p">,</span><span class="w"> </span><span class="n">tucker_decomp</span><span class="o">$</span><span class="n">U</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="n">fnorm</span><span class="p">(</span><span class="n">wdi_tnsr</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">wdi_tnsr_approx</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">fnorm</span><span class="p">(</span><span class="n">wdi_tnsr</span><span class="p">)</span><span class="w">
</span><span class="c1"># [1] 0.005908934</span><span class="w">
</span></code></pre></div></div>

<p>and at the percentage of the norm of the original tensor explained by the Tucker decomposition</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tucker_decomp</span><span class="o">$</span><span class="n">norm_percent</span><span class="w">
</span><span class="c1"># [1] 99.40911</span><span class="w">
</span></code></pre></div></div>

<p>We, observe that we indeed achieve a recovery with an accuracy of over 99%. For comparison, the original tensor contains <code class="highlighter-rouge">247 * 3 * 55 = 40755</code> entries, while the computed Tucker decomposition consists of only <code class="highlighter-rouge">127 * 3 + 3 * 3 + 55 * 3 + 3 * 3 * 3 = 582</code> numbers. That’s a reduction in size by a factor greater than 70.</p>

<p>Even though data compression does not make much sense for the size of the dataset considered here, it clearly shows potential to be very useful for purposes of data distribution and data storage, when the data size far exceeds the terabyte range.</p>

  </div>

  <div class="date">
    Written on April  5, 2017
  </div>

  <div class="date">
    Tags:
    
    <a href="/tag/r">#r</a>
    
    <a href="/tag/math">#math</a>
    
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'agisga';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:alexej.go@gmail.com"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/alexej.yexela"><i class="svg-icon facebook"></i></a>

<a href="https://github.com/agisga"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/alexejgossmann"><i class="svg-icon linkedin"></i></a>

<a href="/feed.xml"><i class="svg-icon rss"></i></a>




        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-94080131-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/tensor_decomposition_tucker/',
		  'title': 'Understanding the Tucker decomposition, and compressing tensor-valued data (with R code)'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
